import os
import sys
sys.path.append(os.path.dirname(os.path.realpath(__file__)))
import asyncio
import numpy as np
import torch
import google.generativeai as genai
from PIL import Image
import io
import scipy.io.wavfile
import json
import concurrent.futures
import replicate
import tempfile
import requests
import subprocess
import shutil
import random
import time
import webbrowser
import re
import base64
import folder_paths
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading
import socket
import math

class SceneGenNode:
    def __init__(self):
        # Locate ComfyUI output directory
        self.output_dir = folder_paths.get_output_directory()
        self.cancel_event = threading.Event()
        self.cancel_server = None
        self.cancel_port = None

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "The input audio file (WAV/MP3) to analyze and generate video for."}),
                "gemini_api_key": ("STRING", {"multiline": False, "default": "", "tooltip": "Your Google Gemini API Key. Required for analysis and prompt generation."}),
                "replicate_api_token": ("STRING", {"multiline": False, "default": "", "tooltip": "Your Replicate API Token. Required for video generation models."}),
                "prompt_instruction": ("STRING", {"multiline": True, "default": "Describe a scene matching the music.", "tooltip": "Main instruction for the AI. Describe the desired mood, style, story, or specific visual elements."}),
                "filename_prefix": ("STRING", {"default": "scene_gen", "tooltip": "Prefix for all generated files (video, images, logs)."}),
                "fps": ("FLOAT", {"default": 24.0, "min": 1.0, "max": 120.0, "step": 0.1, "tooltip": "Frame rate of the final output video."}),
                "render_mode": (["Full Render", "Prompt Mode"], {"default": "Full Render", "tooltip": "Full Render: Generates video using Replicate. Prompt Mode: Generates prompts and assets, then creates a slideshow from start frames (skips Replicate)."}),
                "model_text": ("STRING", {"default": "gemini-3-pro-preview", "tooltip": "Gemini model used for text analysis, scripting, and prompting."}),
                "model_image": ("STRING", {"default": "gemini-3-pro-image-preview", "tooltip": "Gemini model used for generating start frames and assets."}),
                "creativity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "0.0 = Strict adherence to prompt. 1.0 = High hallucination/creative freedom."}),
                "dynamicity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "0.0 = Slow, static, contemplative shots. 1.0 = Fast cuts, high movement, intense action."}),
                "video_quality": (["Low", "Medium", "High"], {"default": "Medium", "tooltip": "Controls resolution and quality settings for video models. High = 1080p (where available), Medium = 720p/768p, Low = 480p."}),
                "aspect_ratio": (["16:9", "1:1", "9:16", "4:3"], {"default": "16:9", "tooltip": "Aspect ratio of the generated video."}),
                "resolution_multiplier": ("FLOAT", {"default": 1.0, "min": 0.5, "max": 2.0, "step": 0.1, "tooltip": "Scales the resolution of generated start frames."}),
                "enable_prompt_expansion": ("BOOLEAN", {"default": True, "tooltip": "If True, the AI will expand your simple instructions into highly detailed visual prompts."}),
                "save_segments": ("BOOLEAN", {"default": False, "tooltip": "If True, saves every individual video clip generated by Replicate to the output folder."}),
                "save_images": ("BOOLEAN", {"default": False, "tooltip": "If True, saves the start frame images generated by Gemini."}),
                "save_assets": ("BOOLEAN", {"default": True, "tooltip": "If True, saves the generated asset images (characters, props, environments)."}),
                "gemini_concurrency": ("INT", {"default": 5, "min": 1, "max": 50, "tooltip": "Max parallel requests to Gemini API."}),
                "replicate_concurrency": ("INT", {"default": 10, "min": 1, "max": 50, "tooltip": "Max parallel video generation jobs on Replicate."}),
                "use_wan_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Wan 2.5 Fast model."}),
                "use_wan_2_5": ("BOOLEAN", {"default": True, "tooltip": "Enable Wan 2.5 Standard model."}),
                "use_kling_turbo": ("BOOLEAN", {"default": True, "tooltip": "Enable Kling v2.5 Turbo model."}),
                "use_omni_human": ("BOOLEAN", {"default": True, "tooltip": "Enable OmniHuman model (good for realistic human movement)."}),
                "use_hailuo": ("BOOLEAN", {"default": True, "tooltip": "Enable Hailuo 2.3 model."}),
                "use_hailuo_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Hailuo 2.3 Fast model."}),
                "use_veo_3_1": ("BOOLEAN", {"default": True, "tooltip": "Enable Google Veo 3.1 model."}),
                "use_veo_3_1_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Google Veo 3.1 Fast model."}),
                "aggressive_edit": ("BOOLEAN", {"default": False, "tooltip": "If True, forces fast-paced editing with cuts strictly on beat. Generates full clips but trims them aggressively."}),
                "word_influence": ("FLOAT", {"default": 0.0, "min": -1.0, "max": 1.0, "step": 0.1, "tooltip": "1.0 = Literal visualization of lyrics. -1.0 = Ignore lyrics, focus on vibe/atmosphere. 0.0 = Balanced."}),
                "save_edl": ("BOOLEAN", {"default": True, "tooltip": "If True, exports a CMX 3600 .edl file for importing the timeline into Premiere Pro/DaVinci Resolve."}),
                "open_coffee_link": ("BOOLEAN", {"default": True, "tooltip": "Support the creator! Opens Buy Me a Coffee page after generation."}),
                "render_mode": (["Full Render", "Prompt Mode"], {"default": "Full Render", "tooltip": "Full Render: Generates video using Replicate. Prompt Mode: Generates prompts and assets, then creates a slideshow from start frames (skips Replicate)."}),
                "dialogues_gen": ("BOOLEAN", {"default": False, "tooltip": "Enable dialogue generation for supported models (Veo3, Wan 2.5). Prevents trimming of dialogue shots."}),
                "open_report": ("BOOLEAN", {"default": True, "tooltip": "Opens a live HTML report that updates in real-time during generation."}),
            },
            "optional": {
                "reference_images": ("IMAGE", {"tooltip": "Optional images to use as references for style, characters, or environments."}),
            }
        }

    RETURN_TYPES = (
        "IMAGE", "IMAGE", "IMAGE", "IMAGE",
        "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING",
        "STRING", "STRING"
    )
    RETURN_NAMES = (
        "Environment Images", "Asset Images", "Actor Images", "Scene Start Frames",
        "Analysis (S1)", "Style (S2)", "Palette (S3)", "Assets (S4)", "Montage (S5)", 
        "Prompts (S6)", "Start Frames Info (S7)", "Motion Refinement (S8)", "Timeline Data (S9)", "Generation Status (S10)", "Stitching Info (S11)",
        "Cost Data (JSON)", "Final Video Path"
    )
    FUNCTION = "process"
    CATEGORY = "Scene Gen"

    def process(self, audio, gemini_api_key, replicate_api_token, prompt_instruction, filename_prefix, fps, model_text, model_image, creativity, dynamicity, video_quality, aspect_ratio, resolution_multiplier, enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency, use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, aggressive_edit, word_influence, save_edl, open_coffee_link, render_mode, dialogues_gen, open_report, reference_images=None):
        print(f"\n[SceneGen] === Starting Iterative Process ===")
        
        if not gemini_api_key: raise ValueError("Gemini API Key is required.")
        if not replicate_api_token: raise ValueError("Replicate API Token is required.")
        
        os.environ["REPLICATE_API_TOKEN"] = replicate_api_token
        genai.configure(api_key=gemini_api_key)

        # Setup Session
        timestamp = int(time.time())
        session_dir = os.path.join(self.output_dir, f"{filename_prefix}_{timestamp}")
        os.makedirs(session_dir, exist_ok=True)
        
        # Process Audio
        waveform = audio['waveform']
        sample_rate = audio['sample_rate']
        if waveform.dim() == 3: waveform = waveform.squeeze(0)
        audio_np = waveform.cpu().numpy()
        
        # Ensure correct shape (samples, channels)
        if audio_np.shape[0] < audio_np.shape[1]: 
            audio_np = audio_np.T
            
        # Safe conversion to int16 to prevent clipping/wrapping
        # Clip to [-1, 1] range first to avoid overflow artifacts
        audio_np = np.clip(audio_np, -1.0, 1.0)
        audio_int16 = (audio_np * 32767).astype(np.int16)
        
        # Handle Reference Images
        ref_images_pil = []
        if reference_images is not None:
            for img_tensor in reference_images:
                i = 255. * img_tensor.cpu().numpy()
                img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
                ref_images_pil.append(img)

        # Run Async Pipeline
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, self.async_process(
                audio_int16, sample_rate, prompt_instruction, filename_prefix, session_dir, fps, 
                model_text, model_image, creativity, dynamicity, video_quality, aspect_ratio, resolution_multiplier,
                enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency,
                use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, 
                aggressive_edit, word_influence, save_edl, ref_images_pil, render_mode, dialogues_gen, open_report
            ))
            result = future.result()
            
            if open_coffee_link:
                try:
                    webbrowser.open("https://buymeacoffee.com/eyb8tkx3to", new=2)
                except:
                    pass

            print(f"[SceneGen] === Process Complete ===\n")
            return result

    async def async_process(self, audio_data, sample_rate, instruction, prefix, session_dir, fps, model_text_name, model_image_name, creativity, dynamicity, video_quality, aspect_ratio, resolution_multiplier, enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency, use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, aggressive_edit, word_influence, save_edl, ref_images_pil, render_mode, dialogues_gen, open_report):
        
        # Usage Tracking
        usage_stats = {
            "gemini_text_input_tokens": 0,
            "gemini_text_output_tokens": 0,
            "gemini_images_generated": 0,
            "replicate_seconds": {},
            "session_subfolder": os.path.relpath(session_dir, self.output_dir),
            "generated_assets": [],
            "generated_segments": []
        }
        
        # Reset cancel event for new run
        self.cancel_event.clear()
        
        # Start Cancel HTTP Server
        def find_free_port():
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('', 0))
                s.listen(1)
                port = s.getsockname()[1]
            return port
        
        self.cancel_port = find_free_port()
        
        class CancelHandler(BaseHTTPRequestHandler):
            def do_POST(handler_self):
                if handler_self.path == '/cancel':
                    self.cancel_event.set()
                    print("[SceneGen] Cancel requested via HTTP endpoint")
                    handler_self.send_response(200)
                    handler_self.send_header('Content-type', 'text/plain')
                    handler_self.send_header('Access-Control-Allow-Origin', '*')
                    handler_self.end_headers()
                    handler_self.wfile.write(b'Cancelled')
                else:
                    handler_self.send_response(404)
                    handler_self.end_headers()
            
            def do_OPTIONS(handler_self):
                handler_self.send_response(200)
                handler_self.send_header('Access-Control-Allow-Origin', '*')
                handler_self.send_header('Access-Control-Allow-Methods', 'POST, OPTIONS')
                handler_self.send_header('Access-Control-Allow-Headers', 'Content-Type')
                handler_self.end_headers()
            
            def log_message(handler_self, format, *args):
                pass  # Suppress HTTP server logs
        
        self.cancel_server = HTTPServer(('localhost', self.cancel_port), CancelHandler)
        server_thread = threading.Thread(target=self.cancel_server.serve_forever, daemon=True)
        server_thread.start()
        print(f"[SceneGen] Cancel server running on http://localhost:{self.cancel_port}")

        # --- Live Reporting Setup ---
        # --- Live Reporting Setup ---
        report_filename = f"status_{prefix}.html"
        report_js_filename = f"status_data_{prefix}.js"
        
        # Save report INSIDE the session directory
        report_path = os.path.join(session_dir, report_filename)
        report_js_path = os.path.join(session_dir, report_js_filename)
        
        # Paths for JS (relative to the HTML file)
        js_filename = report_js_filename
        js_session_path = "" # Assets are in the same folder
        
        # Embed Header Image (Base64) to avoid path issues
        header_bg_style = "background-color: #333;" # Fallback
        try:
            node_dir = os.path.dirname(os.path.realpath(__file__))
            img_path = os.path.join(node_dir, "pablo-4k.jpeg")
            if os.path.exists(img_path):
                with open(img_path, "rb") as img_f:
                    img_data = img_f.read()
                    b64_img = base64.b64encode(img_data).decode('utf-8')
                    header_bg_style = f"background: url('data:image/jpeg;base64,{b64_img}') center/cover no-repeat;"
                    print(f"[SceneGen] Header image loaded: {len(img_data)} bytes from {img_path}")
            else:
                print(f"[SceneGen] Header image not found at: {img_path}")
        except Exception as e:
            print(f"[SceneGen] Failed to embed header image: {e}")

        report_state = {
            "status": "Initializing...",
            "progress": 0,
            "logs": ["Process Started."],
            "costs": usage_stats,
            "assets": [],
            "videos": [],
            "session_path": js_session_path,
            "final_video": None,
            "montage": [],
            "style": {},
            "palette": {},
            "analysis": "",
            "edl_file": None
        }

        def update_report(status=None, progress=None, log=None):
            if status: report_state["status"] = status
            if progress is not None: report_state["progress"] = progress
            if log: 
                print(f"[SceneGen] {log}")
                report_state["logs"].append(f"[{time.strftime('%H:%M:%S')}] {log}")
            
            # Update costs in state
            report_state["costs"] = usage_stats
            
            try:
                # Write as JSONP / JS function call to bypass CORS
                json_str = json.dumps(report_state, indent=2)
                js_content = f"receiveData({json_str});"
                with open(report_js_path, "w", encoding="utf-8") as f:
                    f.write(js_content)
            except Exception as e:
                print(f"Report Update Error: {e}")
        
        def check_cancelled():
            if self.cancel_event.is_set():
                update_report("Cancelled", report_state.get("progress", 0), "Generation cancelled by user")
                raise InterruptedError("Generation cancelled by user")

        # Create HTML File IMMEDIATELY
        html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scene Gen Live Status - {prefix}</title>
    <style>
        body {{ font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; background-color: #121212; color: #e0e0e0; margin: 0; padding: 0; }}
        .container {{ max-width: 1400px; margin: 0 auto; padding: 20px; }}
        
        /* Header */
        header {{ {header_bg_style} height: 350px; position: relative; border-radius: 0 0 15px 15px; overflow: hidden; box-shadow: 0 4px 20px rgba(0,0,0,0.5); }}
        header::after {{ content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(to bottom, rgba(0,0,0,0.1), #121212); }}
        .header-content {{ position: absolute; bottom: 20px; left: 20px; z-index: 10; display: flex; justify-content: space-between; align-items: flex-end; width: calc(100% - 40px); }}
        .title-group h1 {{ margin: 0; font-size: 2.5rem; text-shadow: 0 2px 4px rgba(0,0,0,0.8); }}
        .title-group .subtitle {{ font-size: 1.1rem; opacity: 0.8; }}
        .gh-links {{ display: flex; gap: 10px; }}
        .gh-btn {{ background: rgba(255,255,255,0.1); backdrop-filter: blur(5px); color: white; text-decoration: none; padding: 8px 15px; border-radius: 20px; font-size: 0.9rem; border: 1px solid rgba(255,255,255,0.2); transition: all 0.2s; display: flex; align-items: center; gap: 5px; }}
        .gh-btn:hover {{ background: rgba(255,255,255,0.2); transform: translateY(-2px); }}
        
        /* Status Bar */
        .status-bar {{ background: #1e1e1e; padding: 15px; margin-top: 20px; border-radius: 10px; display: flex; align-items: center; justify-content: space-between; box-shadow: 0 2px 10px rgba(0,0,0,0.2); }}
        .status-text {{ font-size: 1.2rem; font-weight: bold; color: #4caf50; }}
        .progress-container {{ flex-grow: 1; margin: 0 20px; background: #333; height: 10px; border-radius: 5px; overflow: hidden; }}
        .progress-bar {{ height: 100%; background: #4caf50; width: 0%; transition: width 0.5s ease; }}
        
        /* Layout & Active Stage */
        .grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 20px; }}
        .card {{ background: #1e1e1e; border-radius: 10px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.2); display: flex; flex-direction: column; transition: all 0.3s ease; border: 1px solid transparent; position: relative; }}
        .card h3 {{ margin-top: 0; border-bottom: 1px solid #333; padding-bottom: 10px; display: flex; align-items: center; justify-content: flex-start; gap: 10px; }}
        
        .active-stage {{ border-color: #4caf50; box-shadow: 0 0 15px rgba(76, 175, 80, 0.3); transform: translateY(-2px); }}
        .active-stage h3 {{ color: #4caf50; }}
        
        /* Final Video */
        .final-video-container {{ width: 100%; aspect-ratio: 16/9; background: #000; border-radius: 5px; overflow: hidden; display: flex; align-items: center; justify-content: center; }}
        .final-video-container video {{ width: 100%; height: 100%; }}
        .placeholder-text {{ color: #555; }}

        /* Timeline */
        .timeline-wrapper {{ overflow-x: auto; padding-bottom: 10px; }}
        .timeline {{ display: flex; height: 150px; background: #222; border-radius: 5px; position: relative; min-width: 100%; }}
        .timeline-segment {{ position: relative; height: 100%; border-right: 1px solid #444; overflow: hidden; transition: all 0.3s; min-width: 50px; cursor: pointer; }}
        .timeline-segment:hover {{ filter: brightness(1.3); transform: scale(1.02); z-index: 10; box-shadow: 0 4px 8px rgba(0,0,0,0.5); }}
        .timeline-segment img {{ width: 100%; height: 100%; object-fit: cover; object-position: center; }}
        .timeline-segment .seg-label {{ position: absolute; bottom: 5px; left: 5px; font-size: 0.7rem; background: rgba(0,0,0,0.8); padding: 3px 6px; border-radius: 3px; pointer-events: none; color: #fff; font-weight: bold; }}
        .timeline-segment .seg-time {{ position: absolute; top: 5px; right: 5px; font-size: 0.65rem; background: rgba(76,175,80,0.8); color: white; padding: 2px 5px; border-radius: 3px; pointer-events: none; font-weight: bold; }}
        .timeline-segment .seg-info {{ position: absolute; bottom: 25px; left: 5px; right: 5px; font-size: 0.6rem; background: rgba(0,0,0,0.7); padding: 2px 4px; border-radius: 2px; color: #ccc; max-height: 0; overflow: hidden; transition: max-height 0.3s; }}
        .timeline-segment:hover .seg-info {{ max-height: 60px; }}

        /* Interactive JSON List */
        .data-display {{ font-size: 0.9rem; color: #ccc; max-height: 300px; overflow-y: auto; font-family: 'Consolas', monospace; }}
        .json-list {{ list-style: none; padding: 0; margin: 0; }}
        .json-item {{ padding: 5px 0; border-bottom: 1px solid #333; display: flex; flex-direction: column; }}
        .json-row {{ display: flex; align-items: baseline; }}
        .json-key {{ color: #888; font-weight: bold; margin-right: 10px; min-width: 120px; cursor: pointer; transition: color 0.2s; display: flex; align-items: center; }}
        .json-key:hover {{ color: #4caf50; }}
        .json-key::after {{ content: 'üìã'; font-size: 0.8em; margin-left: 5px; opacity: 0; transition: opacity 0.2s; }}
        .json-key:hover::after {{ opacity: 1; }}
        .json-val {{ color: #ddd; word-break: break-word; flex: 1; }}
        .json-sublist {{ margin-left: 20px; border-left: 2px solid #333; padding-left: 10px; margin-top: 5px; }}
        
        .palette-container {{ display: flex; flex-wrap: wrap; gap: 10px; margin-top: 10px; }}
        .color-swatch {{ width: 60px; height: 60px; border-radius: 8px; display: flex; align-items: center; justify-content: center; font-size: 0.7rem; color: #fff; text-shadow: 0 1px 2px rgba(0,0,0,0.8); box-shadow: 0 2px 5px rgba(0,0,0,0.3); transition: transform 0.2s; cursor: pointer; }}
        .color-swatch:hover {{ transform: scale(1.1); z-index: 10; }}

        /* Logs & Galleries */
        .log-box {{ background: #000; font-family: 'Consolas', monospace; padding: 10px; height: 200px; overflow-y: auto; border-radius: 5px; font-size: 0.85rem; color: #aaa; }}
        .log-entry {{ margin-bottom: 5px; border-bottom: 1px solid #222; padding-bottom: 2px; }}
        
        .gallery {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); gap: 15px; max-height: 400px; overflow-y: auto; padding: 5px; }}
        .gallery-item {{ position: relative; border-radius: 8px; overflow: hidden; aspect-ratio: 16/9; background: #333; transition: all 0.3s; cursor: zoom-in; box-shadow: 0 2px 5px rgba(0,0,0,0.3); }}
        .gallery-item:hover {{ transform: scale(1.05); z-index: 5; box-shadow: 0 8px 20px rgba(0,0,0,0.6); }}
        .gallery-item img, .gallery-item video {{ width: 100%; height: 100%; object-fit: cover; }}
        .gallery-item .label {{ position: absolute; bottom: 0; left: 0; right: 0; background: linear-gradient(transparent, rgba(0,0,0,0.9)); font-size: 0.7rem; padding: 15px 5px 5px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; font-weight: 500; }}
        .gallery-item .badge {{ position: absolute; top: 5px; left: 5px; background: rgba(76,175,80,0.9); color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.6rem; font-weight: bold; }}
        
        .cost-table {{ width: 100%; border-collapse: collapse; font-size: 0.9rem; }}
        .cost-table td, .cost-table th {{ padding: 8px; border-bottom: 1px solid #333; text-align: left; }}
        .total-cost {{ font-size: 1.5rem; color: #ffeb3b; text-align: right; margin-top: 10px; font-weight: bold; }}
        
        .footer {{ text-align: center; margin-top: 40px; padding: 20px; border-top: 1px solid #333; }}
        .btn-coffee {{ display: inline-block; transition: transform 0.2s; }}
        .btn-coffee:hover {{ transform: scale(1.05); }}

        /* Loaders */
        .loading-placeholder {{ display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100%; min-height: 150px; color: #666; }}
        .spinner {{ width: 40px; height: 40px; border: 4px solid rgba(255,255,255,0.1); border-left-color: #4caf50; border-radius: 50%; animation: spin 1s linear infinite; margin-bottom: 10px; }}
        @keyframes spin {{ 0% {{ transform: rotate(0deg); }} 100% {{ transform: rotate(360deg); }} }}

        /* Tooltips & Overlays */
        .gallery-item .overlay {{ position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: rgba(0,0,0,0.6); opacity: 0; transition: opacity 0.2s; display: flex; align-items: center; justify-content: center; }}
        .gallery-item:hover .overlay {{ opacity: 1; }}
        .download-icon {{ color: white; font-size: 1.5rem; text-decoration: none; background: rgba(0,0,0,0.5); padding: 10px; border-radius: 50%; transition: transform 0.2s; }}
        .download-icon:hover {{ transform: scale(1.1); background: #4caf50; }}
        
        .btn-action {{ background: #4caf50; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; font-weight: bold; text-decoration: none; display: inline-block; margin-top: 10px; }}
        .btn-action:hover {{ background: #45a049; }}
        .btn-action.disabled {{ background: #333; color: #555; cursor: not-allowed; pointer-events: none; }}
        
        .btn-cancel {{ background: #f44336; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; font-weight: bold; font-size: 0.9rem; transition: all 0.2s; }}
        .btn-cancel:hover {{ background: #da190b; transform: scale(1.05); }}
        .btn-cancel.hidden {{ display: none; }}

        /* Help System */
        .card {{ position: relative; }}
        .help-btn {{ position: absolute; top: 15px; right: 15px; width: 24px; height: 24px; border-radius: 50%; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.3); color: #aaa; font-size: 0.8rem; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s; z-index: 20; }}
        .help-btn:hover {{ background: #4caf50; color: white; border-color: #4caf50; }}
        
        .copy-btn {{ cursor: pointer; margin-right: 8px; font-size: 1rem; opacity: 0.7; transition: opacity 0.2s; background: none; border: none; color: #ccc; padding: 0; }}
        .copy-btn:hover {{ opacity: 1; color: #fff; }}

        .palette-item {{ display: flex; flex-direction: column; align-items: center; width: 100px; margin-bottom: 10px; cursor: pointer; transition: transform 0.2s; }}
        .palette-item:hover {{ transform: scale(1.05); }}
        .palette-swatch {{ width: 100%; height: 60px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.3); margin-bottom: 5px; }}
        .palette-info {{ font-size: 0.7rem; text-align: center; color: #aaa; line-height: 1.2; }}
        .palette-hex {{ font-weight: bold; color: #fff; margin-bottom: 2px; }}
        
        /* Modal */
        .modal {{ display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0,0,0,0.8); backdrop-filter: blur(5px); }}
        .modal-content {{ background-color: #1e1e1e; margin: 10% auto; padding: 30px; border: 1px solid #333; width: 80%; max-width: 600px; border-radius: 10px; box-shadow: 0 5px 30px rgba(0,0,0,0.5); position: relative; }}
        .close-modal {{ color: #aaa; float: right; font-size: 28px; font-weight: bold; cursor: pointer; }}
        .close-modal:hover {{ color: white; }}
        .modal h2 {{ margin-top: 0; color: #4caf50; border-bottom: 1px solid #333; padding-bottom: 10px; }}
        .modal p {{ line-height: 1.6; color: #ccc; }}
        
        /* Lightbox */
        .lightbox {{ display: none; position: fixed; z-index: 2000; left: 0; top: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.95); backdrop-filter: blur(10px); }}
        .lightbox-content {{ position: relative; margin: auto; padding: 20px; max-width: 95vw; max-height: 95vh; display: flex; align-items: center; justify-content: center; height: 100%; }}
        .lightbox-content img, .lightbox-content video {{ max-width: 100%; max-height: 90vh; object-fit: contain; border-radius: 5px; box-shadow: 0 10px 50px rgba(0,0,0,0.8); }}
        .lightbox-close {{ position: absolute; top: 20px; right: 40px; color: white; font-size: 40px; font-weight: bold; cursor: pointer; z-index: 2001; transition: color 0.2s; }}
        .lightbox-close:hover {{ color: #f44336; }}
        
        /* Lightbox Navigation */
        .nav-btn {{ position: absolute; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.5); color: white; border: none; width: 50px; height: 50px; border-radius: 50%; font-size: 24px; cursor: pointer; transition: all 0.2s; display: flex; align-items: center; justify-content: center; user-select: none; }}
        .nav-btn:hover {{ background: rgba(76,175,80,0.8); transform: translateY(-50%) scale(1.1); }}
        .nav-prev {{ left: 20px; }}
        .nav-next {{ right: 20px; }}
        
        .lightbox-caption {{ position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%); color: white; background: rgba(0,0,0,0.7); padding: 10px 20px; border-radius: 5px; font-size: 1rem; max-width: 80%; text-align: center; }}

    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="title-group">
                    <h1>Scene Gen Status</h1>
                    <div class="subtitle">Project: {prefix}</div>
                </div>
                <div class="gh-links">
                    <a href="https://www.youtube.com/channel/UCHotSlu3SDgp35lat92ExIQ" target="_blank" class="gh-btn" style="background: rgba(255,0,0,0.2); border-color: rgba(255,0,0,0.4);">üì∫ YouTube</a>
                    <a href="https://github.com/lazniak/scene_gen" target="_blank" class="gh-btn">‚≠ê Star on GitHub</a>
                    <a href="https://github.com/lazniak" target="_blank" class="gh-btn">üë§ Follow @lazniak</a>
                </div>
            </div>
        </header>
        
        <div class="status-bar">
            <div id="status" class="status-text">Initializing...</div>
            <div class="progress-container">
                <div id="progress" class="progress-bar"></div>
            </div>
            <div id="progress-text">0%</div>
            <button id="btn-cancel" class="btn-cancel" onclick="cancelGeneration()">‚èπ Cancel</button>
        </div>
        
        <!-- Final Video Section -->
        <div id="card-s11" class="card" style="margin-top: 20px;">
            <button class="help-btn" onclick="showHelp('final_video')">?</button>
            <h3>
                Final Video Output
                <a id="btn-dl-video" href="#" class="btn-action disabled" download style="margin-left: auto;">Download Video</a>
            </h3>
            <div class="final-video-container" id="final-video-box">
                <div class="loading-placeholder">
                    <div class="spinner"></div>
                    <div>Waiting for video generation...</div>
                </div>
            </div>
        </div>

        <!-- Timeline Section -->
        <div id="card-timeline" class="card" style="margin-top: 20px;">
            <button class="help-btn" onclick="showHelp('timeline')">?</button>
            <h3>
                <button class="copy-btn" onclick="copyData('montage')" title="Copy Montage JSON">üìã</button>
                Montage Timeline
                <a id="btn-dl-edl" href="#" class="btn-action disabled" download style="margin-left: auto;">Download EDL</a>
            </h3>
            <div style="display: flex; align-items: center; gap: 10px; padding: 0 15px 10px 15px;">
                <label for="timeline-zoom" style="font-size: 0.8rem; color: #aaa;">Zoom:</label>
                <input type="range" id="timeline-zoom" min="10" max="300" value="50" style="width: 150px; cursor: pointer;">
            </div>
            <div class="timeline-wrapper">
                <div id="timeline" class="timeline">
                    <div style="width:100%; display:flex; align-items:center; justify-content:center; color:#555;">
                        <div class="loading-placeholder" style="min-height:auto;">
                            <div class="spinner" style="width:20px; height:20px; border-width:2px;"></div>
                            <div style="font-size:0.8rem;">Waiting for montage plan...</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="grid">
            <!-- Style & Analysis -->
            <div id="card-s1" class="card">
                <button class="help-btn" onclick="showHelp('analysis')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('analysis')" title="Copy Analysis">üìã</button>
                    Audio Analysis (S1)
                </h3>
                <div id="analysis-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Analyzing audio...</div>
                    </div>
                </div>
            </div>

            <div id="card-s2" class="card">
                <button class="help-btn" onclick="showHelp('style')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('style')" title="Copy Style Data">üìã</button>
                    Style Definition (S2)
                </h3>
                <div id="style-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Defining visual style...</div>
                    </div>
                </div>
            </div>

            <!-- Narrative Planning -->
            <div id="card-s3" class="card">
                <button class="help-btn" onclick="showHelp('narrative')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('narrative')" title="Copy Narrative Data">üìã</button>
                    Narrative & Asset Planning (S3)
                </h3>
                <div id="narrative-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Planning storyline and assets...</div>
                    </div>
                </div>
            </div>
            
            <!-- Color Palette -->
            <div id="card-s3-palette" class="card">
                <button class="help-btn" onclick="showHelp('palette')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('palette')" title="Copy Palette JSON">üìã</button>
                    Color Palette (S3)
                </h3>
                <div id="palette-display" class="palette-container" style="min-height:100px; align-items:center; justify-content:center;">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Generating palette...</div>
                    </div>
                </div>
            </div>
        </div>

        <div class="grid">
            <!-- Prompts -->
            <div id="card-s7" class="card">
                <button class="help-btn" onclick="showHelp('prompts')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('prompts')" title="Copy Prompts">üìã</button>
                    Scene Prompts (S7)
                </h3>
                <div id="prompts-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Generating prompts...</div>
                    </div>
                </div>
            </div>

            <!-- Motion -->
            <div id="card-s8" class="card">
                <button class="help-btn" onclick="showHelp('motion')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('motion')" title="Copy Motion Data">üìã</button>
                    Motion Logic (S8)
                </h3>
                <div id="motion-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Refining motion...</div>
                    </div>
                </div>
            </div>

            <!-- Costs -->
            <div id="card-costs" class="card">
                <button class="help-btn" onclick="showHelp('costs')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('costs')" title="Copy Cost Data">üìã</button>
                    Estimated Costs
                </h3>
                <div id="costs">Calculating...</div>
            </div>
        </div>
        
        <div id="dynamic-assets-container" class="grid">
            <!-- Dynamic Asset Categories will be injected here -->
            <div id="card-assets" class="card">
                <button class="help-btn" onclick="showHelp('assets')">?</button>
                <h3>Generated Assets</h3>
                <div id="assets-gallery" class="gallery"></div>
            </div>
        </div>
        
        <div class="grid">
            <div id="card-segments" class="card">
                <button class="help-btn" onclick="showHelp('segments')">?</button>
                <h3>Video Segments</h3>
                <div id="videos-gallery" class="gallery"></div>
            </div>
             <div id="card-logs" class="card">
                <button class="help-btn" onclick="showHelp('logs')">?</button>
                <h3>Live Logs</h3>
                <div id="logs" class="log-box"></div>
            </div>
        </div>
        
        <div class="footer">
            <p>Enjoying Scene Gen? Support the development!</p>
            <a href="https://www.buymeacoffee.com/EYB8tkx3tO" class="btn-coffee" target="_blank">
                <img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" >
            </a>
        </div>
    </div>

    <!-- Help Modal -->
    <div id="helpModal" class="modal">
        <div class="modal-content">
            <span class="close-modal" onclick="closeHelp()">&times;</span>
            <h2 id="helpTitle">Help</h2>
            <div id="helpContent"></div>
        </div>
    </div>
    
    <!-- Lightbox -->
    <div id="lightbox" class="lightbox" onclick="closeLightbox(event)">
        <span class="lightbox-close" onclick="closeLightbox(event)">&times;</span>
        <button class="nav-btn nav-prev" onclick="changeSlide(-1, event)">&#10094;</button>
        <button class="nav-btn nav-next" onclick="changeSlide(1, event)">&#10095;</button>
        <div class="lightbox-content" id="lightbox-content">
            <!-- Content injected via JS -->
        </div>
        <div id="lightbox-caption" class="lightbox-caption"></div>
    </div>

    <script>
        const jsPath = '{js_filename}';
        
        let currentData = {{}};
        let galleryItems = [];
        let currentSlideIndex = 0;

        // Global data receiver for JSONP
        window.receiveData = function(data) {{
            window.statusData = data;
        }};

        // --- Interactive JSON Renderer ---
        function renderInteractiveJson(data, elementId) {{
            const container = document.getElementById(elementId);
            if (!container) return;
            if (!data) {{
                container.innerHTML = '<div style="color:#666; padding:10px;">No data available yet...</div>';
                return;
            }}
            
            let html = '<ul class="json-list">';
            
            function buildList(obj) {{
                let listHtml = '';
                for (const [key, value] of Object.entries(obj)) {{
                    listHtml += '<li class="json-item">';
                    listHtml += '<div class="json-row">';
                    // Key with copy functionality
                    listHtml += `<span class="json-key" onclick="copyText('${{key}}')" title="Click to copy key">${{key}}:</span>`;
                    
                    if (typeof value === 'object' && value !== null) {{
                        listHtml += '<span class="json-val">[Object/Array]</span>';
                        listHtml += '</div>';
                        listHtml += '<div class="json-sublist"><ul class="json-list">';
                        listHtml += buildList(value);
                        listHtml += '</ul></div>';
                    }} else {{
                        let displayVal = value;
                        if (typeof value === 'string' && value.startsWith('http')) {{
                            displayVal = `<a href="${{value}}" target="_blank" style="color:#4caf50;">Link</a>`;
                        }}
                        listHtml += `<span class="json-val">${{displayVal}}</span>`;
                        listHtml += '</div>';
                    }}
                    listHtml += '</li>';
                }}
                return listHtml;
            }}
            
            html += buildList(data);
            html += '</ul>';
            container.innerHTML = html;
        }}

        function copyText(text) {{
            navigator.clipboard.writeText(text).then(() => {{
                console.log('Copied:', text);
            }});
        }}

        // --- Main Update Loop ---
        function updateStatus() {{
            const script = document.createElement('script');
            script.src = jsPath + '?t=' + new Date().getTime();
            
            script.onload = () => {{
                if (window.statusData) {{
                    currentData = window.statusData;
                    renderUI(window.statusData);
                }}
                // Clean up
                try {{ document.body.removeChild(script); }} catch(e) {{}}
            }};
            
            script.onerror = () => {{
                console.log("Waiting for data... (file might not exist yet)");
                try {{ document.body.removeChild(script); }} catch(e) {{}}
            }};
            
            document.body.appendChild(script);
        }}

        function renderUI(data) {{
            // 1. Status Bar
            const statusEl = document.getElementById('status');
            if(statusEl) statusEl.textContent = data.status || 'Idle';
            
            const progEl = document.getElementById('progress');
            if(progEl) progEl.style.width = (data.progress || 0) + '%';
            
            const progText = document.getElementById('progress-text');
            if(progText) progText.textContent = (data.progress || 0) + '%';
            
            if (data.done || data.error) {{
                const btnCancel = document.getElementById('btn-cancel');
                if(btnCancel) btnCancel.classList.add('hidden');
            }}

            // 2. Active Stage Highlighting
            document.querySelectorAll('.card').forEach(el => el.classList.remove('active-stage'));
            const stageMap = {{
                'Stage 1': 'card-s1',
                'Stage 2': 'card-s2',
                'Stage 3': 'card-s3',
                'Stage 4': 'card-s3-palette',
                'Stage 5': 'card-assets',
                'Stage 6': 'card-timeline',
                'Stage 7': 'card-s7',
                'Stage 8': 'card-s8',
                'Stage 9': 'card-timeline',
                'Stage 10': 'card-s11',
                'Stage 11': 'card-s11'
            }};
            
            const statusText = data.status || "";
            for (const [key, id] of Object.entries(stageMap)) {{
                if (statusText.includes(key)) {{
                    const el = document.getElementById(id);
                    if (el) el.classList.add('active-stage');
                    break;
                }}
            }}

            // 3. Render Data Sections
            renderInteractiveJson(data.analysis, 'analysis-display');
            renderInteractiveJson(data.style, 'style-display');
            renderInteractiveJson(data.narrative, 'narrative-display');
            renderInteractiveJson(data.prompts, 'prompts-display');
            renderInteractiveJson(data.motion, 'motion-display');
            
            // 4. Palette
            if (data.palette) {{
                const pContainer = document.getElementById('palette-display');
                if(pContainer) {{
                    let pHtml = '';
                    if (data.palette.colors) {{
                        data.palette.colors.forEach(c => {{
                            pHtml += `<div class="palette-item" onclick="copyText('${{c.hex}}')">
                                <div class="palette-swatch" style="background-color: ${{c.hex}}"></div>
                                <div class="palette-hex">${{c.hex}}</div>
                                <div class="palette-info">${{c.description || ''}}</div>
                            </div>`;
                        }});
                    }}
                    pContainer.innerHTML = pHtml || '<div style="padding:10px; color:#666;">No palette data</div>';
                }}
            }}

            // 5. Timeline
            renderTimeline(data);

            // 6. Galleries
            renderGallery(data.assets, 'assets-gallery', 'asset');
            renderGallery(data.videos, 'videos-gallery', 'video');
            
            // 7. Final Video
            if (data.final_video) {{
                const vContainer = document.getElementById('final-video-box');
                if (vContainer && !vContainer.querySelector('video')) {{
                    vContainer.innerHTML = `<video controls src="${{data.final_video}}" poster="${{data.final_poster || ''}}" style="width:100%"></video>`;
                    const btn = document.getElementById('btn-dl-video');
                    if(btn) {{
                        btn.href = data.final_video;
                        btn.classList.remove('disabled');
                    }}
                }}
            }}

            // 8. Logs
            if (data.logs) {{
                const logBox = document.getElementById('logs');
                if(logBox) {{
                    logBox.innerHTML = data.logs.map(l => `<div class="log-entry">${{l}}</div>`).join('');
                    requestAnimationFrame(() => {{
                        logBox.scrollTop = logBox.scrollHeight;
                    }});
                }}
            }}
            
            // 9. Costs
            if (data.cost) {{
                const costsEl = document.getElementById('costs');
                if(costsEl) {{
                    costsEl.innerHTML = `
                        <table class="cost-table">
                            <tr><th>Model</th><th>Count</th><th>Cost</th></tr>
                            ${{Object.entries(data.cost.breakdown || {{}}).map(([k, v]) => `<tr><td>${{k}}</td><td>${{v.count}}</td><td>$${{v.cost.toFixed(4)}}</td></tr>`).join('')}}
                        </table>
                        <div class="total-cost">Total: $${{data.cost.total.toFixed(4)}}</div>
                    `;
                }}
            }}
        }}

        function renderTimeline(data) {{
            const tContainer = document.getElementById('timeline');
            if(!tContainer) return;
            
            const scenes = data.montage || data.prompts || [];
            if (!scenes || scenes.length === 0) return;

            let html = '';
            const zoomEl = document.getElementById('timeline-zoom');
            const zoom = zoomEl ? zoomEl.value : 50;
            
            scenes.forEach((scene, idx) => {{
                const duration = scene.duration || 5;
                const width = duration * (zoom / 5); 
                
                const img = scene.image_file || scene.image || '';
                const label = `Scene ${{idx + 1}}`;
                const time = `${{duration}}s`;
                const desc = scene.description || scene.visual || "No description";

                html += `<div class="timeline-segment" style="width: ${{width}}px; flex-shrink: 0;" onclick="openLightbox('timeline', ${{idx}})">
                    ${{img ? `<img src="${{img}}" loading="lazy">` : '<div style="width:100%;height:100%;background:#333;"></div>'}}
                    <div class="seg-label">${{label}}</div>
                    <div class="seg-time">${{time}}</div>
                    <div class="seg-info">${{desc}}</div>
                </div>`;
            }});
            tContainer.innerHTML = html;
        }}

        function renderGallery(items, containerId, type) {{
            const container = document.getElementById(containerId);
            if (!container) return;
            if (!items || items.length === 0) return;
            
            if (container.children.length === items.length) return;

            let html = '';
            items.forEach((item, idx) => {{
                const src = item.file || item.url;
                const thumb = item.thumbnail || src;
                const label = item.name || `Item ${{idx+1}}`;
                
                html += `<div class="gallery-item" onclick="openLightbox('${{type}}', ${{idx}})">
                    ${{type === 'video' || src.endsWith('.mp4') 
                        ? `<video src="${{src}}" muted onmouseover="this.play()" onmouseout="this.pause();this.currentTime=0;"></video>`
                        : `<img src="${{thumb}}" loading="lazy">`
                    }}
                    <div class="badge">${{type.toUpperCase()}}</div>
                    <div class="label">${{label}}</div>
                    <div class="overlay">
                        <a href="${{src}}" download class="download-icon" onclick="event.stopPropagation()">‚¨á</a>
                    </div>
                </div>`;
            }});
            container.innerHTML = html;
        }}

        // --- Lightbox ---
        function openLightbox(context, index) {{
            const lightbox = document.getElementById('lightbox');
            
            // Collect items
            if (context === 'timeline') {{
                galleryItems = (currentData.montage || currentData.prompts || []).map(s => ({{
                    src: s.image_file || s.image,
                    type: 'image',
                    caption: s.visual || s.description
                }}));
            }} else if (context === 'asset') {{
                galleryItems = (currentData.assets || []).map(a => ({{
                    src: a.file,
                    type: 'image',
                    caption: a.name
                }}));
            }} else if (context === 'video') {{
                galleryItems = (currentData.videos || []).map(v => ({{
                    src: v.file,
                    type: 'video',
                    caption: v.name
                }}));
            }}

            currentSlideIndex = index;
            showSlide(index);
            lightbox.style.display = 'block';
        }}

        function showSlide(index) {{
            if (index < 0) index = galleryItems.length - 1;
            if (index >= galleryItems.length) index = 0;
            currentSlideIndex = index;
            
            const item = galleryItems[index];
            const content = document.getElementById('lightbox-content');
            const caption = document.getElementById('lightbox-caption');
            
            if (item.type === 'video' || item.src.endsWith('.mp4')) {{
                content.innerHTML = `<video src="${{item.src}}" controls autoplay style="max-width:100%; max-height:90vh;"></video>`;
            }} else {{
                content.innerHTML = `<img src="${{item.src}}" style="max-width:100%; max-height:90vh;">`;
            }}
            caption.textContent = `${{index + 1}}/${{galleryItems.length}}: ${{item.caption || ''}}`;
        }}

        function changeSlide(n, event) {{
            if(event) event.stopPropagation();
            showSlide(currentSlideIndex + n);
        }}

        function closeLightbox(event) {{
            if (event.target.id === 'lightbox' || event.target.classList.contains('lightbox-close')) {{
                document.getElementById('lightbox').style.display = 'none';
                document.getElementById('lightbox-content').innerHTML = '';
            }}
        }}
        
        function showHelp(section) {{
            const modal = document.getElementById('helpModal');
            const title = document.getElementById('helpTitle');
            const content = document.getElementById('helpContent');
            
            const helpData = {{
                'analysis': 'Stage 1: Gemini analyzes the audio file...',
                'style': 'Stage 2: Visual style definition...',
                'narrative': 'Stage 3: Narrative structure...',
                'palette': 'Stage 3: Color palette...',
                'prompts': 'Stage 7: Image prompts...',
                'motion': 'Stage 8: Motion instructions...',
                'timeline': 'Timeline of scenes...',
                'assets': 'Generated assets...',
                'segments': 'Video segments...',
                'final_video': 'Final stitched video...',
                'logs': 'Process logs...'
            }};
            
            title.textContent = section.charAt(0).toUpperCase() + section.slice(1) + ' Help';
            content.textContent = helpData[section] || 'No help available.';
            modal.style.display = 'block';
        }}
        
        function closeHelp() {{
            document.getElementById('helpModal').style.display = 'none';
        }}

        document.addEventListener('keydown', function(e) {{
            if (document.getElementById('lightbox').style.display === 'block') {{
                if (e.key === 'ArrowLeft') changeSlide(-1);
                if (e.key === 'ArrowRight') changeSlide(1);
                if (e.key === 'Escape') document.getElementById('lightbox').style.display = 'none';
            }}
        }});

        // Init
        setInterval(updateStatus, 2000);
        updateStatus();
    </script>
</body>
</html>
"""        
        try:
            with open(report_path, "w", encoding="utf-8") as f: f.write(html_content)
            print(f"[SceneGen] Created report at: {report_path}")
            
            # Initial write of JSON data
            update_report(status="Starting Process...", progress=0)
            
            if open_report:
                print(f"[SceneGen] Opening report in browser...")
                webbrowser.open(f"file://{os.path.abspath(report_path)}", new=2)
        except Exception as e:
            print(f"[SceneGen] Failed to create/open report: {e}")

        def track_text(input_str, output_str):
            # Rough estimate: 1 token ~= 4 chars
            usage_stats["gemini_text_input_tokens"] += len(str(input_str)) // 4
            usage_stats["gemini_text_output_tokens"] += len(str(output_str)) // 4
            # No need to update report on every token count, just at key stages

        # --- Setup ---
        # Configure safety settings to avoid false positives (PROHIBITED_CONTENT)
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        model_text = genai.GenerativeModel(model_text_name, safety_settings=safety_settings)
        model_image_gen = genai.GenerativeModel(model_image_name, safety_settings=safety_settings)
        
        # Save original audio
        original_audio_path = os.path.join(session_dir, "original_audio.wav")
        scipy.io.wavfile.write(original_audio_path, sample_rate, audio_data)
        total_duration_sec = len(audio_data) / sample_rate
        
        # Resolution Setup
        ar_map = {"16:9": (1280, 720), "1:1": (1024, 1024), "9:16": (720, 1280), "4:3": (1024, 768)}
        base_w, base_h = ar_map.get(aspect_ratio, (1280, 720))
        img_w = int(base_w * resolution_multiplier)
        img_h = int(base_h * resolution_multiplier)
        print(f"[SceneGen] Resolution set to {img_w}x{img_h} ({aspect_ratio})")
        
        # Temp audio for Gemini
        with open(original_audio_path, "rb") as f:
            audio_bytes = f.read()

        available_models = []
        if use_wan_fast: available_models.append("wan-video/wan-2.5-i2v-fast")
        if use_wan_2_5: available_models.append("wan-video/wan-2.5-i2v")
        if use_kling_turbo: available_models.append("kwaivgi/kling-v2.5-turbo-pro")
        if use_omni_human: available_models.append("bytedance/omni-human-1.5")
        if use_hailuo: available_models.append("minimax/hailuo-2.3")
        if use_hailuo_fast: available_models.append("minimax/hailuo-2.3-fast")
        if use_veo_3_1: available_models.append("google/veo-3.1")
        if use_veo_3_1_fast: available_models.append("google/veo-3.1-fast")
        
        if not available_models and render_mode == "Full Render":
            print("[SceneGen] No video models selected. Will generate images only (slideshow mode).")
        elif not available_models and render_mode == "Prompt Mode":
            print("[SceneGen] Prompt Mode: Skipping all image and video generation.") 

        # --- STAGE 1: Audio Analysis ---
        check_cancelled()
        update_report("Stage 1: Analyzing Audio...", 5, "Sending audio to Gemini for analysis...")
        print("[SceneGen] Stage 1: Audio Analysis...")
        prompt_s1 = f"""
        CRITICAL TASK: Perform ULTRA-PRECISE audio analysis. This data will be used for exact video montage synchronization.
        
        Audio Duration: {total_duration_sec:.2f} seconds
        User Instruction: "{instruction}"
        
        REQUIREMENTS (ALL MANDATORY):
        
        1. **WORD-LEVEL LYRICS ANALYSIS** (if vocals present):
           - Transcribe ALL lyrics with EXACT timestamps for EACH word or short phrase
           - Format: [{{"time_start": 0.5, "time_end": 1.2, "text": "word"}}, ...]
           - If no lyrics: return empty array
        
        2. **BEAT/RHYTHM DETECTION**:
           - Identify ALL major beats/drum hits with timestamps
           - Format: [{{"time": 1.5, "strength": "strong/medium/weak", "type": "kick/snare/cymbal"}}, ...]
           - Minimum 10 beat markers across the track
        
        3. **STRUCTURAL ANALYSIS**:
           - Identify PRECISE sections (Intro, Verse, Chorus, Bridge, Outro, etc.)
           - EACH section MUST have exact start/end timestamps
           - Format: [{{"name": "Intro", "start": 0.0, "end": 8.5, "description": "..."}}, ...]
           - Sections MUST cover the ENTIRE {total_duration_sec:.2f}s duration with NO GAPS
        
        4. **DRAMATURGY/DYNAMICS**:
           - Key moments of energy changes, emotional shifts
           - Format: [{{"time": 45.2, "event": "Build-up starts", "intensity": 7}}, ...]
        
        5. **METADATA**:
           - "genre": Music genre
           - "bpm": Estimated BPM (beats per minute)
           - "key": Musical key (if detectable)
           - "mood": Overall emotional tone
           - "characteristics": List of key musical traits
        
        Return VALID JSON:
        {{
            "genre": "...",
            "bpm": 120,
            "key": "C major",
            "mood": "...",
            "characteristics": ["...", "..."],
            "lyrics_timestamped": [{{"time_start": 0.0, "time_end": 0.5, "text": "..."}}],
            "beats": [{{"time": 0.5, "strength": "strong", "type": "kick"}}],
            "structure": [{{"name": "Intro", "start": 0.0, "end": 8.0, "description": "..."}}],
            "dramaturgy": [{{"time": 10.0, "event": "...", "intensity": 5}}]
        }}
        
        CRITICAL: Ensure structure sections cover EXACTLY 0.00s to {total_duration_sec:.2f}s with NO overlap or gaps.
        """
        # Note: We can't easily track input tokens for audio files without API metadata, 
        # but we can track the text prompt part.
        track_text(prompt_s1, "") 
        
        resp_s1 = await model_text.generate_content_async([prompt_s1, {"mime_type": "audio/wav", "data": audio_bytes}])
        debug_s1 = resp_s1.text
        track_text("", debug_s1)
        track_text("", debug_s1)
        with open(os.path.join(session_dir, "stage1_analysis.txt"), "w", encoding="utf-8") as f: f.write(debug_s1)
        try:
            report_state["analysis"] = json.loads(self._clean_json(debug_s1))
        except Exception as e:
            print(f"[SceneGen] Stage 1 JSON Parse Warning: {e}")
            report_state["analysis"] = debug_s1
        update_report("Stage 2: Defining Style...", 10, "Audio analysis complete.")
        
        # --- STAGE 2: Style Definition ---
        print("[SceneGen] Stage 2: Style Definition...")
        prompt_s2 = f"""
        Based on the audio analysis: {debug_s1}
        And user instruction: "{instruction}"
        
        Define a precise, universal visual style instruction for the video.
        Return a JSON object with:
        - "style_instruction": A comprehensive style description (e.g., "Cyberpunk noir with neon lighting and grainy film texture").
        """
        track_text(prompt_s2, "")
        resp_s2 = await model_text.generate_content_async(prompt_s2)
        debug_s2 = resp_s2.text
        track_text("", debug_s2)
        with open(os.path.join(session_dir, "stage2_style.json"), "w", encoding="utf-8") as f: f.write(debug_s2)
        
        try:
            style_instruction = json.loads(self._clean_json(debug_s2)).get("style_instruction", "")
        except json.JSONDecodeError as e:
            print(f"[SceneGen] JSON Error in Stage 2: {e}")
            print(f"[SceneGen] Raw output: {debug_s2[:200]}...")
            # Fallback: use raw text as style instruction
            style_instruction = debug_s2.strip()
            update_report("Stage 2 Warning", 12, "Failed to parse style JSON. Using raw text.")
        
        report_state["style"] = {"style_instruction": style_instruction}
        update_report(None, None, "Style defined.")

        # --- STAGE 3: Narrative & Asset Planning ---
        update_report("Stage 3: Narrative Planning...", 15, "Developing storyline and asset list...")
        print("[SceneGen] Stage 3: Narrative & Asset Planning...")
        
        prompt_s3_plan = f"""
        You are a visionary Director and Screenwriter. Based on the Audio Analysis and Style, create a comprehensive Narrative Plan and Asset List.
        
        Audio Analysis: {debug_s1}
        Style: {style_instruction}
        User Instruction: "{instruction}"
        
        TASK:
        1. Create a detailed Storyline that fits the audio's mood and lyrics.
        2. Define ALL necessary Assets (Actors, Props, Locations). NO LIMITS. If the story needs 5 side characters, define them.
        3. Plan dependencies: Props -> Actors (holding props) -> Locations (containing props).
        4. Define Variants: If an actor changes clothes, define a variant. If a location changes time/weather, define a variant.
        
        REQUIREMENTS:
        - "assets": A list of ALL assets.
          - "name": Unique ID (e.g., "Hero_John", "Prop_Sword", "Loc_Cave").
          - "category": "Actor", "Prop", or "Location".
          - "description": Visual description matching the Style.
          - "parent_asset": If this is a variant or depends on another asset (e.g., Actor holding Prop), name the parent.
        
        Return JSON:
        {{
            "storyline": "Detailed narrative...",
            "assets": [
                {{"name": "...", "category": "...", "description": "...", "parent_asset": null}},
                ...
            ]
        }}
        """
        track_text(prompt_s3_plan, "")
        resp_s3_plan = await model_text.generate_content_async([prompt_s3_plan, {"mime_type": "audio/wav", "data": audio_bytes}])
        debug_s3_plan = resp_s3_plan.text
        track_text("", debug_s3_plan)
        with open(os.path.join(session_dir, "stage3_narrative.json"), "w", encoding="utf-8") as f: f.write(debug_s3_plan)
        
        try:
            narrative_data = json.loads(self._clean_json(debug_s3_plan))
            planned_assets = narrative_data.get("assets", [])
            report_state["narrative"] = narrative_data  # Store full object for interactive display
        except json.JSONDecodeError as e:
            print(f"[SceneGen] JSON Error in Stage 3: {e}")
            planned_assets = []
            report_state["narrative"] = debug_s3_plan  # Fallback to raw text
            update_report("Stage 3 Warning", 17, "Failed to parse narrative JSON.")

        # --- STAGE 4: Color Palette ---
        print("[SceneGen] Stage 3: Color Palette...")
        prompt_s3 = f"""
        Based on the style: "{style_instruction}"
        And audio analysis: {debug_s1}
        
        Create a color palette.
        Return a JSON object with:
        - "palette": List of colors (HEX codes or descriptive names like "Electric Blue").
        - "lighting_mood": Description of the lighting atmosphere.
        """
        track_text(prompt_s3, "")
        resp_s3 = await model_text.generate_content_async(prompt_s3)
        debug_s3 = resp_s3.text
        track_text("", debug_s3)
        with open(os.path.join(session_dir, "stage3_palette.json"), "w", encoding="utf-8") as f: f.write(debug_s3)
        
        try:
            palette_data = json.loads(self._clean_json(debug_s3))
        except json.JSONDecodeError as e:
            print(f"[SceneGen] JSON Error in Stage 3: {e}")
            print(f"[SceneGen] Raw output: {debug_s3[:200]}...")
            # Fallback: default palette
            palette_data = {"palette": ["#1a1a1a", "#333333", "#4a4a4a", "#666666", "#808080"]}
            update_report("Stage 3 Warning", 17, "Failed to parse palette JSON. Using default colors.")
        
        report_state["palette"] = palette_data
        update_report(None, None, "Palette generated.")

        # --- STAGE 3.5: Reference Analysis ---
        print("[SceneGen] Stage 3.5: Reference Analysis...")
        ref_data = []
        asset_library = {} # Name -> Image
        
        # Output lists
        env_imgs = []
        prop_imgs = []
        actor_imgs = []

        if ref_images_pil:
            prompt_s3_5 = f"""
            Analyze the attached {len(ref_images_pil)} images in context of: "{instruction}" and Style: "{style_instruction}".
            For each image, identify its category (Environment, Prop, or Actor) and provide a descriptive name (unique_id) and visual description.
            Return JSON object with "references": list of objects:
            - "index": int (0-based index of the image provided)
            - "category": string ("Environment", "Prop", "Actor")
            - "description": string
            - "name": string (unique identifier)
            """
            try:
                req_content = [prompt_s3_5] + ref_images_pil
                track_text(prompt_s3_5, "")
                resp_s3_5 = await model_text.generate_content_async(req_content)
                track_text("", resp_s3_5.text)
                ref_data = json.loads(self._clean_json(resp_s3_5.text)).get("references", [])
                with open(os.path.join(session_dir, "stage3_5_references.json"), "w", encoding="utf-8") as f: json.dump(ref_data, f, indent=2)
                
                for r in ref_data:
                    idx = r.get("index")
                    if idx is not None and 0 <= idx < len(ref_images_pil):
                        img = ref_images_pil[idx]
                        cat = r.get("category", "").lower()
                        name = r.get("name", f"ref_{idx}")
                        asset_library[name] = img
                        if "env" in cat: env_imgs.append(img)
                        elif "prop" in cat: prop_imgs.append(img)
                        elif "actor" in cat: actor_imgs.append(img)
                        else: actor_imgs.append(img)
            except Exception as e:
                print(f"Stage 3.5 Error: {e}")

        # --- STAGE 5: Asset Generation ---
        update_report("Stage 5: Generating Assets...", 30, f"Creating {len(planned_assets)} assets...")
        print("[SceneGen] Stage 5: Asset Generation (Dependency-Aware)...")
        
        # Use planned assets from Stage 3
        new_assets_list = planned_assets
        
        # Sort assets:
        # 1. By Category Priority (Prop -> Actor -> Location)
        # 2. By Dependency (Parent before Child)
        priority = {"prop": 0, "actor": 1, "location": 2, "environment": 2}
        
        new_assets_list.sort(key=lambda x: (
            priority.get(x["category"].lower(), 3), 
            1 if x.get("parent_asset") else 0
        ))
        
        # Helper to generate assets
        async def gen_asset_task(asset_data):
            name = asset_data["name"]
            cat = asset_data["category"].lower()
            desc = asset_data["description"]
            parent_name = asset_data.get("parent_asset")
            
            suffix = ""
            if "actor" in cat: 
                suffix = "character sheet, face close-up, side profile, full body shot, neutral background, isolated, high detail, concept art"
            elif "prop" in cat: 
                suffix = "object sheet, isolated on white background, highly detailed, 3d render"
            elif "env" in cat or "location" in cat: 
                suffix = "wide shot, empty scene, no people, architectural photography, detailed environment"

            no_text = "NO TEXT, NO WORDS, NO LABELS on the image."
            full_prompt = f"{no_text} {style_instruction}. {desc}. {suffix}. {palette_data.get('lighting_mood', '')} --aspect {aspect_ratio}"
            
            input_parts = [full_prompt]
            
            # 1. Add Parent Asset (Direct Dependency)
            if parent_name and parent_name in asset_library:
                input_parts.append(asset_library[parent_name])
                print(f"  -> Generating {name} using parent {parent_name}")
            
            # 2. Add Contextual Assets (Props mentioned in description)
            # Scan description for names of existing assets (e.g. "holding Prop_Sword")
            # Only for Actors and Locations
            if "actor" in cat or "location" in cat or "env" in cat:
                for existing_name, existing_img in asset_library.items():
                    # Avoid self-reference and parent (already added)
                    if existing_name != name and existing_name != parent_name:
                        # Check if existing asset name is in description
                        # Use simple string matching. Ideally, we'd use regex or exact word match.
                        if existing_name in desc:
                            input_parts.append(existing_img)
                            print(f"  -> Generating {name} with context {existing_name}")
                            # Limit to 2 extra context images to avoid confusion
                            if len(input_parts) >= 4: break
            
            if render_mode == "Prompt Mode":
                # Skip actual image generation
                print(f"  -> [Prompt Mode] Skipping generation for asset: {name}")
                return (name, Image.new('RGB', (img_w, img_h)), cat)
            # Optimize context images to reduce payload size (convert to JPEG)
            def optimize_ref_image(img):
                # Convert to RGB (remove alpha channel if present)
                if img.mode in ('RGBA', 'LA') or (img.mode == 'P' and 'transparency' in img.info):
                    img = img.convert('RGB')
                
                # Save to buffer as JPEG
                buf = io.BytesIO()
                img.save(buf, format='JPEG', quality=85)
                buf.seek(0)
                return Image.open(buf)

            optimized_parts = []
            for p in input_parts:
                if isinstance(p, str):
                    optimized_parts.append(p)
                else:
                    optimized_parts.append(optimize_ref_image(p))
            input_parts = optimized_parts

            # Retry logic with exponential backoff
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    t_start = time.time()
                    if attempt == 0:
                        img_count = sum(1 for p in input_parts if not isinstance(p, str))
                        print(f"[SceneGen] Debug Asset {name}: Sending {img_count} context images.")

                    # Add timeout - Increased to 90s
                    r = await asyncio.wait_for(model_image_gen.generate_content_async(input_parts), timeout=90)
                    
                    dur = time.time() - t_start
                    print(f"[SceneGen] Asset {name} Success in {dur:.1f}s")
                    if hasattr(r, 'parts'):
                        for p in r.parts:
                            if hasattr(p, 'inline_data'):
                                img = Image.open(io.BytesIO(p.inline_data.data))
                                img = img.resize((img_w, img_h)) # Ensure correct resolution
                                if save_assets:
                                    fname = f"asset_{name}.png"
                                    img.save(os.path.join(session_dir, fname))
                                    usage_stats["generated_assets"].append({"name": name, "type": cat, "file": fname})
                                    report_state["assets"].append({"name": name, "type": cat, "file": fname, "prompt": full_prompt}) # Add to report state
                                usage_stats["gemini_images_generated"] += 1 # Track image generation
                                return (name, img, cat)
                except asyncio.TimeoutError:
                    print(f"[SceneGen] WARNING: Asset {name} timed out (Attempt {attempt+1}/{max_retries}). Retrying...")
                except Exception as e:
                    print(f"[SceneGen] WARNING: Asset {name} Error: {e} (Attempt {attempt+1}/{max_retries}). Retrying...")
                
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            
            print(f"[SceneGen] ERROR: Asset {name} failed after {max_retries} attempts.")
            return (name, Image.new('RGB', (img_w, img_h)), cat)

        # Dependency Loop
        pending = {a["name"]: a for a in new_assets_list}
        completed = set(asset_library.keys())
        
        while pending:
            check_cancelled()  # Allow cancel during asset generation
            ready = []
            for name, data in pending.items():
                p = data.get("parent_asset")
                if not p or p in completed:
                    ready.append(name)
            
            if not ready:
                print("Stage 4: Cyclic dependency or missing parent. Forcing remaining...")
                ready = list(pending.keys()) # Force run remaining to avoid infinite loop
            
            print(f"Generating batch: {ready}")
            # Run batch with concurrency control
            batch_results = []
            ready_tasks = [gen_asset_task(pending[n]) for n in ready]
            chunk_size = gemini_concurrency
            
            for i in range(0, len(ready_tasks), chunk_size):
                chunk = ready_tasks[i:i+chunk_size]
                print(f"  -> Asset Generation Chunk {i//chunk_size + 1}/{(len(ready_tasks) + chunk_size - 1)//chunk_size}...")
                chunk_results = await asyncio.gather(*chunk)
                batch_results.extend(chunk_results)
            
            for name, img, cat in batch_results:
                asset_library[name] = img
                completed.add(name)
                del pending[name]
                
                if "env" in cat: env_imgs.append(img)
                elif "prop" in cat: prop_imgs.append(img)
                else: actor_imgs.append(img)
            
            # Update report after batch
            update_report(None, None, f"Generated batch of {len(ready)} assets.")
            
        available_assets_summary = ref_data + new_assets_list
        
        # Save Stage 5 summary
        debug_s4 = json.dumps({
            "reference_assets": ref_data,
            "generated_assets": new_assets_list,
            "total_assets": len(available_assets_summary)
        }, indent=2)
        with open(os.path.join(session_dir, "stage5_assets.json"), "w", encoding="utf-8") as f: f.write(debug_s4)

        # --- STAGE 6: Montage Line ---
        update_report("Stage 6: Creating Montage...", 35, "Structuring video timeline...")
        print("[SceneGen] Stage 6: Montage Line...")
        
        # Build model instruction based on availability
        # Build model instruction based on availability
        if available_models:
            model_instruction = f"""
        - AVAILABLE VIDEO MODELS: {json.dumps(available_models)}
        
        CRITICAL INSTRUCTION FOR MODEL SELECTION:
        You MUST use the EXACT string from the "AVAILABLE VIDEO MODELS" list above for the "model" field.
        DO NOT use short names like "veo", "kling", "hailuo". Use the FULL string (e.g., "google/veo-3.1", "kwaivgi/kling-v2.5-turbo-pro").
        
        Duration Constraints:
        - "wan-video/wan-2.5-i2v" / "wan-video/wan-2.5-i2v-fast": 5s.
        - "kwaivgi/kling-v2.5-turbo-pro": 5s or 10s.
        - "minimax/hailuo-2.3": 6s.
        - "google/veo-3.1" / "google/veo-3.1-fast": 4s, 6s, or 8s.
        
        For each scene:
        - "model": string (EXACTLY as listed above)
        - "duration": float (e.g. 5.0, 6.0, 8.0)
            """
        else:
            model_instruction = """
        - No video models available (slideshow mode).
        - For each scene, use "model": "Slideshow"
        - Specify "duration": float (how long this image should be shown, e.g. 3.0-5.0s)
            """
        
        # Create explicit list of asset names for Gemini
        available_asset_names = [a["name"] for a in available_assets_summary]
        
        prompt_s6 = f"""
        CRITICAL TASK: Create PRECISE video montage synchronized with audio.
        
        CONTEXT:
        - Audio Duration: {total_duration_sec:.2f} seconds (MUST BE FULLY COVERED)
        - Audio Analysis (with timestamps): {debug_s1}
        - Style: {style_instruction}
        - Narrative: {report_state.get("narrative", "")}
        - Available Assets (USE EXACT NAMES): {json.dumps(available_asset_names)}
        - Dynamicity: {dynamicity} (0=slow/calm, 1=fast/energetic)
        - Aggressive Edit: {aggressive_edit} (if True: fast cuts, sync to beats)
        - User Instruction: "{instruction}"
        
        {model_instruction}
        
        MONTAGE REQUIREMENTS (ALL MANDATORY):
        
        1. **SYNCHRONIZATION WITH AUDIO TIMESTAMPS**:
           - Use the timestamped lyrics, beats, and structure from Audio Analysis
           - Align scene changes with:
             * Lyric phrases (time_start/time_end)
             * Beat markers (especially "strong" beats)
             * Structural sections (Intro/Verse/Chorus transitions)
           - Each scene should correspond to a specific moment in the audio
        
        2. **AGGRESSIVE EDIT MODE** (if enabled):
           - Create SHORT, DYNAMIC cuts (0.5s - 3s per scene)
           - Sync EVERY cut to a beat or word
           - Use "trim_duration" SHORTER than "duration" (e.g., generate 5s, use 2s)
           - High scene count (aim for 1 scene per 2-3 seconds of audio)
        
        3. **NORMAL EDIT MODE** (if aggressive_edit=False):
           - Longer, cinematic shots (4s - 10s per scene)
           - Scene changes at major structural points or lyric phrases
           - trim_duration ‚âà duration (or slightly less)
        
        4. **DURATION CONSTRAINTS** (CRITICAL):
           - "duration" (generation length): MUST match model constraints:
             * wan-video: 5s ONLY
             * kling: 5s or 10s ONLY
             * hailuo: 6s ONLY
             * veo-3.1: 4s, 6s, or 8s ONLY
             * Slideshow: any (3-10s)
           - "trim_duration" (used in timeline): CAN be ANY value <= duration
           - Example: {{"model": "google/veo-3.1", "duration": 6.0, "trim_duration": 2.5}}
             ‚Üí Generates 6s video, uses first 2.5s in montage
        
        5. **FULL AUDIO COVERAGE** (CRITICAL):
           - The SUM of ALL "trim_duration" values MUST EQUAL {total_duration_sec:.2f}s
           - NO GAPS, NO OVERLAP
           - Verify: sum([scene["trim_duration"] for scene in scenes]) == {total_duration_sec:.2f}
        
        6. **SCENE CONTENT**:
           - "description": Visual action description (what happens in THIS specific moment)
           - "assets": List of asset names (from Available Assets)
           - "sync_reference": Quote the lyric/beat/timestamp this scene aligns with
           - "model": EXACT model string from AVAILABLE VIDEO MODELS
           - "duration": Generation duration (respecting model constraints)
           - "trim_duration": Final timeline duration (‚â§ duration)
        
        Return VALID JSON:
        {{
            "scenes": [
                {{
                    "description": "...",
                    "assets": ["Asset_Name_1", "Asset_Name_2"],
                    "sync_reference": "Lyrics 'word' at 12.5s" or "Beat at 3.2s" or "Chorus starts",
                    "model": "google/veo-3.1",
                    "duration": 6.0,
                    "trim_duration": 2.5
                }},
                ...
            ]
        }}
        
        FINAL VERIFICATION BEFORE RETURNING:
        - Count scenes
        - Sum all trim_duration values
        - Confirm sum == {total_duration_sec:.2f}s
        - If not equal: ADD/REMOVE scenes or ADJUST trim_duration to match EXACTLY
        """
        # --- STAGE 6: Montage Generation with Feedback Loop ---
        montage_data = []
        max_montage_retries = 3
        debug_s6 = ""
        
        for attempt in range(max_montage_retries):
            full_prompt = prompt_s6
            
            if attempt > 0:
                # Calculate current duration from previous failed attempt
                current_dur = sum(s.get("trim_duration", s.get("duration", 5.0)) for s in montage_data)
                diff = total_duration_sec - current_dur
                
                print(f"[SceneGen] Montage duration mismatch ({current_dur:.2f}s vs {total_duration_sec:.2f}s). Retry {attempt}/{max_montage_retries} with feedback...")
                
                # Calculate timeline coverage
                timeline_info = []
                cumulative = 0.0
                for i, s in enumerate(montage_data):
                    dur = s.get("trim_duration", s.get("duration", 5.0))
                    timeline_info.append(f"Scene {i}: {cumulative:.1f}s-{cumulative+dur:.1f}s ({dur:.1f}s)")
                    cumulative += dur
                
                feedback_prompt = f"""
                CRITICAL ERROR IN PREVIOUS ATTEMPT:
                The generated montage duration was {current_dur:.2f}s, but the audio is {total_duration_sec:.2f}s.
                Difference: {diff:+.2f}s ({'TOO SHORT - missing content' if diff > 0 else 'TOO LONG - remove content'}).
                
                Current Timeline Coverage:
                {chr(10).join(timeline_info)}
                Ends at: {cumulative:.2f}s (should end at {total_duration_sec:.2f}s)
                
                CORRECTION STRATEGY:
                {f'''- You need to ADD {diff:.2f}s of content
                - Option 1: Add new scenes to cover timestamps {cumulative:.1f}s - {total_duration_sec:.1f}s
                - Option 2: Extend trim_duration of existing scenes (respecting duration limits)
                - Check Audio Analysis for what happens in the missing time range''' if diff > 0 else f'''- You need to REMOVE {abs(diff):.2f}s of content
                - Option 1: Remove some scenes
                - Option 2: Shorten trim_duration of scenes (prioritize end scenes)
                - Maintain sync with important moments (beats, lyrics)'''}
                
                MANDATORY: The total 'trim_duration' of all scenes MUST sum EXACTLY to {total_duration_sec:.2f}s.
                
                Return the FULL corrected JSON with all scenes.
                """
                full_prompt += "\n\n" + feedback_prompt

            track_text(full_prompt, "")
            
            try:
                resp_s6 = await model_text.generate_content_async([full_prompt, {"mime_type": "audio/wav", "data": audio_bytes}])
                debug_s6 = resp_s6.text
                track_text("", debug_s6)
                
                try:
                    montage_data = json.loads(self._clean_json(debug_s6)).get("scenes", [])
                except json.JSONDecodeError:
                    print(f"[SceneGen] JSON Error in Stage 6 (Attempt {attempt})")
                    if attempt == max_montage_retries - 1: raise
                    continue

                # Check duration
                total_montage_duration = sum(s.get("trim_duration", s.get("duration", 5.0)) for s in montage_data)
                
                if abs(total_montage_duration - total_duration_sec) <= 2.0:
                    print(f"[SceneGen] Montage duration accepted: {total_montage_duration:.2f}s (Audio: {total_duration_sec:.2f}s)")
                    break # Success!
                
            except Exception as e:
                print(f"[SceneGen] Stage 6 Error (Attempt {attempt}): {e}")
                if attempt == max_montage_retries - 1:
                    # Fallback: simple slideshow if all retries fail
                    montage_data = [{"description": "Scene 1", "assets": [], "duration": 5.0, "trim_duration": 5.0, "model": "Slideshow"}]
                    update_report("Stage 6 Warning", 40, "Failed to generate valid montage. Using fallback.")

        with open(os.path.join(session_dir, "stage6_montage.json"), "w", encoding="utf-8") as f: f.write(debug_s6)
            
        # --- Time Normalization Logic ---
        # Ensure total duration matches audio length exactly
        total_montage_duration = sum(s.get("trim_duration", s.get("duration", 5.0)) for s in montage_data)
        
        if abs(total_montage_duration - total_duration_sec) > 0.5:
            print(f"[SceneGen] Montage duration mismatch: {total_montage_duration:.2f}s vs Audio {total_duration_sec:.2f}s. Normalizing...")
            
            # Calculate scaling factor
            scale_factor = total_duration_sec / total_montage_duration if total_montage_duration > 0 else 1
            
            current_total = 0.0
            for i, scene in enumerate(montage_data):
                orig_dur = scene.get("trim_duration", scene.get("duration", 5.0))
                new_dur = orig_dur * scale_factor
                
                # Round to 2 decimal places for cleaner logs
                new_dur = round(new_dur, 2)
                
                scene["trim_duration"] = new_dur
                # Note: We don't change generation 'duration' (e.g. 5s) unless trim > duration
                # But we must ensure generation is long enough
                if new_dur > scene.get("duration", 5.0):
                     scene["duration"] = math.ceil(new_dur) 
                
                current_total += new_dur
                
            # Fix rounding errors on the last scene
            diff = total_duration_sec - current_total
            if montage_data:
                montage_data[-1]["trim_duration"] += diff
                # Ensure generation duration covers it
                if montage_data[-1]["trim_duration"] > montage_data[-1].get("duration", 5.0):
                     montage_data[-1]["duration"] = math.ceil(montage_data[-1]["trim_duration"])
            
            print(f"[SceneGen] Normalized duration. New total: {sum(s['trim_duration'] for s in montage_data):.2f}s")

        report_state["montage"] = montage_data
        debug_s5 = json.dumps(montage_data, indent=2)  # S5 = Montage
        update_report(None, None, f"Montage created: {len(montage_data)} scenes.")
        
        # Validate assets in montage
        print("[SceneGen] Validating asset assignments...")
        missing_assets = set()
        for scene_idx, scene in enumerate(montage_data):
            scene_assets = scene.get("assets", [])
            for asset_name in scene_assets:
                if asset_name not in asset_library and asset_name not in [a["name"] for a in ref_data]:
                    missing_assets.add(asset_name)
        
        if missing_assets:
            print(f"[SceneGen] WARNING: {len(missing_assets)} assets referenced but not found: {missing_assets}")
            print(f"[SceneGen] Available assets: {list(asset_library.keys())}")

        # --- STAGE 7: Prompt Engineering (Batched) ---
        update_report("Stage 7: Writing Prompts...", 45, "Expanding scene details...")
        print("[SceneGen] Stage 7: Prompt Engineering (Batched)...")
        batch_size = 10
        
        # Prepare batches
        total_batches = (len(montage_data) + batch_size - 1) // batch_size
        
        # Create async function for processing a single batch
        async def process_batch(b_idx):
            start_i = b_idx * batch_size
            end_i = start_i + batch_size
            batch_scenes = montage_data[start_i:end_i]
            
            print(f"  -> Processing Batch {b_idx + 1}/{total_batches} (Scenes {start_i} to {min(end_i, len(montage_data))})...")
            
            prompt_s7 = f"""
            Expand the montage into detailed production prompts.
            Batch {b_idx + 1} of {total_batches}.
            
            Style: {style_instruction}
            Palette: {json.dumps(palette_data)}
            Word Influence: {word_influence} (-1.0 to 1.0)
            
            Scenes to Process: {json.dumps(batch_scenes)}
            
            Instruction for Word Influence:
            - If > 0.5: Prioritize LITERAL visualization of lyrics/text. Show exactly what is described.
            - If < -0.5: Prioritize VIBE/ATMOSPHERE. Ignore literal lyrics, focus on mood and abstract representation.
            - If ~0.0: Balance both.
            
            For EACH scene in this batch, generate:
            - "positive_prompt": A MASSIVE, DETAILED paragraph (50-100 words) describing the scene. 
              CRITICAL: You MUST explicitly describe the background/location using the specific Environment asset details to ensure continuity.
              Focus heavily on COMPOSITION, CAMERA ANGLE, LIGHTING, TEXTURE, and ATMOSPHERE. Make it visually rich and strictly adhering to the defined Style.
              ‚ö†Ô∏è ABSOLUTELY CRITICAL: DO NOT include ANY text, words, captions, titles, subtitles, labels, or written elements in the visual description. The scene must be 100% visual only.
            - "video_trigger_prompt": Motion instruction for video generation.
            - "negative_prompt": Standard negative prompt including: text, words, letters, captions, titles, subtitles, writing, typography, labels.
            
            Return JSON object with "scenes": list of objects (same order as input).
            """
            
            if dialogues_gen:
                prompt_s7 += """
                DIALOGUE GENERATION ENABLED:
                For scenes assigned to 'Veo 3.1', 'Veo 3.1 Fast', 'Wan 2.5', or 'Wan 2.5 Fast', you MAY generate a short dialogue line (max 7 seconds spoken) if it fits the narrative.
                - Add field "dialogue": "Character Name: Spoken text" (or empty string if no dialogue).
                - Keep it brief and impactful.
                """
            
            
            try:
                track_text(prompt_s7, "")
                resp_s7 = await model_text.generate_content_async(prompt_s7)
                track_text("", resp_s7.text)
                batch_result = json.loads(self._clean_json(resp_s7.text)).get("scenes", [])
                
                # Merge details back into the original scene data for this batch
                detailed_batch = []
                for i, scene_detail in enumerate(batch_result):
                    if i < len(batch_scenes):
                        merged = {**batch_scenes[i], **scene_detail}
                        detailed_batch.append(merged)
                return (b_idx, detailed_batch)
            except Exception as e:
                print(f"  Batch {b_idx + 1} Error: {e}")
                # On error, keep original batch scenes without updates
                return (b_idx, batch_scenes)
        
        # Process batches with concurrency control
        batch_tasks = [process_batch(b_idx) for b_idx in range(total_batches)]
        batch_results = []
        chunk_size = max(1, gemini_concurrency // 2)  # Use half of concurrency for batches (each batch has ~10 scenes)
        
        for i in range(0, len(batch_tasks), chunk_size):
            chunk = batch_tasks[i:i+chunk_size]
            print(f"  -> Processing Batch Chunk {i//chunk_size + 1}/{(len(batch_tasks) + chunk_size - 1)//chunk_size}...")
            chunk_results = await asyncio.gather(*chunk)
            batch_results.extend(chunk_results)
        
        # Sort by batch index and flatten
        batch_results.sort(key=lambda x: x[0])
        detailed_scenes = []
        for _, scenes in batch_results:
            detailed_scenes.extend(scenes)
        
        final_scenes = detailed_scenes
        debug_s6 = json.dumps(final_scenes, indent=2)  # S6 = Prompts
        with open(os.path.join(session_dir, "stage7_prompts.json"), "w", encoding="utf-8") as f: f.write(debug_s6)
        
        # Update report state with prompts and montage
        report_state["prompts"] = debug_s6
        report_state["montage"] = final_scenes
        update_report("Stage 8: Generating Start Frames...", 50, "Scene prompts complete.")

        # --- STAGE 8: Image Generation (Start Frames) ---
        scene_images = []
        if render_mode == "Full Render":
            update_report("Stage 8: Generating Start Frames...", 55, f"Generating {len(final_scenes)} start frames...")
            print(f"[SceneGen] Stage 8: Generating Start Frames ({len(final_scenes)} scenes)...")
            
            async def gen_scene_image(idx, scene_data):
                # CRITICAL: NO TEXT ON IMAGES
                # CRITICAL: NO TEXT ON IMAGES & NO COLLAGES
                no_text_instruction = "ABSOLUTELY NO TEXT, NO WORDS, NO CAPTIONS, NO TITLES, NO LABELS on the image. Pure visual scene only."
                ref_instruction = "The attached images are VISUAL REFERENCES ONLY (Character Sheets / Location Refs). DO NOT COPY their composition. DO NOT make a collage, split-screen, or grid. DO NOT show multiple views."
                composition_instruction = "Create a SINGLE, CINEMATIC SHOT based on the text description. Integrate the referenced characters/objects naturally into a NEW scene with the described lighting and angle."
                prompt = f"{no_text_instruction} {ref_instruction} {composition_instruction} {scene_data.get('positive_prompt')} --aspect {aspect_ratio}"
                
                input_parts = [prompt]
                
                # Smart Asset Selection - read both 'assets' and 'asset_refs' for compatibility
                refs = scene_data.get("assets", scene_data.get("asset_refs", []))
                ref_img_found = False
                attached_asset_names = []
                
                if refs:
                    for r_name in refs:
                        r_name_clean = r_name.strip()
                        found_key = None
                        
                        # 1. Exact Match (case-sensitive)
                        if r_name_clean in asset_library:
                            found_key = r_name_clean
                        
                        # 2. Case-insensitive exact match
                        if not found_key:
                            for key in asset_library.keys():
                                if key.lower() == r_name_clean.lower():
                                    found_key = key
                                    break
                        
                        # 3. Partial Match (if LLM added extra text like "Gen_Env_0 (Dark)")
                        if not found_key:
                            for key in asset_library.keys():
                                if key in r_name_clean or r_name_clean in key:
                                    found_key = key
                                    break
                        
                        # 4. Fuzzy: Check if first word matches
                        if not found_key:
                            first_word = r_name_clean.split(" ")[0] if " " in r_name_clean else r_name_clean
                            for key in asset_library.keys():
                                if key.split("_")[0].lower() == first_word.lower():
                                    found_key = key
                                    break
                        
                        if found_key:
                            input_parts.append(asset_library[found_key])
                            attached_asset_names.append(found_key)
                            ref_img_found = True
                            # Allow multiple assets (up to 5) to provide better context
                            if len(input_parts) >= 6:  # 1 prompt + 5 images max
                                break 
                
                # Fallback - log which assets were requested but not found
                if not ref_img_found:
                    if refs:
                        print(f"[SceneGen] WARNING Scene {idx}: Assets {refs} not found in library. Using fallback.")
                        print(f"[SceneGen] Available: {list(asset_library.keys())[:10]}...")
                    
                    elif env_imgs: 
                        input_parts.append(env_imgs[0])
                        attached_asset_names.append("Fallback_Env_0")
                
                # Helper to optimize reference images (convert to JPEG to reduce upload size)
                def optimize_ref_image(img):
                    # Convert to RGB (remove alpha channel if present)
                    if img.mode in ('RGBA', 'LA') or (img.mode == 'P' and 'transparency' in img.info):
                        img = img.convert('RGB')
                    
                    # Save to buffer as JPEG
                    buf = io.BytesIO()
                    img.save(buf, format='JPEG', quality=85)
                    buf.seek(0)
                    return Image.open(buf)

                # Optimize images in input_parts
                optimized_parts = []
                for p in input_parts:
                    if isinstance(p, str):
                        optimized_parts.append(p)
                    else:
                        optimized_parts.append(optimize_ref_image(p))
                
                input_parts = optimized_parts

                
                # Removed individual per-scene print - will show batch summary instead
                
                # Retry logic with exponential backoff
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        t_start = time.time()
                        # Diagnostic log for first attempt
                        if attempt == 0:
                            img_count = sum(1 for p in input_parts if not isinstance(p, str))
                            txt_len = sum(len(p) for p in input_parts if isinstance(p, str))
                            print(f"[SceneGen] Debug Scene {idx}: Sending {img_count} images, {txt_len} chars prompt. Assets: {attached_asset_names}")
                        
                        # Add timeout to prevent infinite hanging - Increased to 90s
                        r = await asyncio.wait_for(model_image_gen.generate_content_async(input_parts), timeout=90)
                        
                        dur = time.time() - t_start
                        print(f"[SceneGen] Scene {idx} Success in {dur:.1f}s")
                        if hasattr(r, 'parts'):
                            for p in r.parts:
                                if hasattr(p, 'inline_data'):
                                    img = Image.open(io.BytesIO(p.inline_data.data))
                                    img = img.resize((img_w, img_h))
                                    if save_images:
                                        fname = f"{prefix}_scene_{idx:03d}.png"
                                        img.save(os.path.join(session_dir, fname))
                                        usage_stats["generated_assets"].append({"name": f"Scene {idx}", "type": "Start Frame", "file": fname})
                                        report_state["assets"].append({"name": f"Scene {idx}", "type": "Start Frame", "file": fname, "prompt": prompt})
                                    usage_stats["gemini_images_generated"] += 1 # Track image generation
                                    return img
                    except asyncio.TimeoutError:
                        print(f"[SceneGen] WARNING: Scene {idx} timed out (Attempt {attempt+1}/{max_retries}). Retrying...")
                    except Exception as e:
                        print(f"[SceneGen] WARNING: Scene {idx} Error: {e} (Attempt {attempt+1}/{max_retries}). Retrying...")
                    
                    if attempt < max_retries - 1:
                        # Strategy for next attempt
                        print(f"[SceneGen] Scene {idx}: Preparing retry strategy...")
                        
                        # Separate text and images
                        current_imgs = [p for p in input_parts if not isinstance(p, str)]
                        current_txt = next((p for p in input_parts if isinstance(p, str)), "")
                        
                        if current_imgs:
                            # Attempt 2 (index 1): Resize images to 512px max dimension to reduce payload
                            if attempt == 0:
                                print("[SceneGen] Strategy: Resizing reference images to 512px...")
                                new_imgs = []
                                for img in current_imgs:
                                    try:
                                        img_copy = img.copy()
                                        img_copy.thumbnail((512, 512))
                                        new_imgs.append(img_copy)
                                    except Exception as e:
                                        print(f"[SceneGen] Resize warning: {e}")
                                        new_imgs.append(img) # Keep original if resize fails
                                input_parts = [current_txt] + new_imgs
                                
                            # Attempt 3 (index 2): Keep only the first image (primary asset)
                            elif attempt == 1:
                                if len(current_imgs) > 1:
                                    print("[SceneGen] Strategy: Keeping only primary reference image...")
                                    input_parts = [current_txt, current_imgs[0]]
                                else:
                                    print("[SceneGen] Strategy: Retrying with same inputs (transient error check)...")
                        
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                
                print(f"[SceneGen] ERROR: Scene {idx} failed after {max_retries} attempts. Using black placeholder.")
                return Image.new('RGB', (img_w, img_h))

            # Generate start frames with concurrency control
            scene_images = []
            tasks = [gen_scene_image(i, s) for i, s in enumerate(final_scenes)]
            chunk_size = gemini_concurrency
            
            for i in range(0, len(tasks), chunk_size):
                chunk = tasks[i:i+chunk_size]
                print(f"  -> Generating Start Frames Batch {i//chunk_size + 1}/{(len(tasks) + chunk_size - 1)//chunk_size}...")
                batch_results = await asyncio.gather(*chunk)
                scene_images.extend(batch_results)
                # Log batch completion with scene index range
                start_idx = i
                end_idx = min(i + len(chunk) - 1, len(tasks) - 1)
                print(f"[SceneGen] Completed Start Frames Batch {i//chunk_size + 1}: scenes {start_idx}‚Äë{end_idx}")
            
            update_report(None, 65, "Start frames generated.")
        else:
            update_report("Stage 8: Skipped (Prompt Mode - Text Only)", 65, "Skipping start frame generation.")
            print("[SceneGen] Stage 8: Skipped (Prompt Mode - Text Only)")
            # Provide placeholder images for subsequent stages that might expect them
            scene_images = [Image.new('RGB', (img_w, img_h))] * len(final_scenes)

        debug_s7 = f"Generated {len(scene_images)} start frames."  # S7 = Start Frames Info
        with open(os.path.join(session_dir, "stage8_generation_status.txt"), "w", encoding="utf-8") as f: f.write(debug_s7)

        # Add planned filenames to montage
        for idx, scene in enumerate(final_scenes):
            # Image filename (start frame)
            scene["image_file"] = f"{prefix}_scene_{idx:03d}.png"
            # Video filename (will be generated in Stage 11)
            if save_segments:
                scene["video_file"] = f"{prefix}_{idx:03d}.mp4"
            else:
                scene["video_file"] = f"seg_{idx:03d}.mp4"

        # --- STAGE 9: Vision-Aware Video Prompt Refinement ---
        update_report("Stage 9: Refining Motion...", 70, "Analyzing frames for motion prompts...")
        print(f"[SceneGen] Stage 9: Refining Video Prompts with Vision ({len(final_scenes)} scenes)...")
        
        current_time_s9 = 0.0
        refine_tasks = []
        
        async def refine_scene_prompt(idx, scene, img, start_time):
            model_name = scene.get("model", "generic")
            
            prompt_s9 = f"""
            Role: Expert Video Prompt Engineer for {model_name}.
            
            Input:
            1. Start Frame Image (Attached).
            2. Audio Analysis/Lyrics: {debug_s1}
            3. Current Scene Time: {start_time:.1f}s
            4. Context/Clue: "{scene.get('clue', '')}"
            
            Task:
            Write a highly specific "Motion Prompt" (video_trigger_prompt) for this exact shot.
            
            CRITICAL RULES:
            1. NEVER allow a static shot. Always describe motion.
            2. If the scene is calm, use terms like "subtle natural movement", "micro-movements", "delicate living environment", "slow camera drift", "wind blowing hair/cloth", "breathing".
            3. If energetic, describe the action vividly and strictly synced to the music/lyrics at {start_time:.1f}s.
            4. Use the specific prompting style best for {model_name} (e.g. for Wan/Veo use "Camera pan...", for Kling use "The character moves...").
            
            Return JSON: {{ "video_trigger_prompt": "..." }}
            """
            
            try:
                # Gemini supports image input directly
                track_text(prompt_s9, "")
                resp = await model_text.generate_content_async([prompt_s9, img])
                track_text("", resp.text)
                res_json = json.loads(self._clean_json(resp.text))
                return (idx, res_json.get("video_trigger_prompt", ""))
            except Exception as e:
                print(f"Stage 9 Error (Scene {idx}): {e}")
                return (idx, "")

        for i, scene in enumerate(final_scenes):
            dur = float(scene.get("duration", 5))
            # Ensure we have an image
            img = scene_images[i]
            refine_tasks.append(refine_scene_prompt(i, scene, img, current_time_s9))
            current_time_s9 += dur
            
        # Run with concurrency limit (chunking)
        refined_results = []
        chunk_size = gemini_concurrency
        for i in range(0, len(refine_tasks), chunk_size):
            chunk = refine_tasks[i:i+chunk_size]
            print(f"  -> Processing Vision Batch {i//chunk_size + 1}...")
            refined_results.extend(await asyncio.gather(*chunk))
            
        # Apply updates
        for idx, new_prompt in refined_results:
            if new_prompt:
                final_scenes[idx]["video_trigger_prompt"] = new_prompt
                
        debug_s8 = json.dumps([{"scene": i, "motion": p} for i, p in refined_results], indent=2) if refined_results else "[]"  # S8 = Motion Refinement
        with open(os.path.join(session_dir, "stage9_prompts_status.txt"), "w", encoding="utf-8") as f: f.write(debug_s8)
        report_state["motion"] = debug_s8
        # Update montage with new prompts
        report_state["montage"] = final_scenes
        debug_s9 = json.dumps(final_scenes, indent=2)  # S9 = Timeline Data (final montage with all details)
        with open(os.path.join(session_dir, "stage9_timeline.json"), "w", encoding="utf-8") as f: f.write(debug_s9)
        update_report("Stage 11: Audio Slicing...", 75, "Motion refinement complete.")

        # --- STAGE 10: Duration Verification & Audio Slicing ---
        print("[SceneGen] Stage 10: Duration Verification...")
        current_sample = 0
        replicate_tasks = []
        
        for i, scene in enumerate(final_scenes):
            target_dur = float(scene.get("duration", 5))
            trim_dur = float(scene.get("trim_duration", target_dur))
            
            # Dialogue Logic: If dialogue is present on a supported model, DO NOT TRIM aggressively
            has_dialogue = bool(scene.get("dialogue", "")) and dialogues_gen
            # Check if model supports dialogue (Veo, Wan)
            raw_model = scene.get("model", "").lower()
            is_dialogue_model = "veo" in raw_model or "wan" in raw_model
            
            if not aggressive_edit or (has_dialogue and is_dialogue_model): 
                trim_dur = target_dur # Use full duration, do not trim
            
            # Quantize to nearest frame to ensure EDL/Video sync
            frame_dur = 1.0 / fps
            trim_dur = round(trim_dur / frame_dur) * frame_dur
            
            raw_model_name = scene.get("model", "")
            model = available_models[0] if available_models else "Slideshow"
            
            # Validate and fix model name
            if available_models:
                if raw_model_name in available_models:
                    model = raw_model_name
                else:
                    # Try fuzzy match
                    found = False
                    for av in available_models:
                        if raw_model_name.lower() in av.lower() or av.lower() in raw_model_name.lower():
                            model = av
                            found = True
                            print(f"[SceneGen] Fixed model name for Scene {i}: '{raw_model_name}' -> '{model}'")
                            break
                    if not found:
                        print(f"[SceneGen] Warning: Model '{raw_model_name}' not available. Using fallback: '{model}'")
            
            scene["model"] = model # Update scene with correct model name
            
            # Snap logic
            gen_dur = target_dur
            if "wan-video" in model or "kling" in model:
                gen_dur = 5 if target_dur <= 5 else 10
            elif "hailuo" in model:
                gen_dur = 6 if target_dur <= 6 else 10
            elif "veo" in model:
                if target_dur <= 4: gen_dur = 4
                elif target_dur <= 6: gen_dur = 6
                else: gen_dur = 8
            
            if target_dur > gen_dur: target_dur = gen_dur
            
            # Audio Slice
            start_sec = current_sample / sample_rate
            gen_end_sec = start_sec + gen_dur
            start_samp = int(start_sec * sample_rate)
            gen_end_samp = int(gen_end_sec * sample_rate)
            
            if start_samp >= len(audio_data): break
            
            raw_slice = audio_data[start_samp:gen_end_samp]
            if len(raw_slice) < (gen_end_samp - start_samp):
                pad = (gen_end_samp - start_samp) - len(raw_slice)
                if pad > 0:
                    silence = np.zeros((pad,) + raw_slice.shape[1:], dtype=raw_slice.dtype)
                    raw_slice = np.concatenate((raw_slice, silence), axis=0)
            
            replicate_tasks.append({
                "index": i,
                "model": model,
                "prompt": f"{scene.get('video_trigger_prompt')} {scene.get('positive_prompt')}",
                "positive_prompt": scene.get('positive_prompt'),
                "video_trigger_prompt": scene.get('video_trigger_prompt'),
                "negative_prompt": scene.get("negative_prompt", ""),
                "duration": gen_dur,
                "trim_duration": trim_dur,
                "audio_slice": raw_slice,
                "image": scene_images[i]
            })
            
            current_sample += int(trim_dur * sample_rate)
            
        # Save Stage 10 details
        tasks_summary = []
        for t in replicate_tasks:
            tasks_summary.append({
                "index": t["index"],
                "model": t["model"],
                "prompt": t["prompt"],
                "positive_prompt": t["positive_prompt"],
                "video_trigger_prompt": t["video_trigger_prompt"],
                "negative_prompt": t["negative_prompt"],
                "duration": t["duration"],
                "trim_duration": t["trim_duration"]
            })
            
            # Track Replicate Usage
            m = t["model"]
            if m not in usage_stats["replicate_seconds"]: usage_stats["replicate_seconds"][m] = 0
            usage_stats["replicate_seconds"][m] += t["duration"]
        
        debug_s10 = json.dumps(tasks_summary, indent=2)
        with open(os.path.join(session_dir, "stage10_tasks.json"), "w", encoding="utf-8") as f: f.write(debug_s10)

        # --- STAGE 11: Replicate Execution ---
        update_report("Stage 11: Rendering Video...", 75, f"Starting render of {len(replicate_tasks)} clips...")
        print(f"[SceneGen] Stage 11: Replicate Execution ({len(replicate_tasks)} tasks)...")
        
        video_paths = [None] * len(replicate_tasks)
        
        if render_mode == "Full Render" and available_models:
            def run_replicate(task):
                idx = task["index"]
                print(f"Task {idx} ({task['model']}) starting...")
                
                # Temp files
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tf_img:
                    task["image"].save(tf_img, format="PNG")
                    img_path = tf_img.name
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tf_aud:
                    scipy.io.wavfile.write(tf_aud.name, sample_rate, task["audio_slice"])
                    aud_path = tf_aud.name
                    
                try:
                    video_url = None
                    max_retries = 2
                    current_prompt = task["prompt"]
                    
                    for attempt in range(max_retries + 1):
                        try:
                            with open(img_path, "rb") as f_img, open(aud_path, "rb") as f_aud:
                                input_data = {}
                                # Map inputs based on model
                                if "wan-video" in task["model"]:
                                    res = "1080p" if video_quality == "High" else "720p"
                                    if video_quality == "Low": res = "480p"
                                    input_data = {"image": f_img, "audio": f_aud, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "resolution": res, "enable_prompt_expansion": enable_prompt_expansion}
                                elif "kling" in task["model"]:
                                    input_data = {"start_image": f_img, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "aspect_ratio": aspect_ratio, "guidance_scale": 0.5}
                                elif "omni-human" in task["model"]:
                                    input_data = {"image": f_img, "audio": f_aud, "prompt": current_prompt, "fast_mode": (video_quality != "High")}
                                elif "hailuo" in task["model"]:
                                    res = "1080p" if video_quality == "High" else "768p"
                                    if task["duration"] == 10: res = "768p" # Enforce resolution constraint
                                    input_data = {"first_frame_image": f_img, "prompt": current_prompt, "duration": task["duration"], "resolution": res, "prompt_optimizer": True}
                                elif "veo" in task["model"]:
                                    res = "1080p" if video_quality == "High" else "720p"
                                    input_data = {"image": f_img, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "resolution": res, "aspect_ratio": aspect_ratio, "generate_audio": True}

                                output = replicate.run(task["model"], input=input_data)
                                video_url = str(output)
                                break
                        except Exception as e:
                            if ("sensitive" in str(e).lower() or "E005" in str(e)) and attempt < max_retries:
                                print(f"Task {idx} Sensitive Content. Sanitizing...")
                                current_prompt = "A beautiful cinematic scene, safe for work, artistic style." 
                            else:
                                raise e

                    # Download
                    if save_segments: 
                        fname = f"{prefix}_{idx:03d}.mp4"
                        local_path = os.path.join(session_dir, fname)
                    else: 
                        fname = f"seg_{idx:03d}.mp4"
                        local_path = os.path.join(session_dir, fname)
                    
                    with requests.get(video_url, stream=True) as r:
                        r.raise_for_status()
                        with open(local_path, 'wb') as f:
                            for chunk in r.iter_content(chunk_size=8192): f.write(chunk)
                    
                    # Track for report
                    return (idx, local_path, fname, current_prompt)
                    
                except Exception as e:
                    print(f"Task {idx} Failed: {e}. Creating fallback video...")
                    try:
                        # Fallback: Create video from start image
                        if save_segments: 
                            fname = f"{prefix}_{idx:03d}_fallback.mp4"
                            local_path = os.path.join(session_dir, fname)
                        else: 
                            fname = f"seg_{idx:03d}_fallback.mp4"
                            local_path = os.path.join(session_dir, fname)
                        
                        dur = task["trim_duration"]
                        
                        # ffmpeg command to create video from image
                        cmd = [
                            "ffmpeg", "-y", "-loop", "1", "-i", img_path, "-t", str(dur),
                            "-c:v", "libx264", "-pix_fmt", "yuv420p", "-r", "24", 
                            "-vf", f"scale={img_w}:{img_h}:force_original_aspect_ratio=decrease,pad={img_w}:{img_h}:(ow-iw)/2:(oh-ih)/2",
                            "-an", local_path
                        ]
                        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                        
                        print(f"Task {idx} Fallback created successfully.")
                        return (idx, local_path, fname, "FALLBACK: " + task["prompt"])
                        
                    except Exception as e2:
                        print(f"Task {idx} Fallback Failed: {e2}")
                        return (idx, None, None, None)
                finally:
                    try: os.unlink(img_path); os.unlink(aud_path)
                    except: pass

            loop = asyncio.get_running_loop()
            rep_futures = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=replicate_concurrency) as rep_exec:
                for task in replicate_tasks:
                    rep_futures.append(loop.run_in_executor(rep_exec, run_replicate, task))
                results = await asyncio.gather(*rep_futures)
                
            for idx, path, fname, prompt in results:
                if path: 
                    video_paths[idx] = path
                    usage_stats["generated_segments"].append({"index": idx, "file": fname})
                    report_state["videos"].append({"index": idx, "file": fname, "prompt": prompt})
            
            update_report(None, 90, f"Render complete. {sum(1 for v in video_paths if v)} clips generated.")
            debug_s10 = f"Generated {sum(1 for v in video_paths if v)} videos."
        else:
            print("Prompt Mode: Skipping Replicate generation.")
            update_report(None, 90, "Prompt Mode: Skipped Replicate.")
            debug_s10 = "Skipped (Prompt Mode)"

        with open(os.path.join(session_dir, "stage10_paths.json"), "w", encoding="utf-8") as f: json.dump(video_paths, f, indent=2)

        # --- STAGE 12: Stitching ---
        update_report("Stage 12: Stitching...", 95, "Combining clips into final video...")
        print("[SceneGen] Stage 12: Stitching...")
        final_video_path = os.path.join(session_dir, f"{prefix}_final.mp4")
        
        if render_mode == "Full Render":
            valid_videos = [(i, v) for i, v in enumerate(video_paths) if v is not None]
            
            # --- VIDEO STITCHING ---
            if valid_videos:
                concat_list = os.path.join(session_dir, "concat.txt")
                norm_videos = []
                target_w, target_h = img_w, img_h
                
                for i, v in valid_videos:
                    trim = replicate_tasks[i]["trim_duration"]
                    norm = os.path.join(session_dir, f"norm_{i:03d}.mp4")
                    cmd = [
                        "ffmpeg", "-y", "-i", v, "-t", str(trim),
                        "-vf", f"scale={target_w}:{target_h}:force_original_aspect_ratio=decrease,pad={target_w}:{target_h}:(ow-iw)/2:(oh-ih)/2",
                        "-r", "24", "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-an", norm
                    ]
                    try:
                        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                        norm_videos.append(norm)
                    except: pass
                
                if norm_videos:
                    with open(concat_list, "w") as f:
                        for n in norm_videos: f.write(f"file '{n.replace(os.sep, '/')}'\n")
                    
                    silent_path = os.path.join(session_dir, "silent.mp4")
                    subprocess.run(["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", concat_list, "-c", "copy", silent_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    
                    # Mux with original audio
                    subprocess.run([
                        "ffmpeg", "-y", "-i", silent_path, "-i", original_audio_path,
                        "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0", "-shortest", final_video_path
                    ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            # --- SLIDESHOW FALLBACK ---
            else:
                # Check if we have start frames to make a slideshow
                start_frames = [a for a in report_state["assets"] if a["type"] == "Start Frame"]
                if start_frames:
                    print("[SceneGen] No video segments found. Generating slideshow from Start Frames...")
                    concat_list = os.path.join(session_dir, "concat_slideshow.txt")
                    norm_videos = []
                    target_w, target_h = img_w, img_h
                    
                    # Sort by scene index (assuming name is "Scene X")
                    # We need to match frames to scene durations
                    # final_scenes has the duration info
                    
                    for idx, scene in enumerate(final_scenes):
                        # Find matching frame
                        frame = next((f for f in start_frames if f["name"] == f"Scene {idx}"), None)
                        if frame:
                            img_path = os.path.join(session_dir, frame["file"])
                            
                            # Calculate duration exactly as in Stage 9 to ensure sync
                            target_dur = float(scene.get("duration", 5))
                            trim_dur = float(scene.get("trim_duration", target_dur))
                            if not aggressive_edit: trim_dur = target_dur
                            
                            norm = os.path.join(session_dir, f"slide_{idx:03d}.mp4")
                            
                            # Create video clip from image with specific duration
                            cmd = [
                                "ffmpeg", "-y", "-loop", "1", "-i", img_path, "-t", str(trim_dur),
                                "-vf", f"scale={target_w}:{target_h}:force_original_aspect_ratio=decrease,pad={target_w}:{target_h}:(ow-iw)/2:(oh-ih)/2",
                                "-r", "24", "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-pix_fmt", "yuv420p", "-an", norm
                            ]
                            try:
                                subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                                norm_videos.append(norm)
                            except Exception as e:
                                print(f"Slideshow frame error {idx}: {e}")

                    if norm_videos:
                        with open(concat_list, "w") as f:
                            for n in norm_videos: f.write(f"file '{n.replace(os.sep, '/')}'\n")
                        
                        silent_path = os.path.join(session_dir, "silent_slideshow.mp4")
                        subprocess.run(["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", concat_list, "-c", "copy", silent_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                        
                        # Mux with original audio
                        subprocess.run([
                            "ffmpeg", "-y", "-i", silent_path, "-i", original_audio_path,
                            "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0", "-shortest", final_video_path
                        ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                        
                        # Cleanup slides
                        if not save_segments:
                            for n in norm_videos: 
                                try: os.unlink(n)
                                except: pass
                    
                    # Cleanup
                    if not save_segments:
                        for n in norm_videos: os.unlink(n)
                        os.unlink(concat_list)
                        os.unlink(silent_path)
                        for _, v in valid_videos: os.unlink(v)
        else:
            # Prompt Mode: Slideshow Stitching
            print("Prompt Mode: Creating Slideshow from Start Frames...")
            concat_list = os.path.join(session_dir, "concat_slideshow.txt")
            slideshow_clips = []
            
            for i, task in enumerate(replicate_tasks):
                img = task["image"]
                trim = task["trim_duration"]
                
                # Save temp image for ffmpeg
                slide_img_path = os.path.join(session_dir, f"slide_src_{i:03d}.png")
                img.save(slide_img_path)
                
                slide_vid_path = os.path.join(session_dir, f"slide_{i:03d}.mp4")
                
                # Create video from image with duration
                # ffmpeg -loop 1 -i img.png -t duration -c:v libx264 ...
                cmd = [
                    "ffmpeg", "-y", "-loop", "1", "-i", slide_img_path, "-t", str(trim),
                    "-c:v", "libx264", "-pix_fmt", "yuv420p", "-r", "24", "-vf", f"scale={img_w}:{img_h}", slide_vid_path
                ]
                try:
                    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    slideshow_clips.append(slide_vid_path)
                except Exception as e:
                    print(f"Slideshow frame {i} failed: {e}")
            
            if slideshow_clips:
                with open(concat_list, "w") as f:
                    for n in slideshow_clips: f.write(f"file '{n.replace(os.sep, '/')}'\n")
                
                silent_path = os.path.join(session_dir, "silent_slideshow.mp4")
                subprocess.run(["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", concat_list, "-c", "copy", silent_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # Mux with original audio
                subprocess.run([
                    "ffmpeg", "-y", "-i", silent_path, "-i", original_audio_path,
                    "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0", "-shortest", final_video_path
                ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # Cleanup
                for n in slideshow_clips: os.unlink(n)
                os.unlink(concat_list)
                os.unlink(silent_path)
                # Cleanup slide images
                for i in range(len(replicate_tasks)):
                    try: os.unlink(os.path.join(session_dir, f"slide_src_{i:03d}.png"))
                    except: pass

        debug_s11 = f"Final video: {final_video_path}"
        with open(os.path.join(session_dir, "stage11_output.txt"), "w", encoding="utf-8") as f: f.write(debug_s11)
        
        report_state["final_video"] = os.path.basename(final_video_path)
        update_report("Complete", 100, "Process finished successfully.")
        cost_json = json.dumps(usage_stats, indent=2)
        
        # EDL Generation
        if save_edl:
            edl_path = os.path.join(session_dir, f"{prefix}.edl")
            with open(edl_path, "w") as edl:
                edl.write(f"TITLE: {prefix}\nFCM: NON-DROP FRAME\n\n")
                # CMX 3600 Format
                # 001  AX       V     C        00:00:00:00 00:00:05:00 00:00:00:00 00:00:05:00
                
                rec_in_sec = 0.0
                valid_videos_for_edl = [(i, v) for i, v in enumerate(video_paths) if v is not None] if render_mode == "Full Render" else [(i, f"slide_{i:03d}.mp4") for i, _ in enumerate(replicate_tasks)]
                
                for i, v_name in valid_videos_for_edl:
                    trim = replicate_tasks[i]["trim_duration"]
                    
                    # Source TC (Assume 0 start for generated clips)
                    src_in = "00:00:00:00"
                    src_out_sec = trim
                    
                    # Record TC
                    rec_out_sec = rec_in_sec + trim
                    
                    def sec_to_tc(s):
                        frames = int(s * fps)
                        hh = frames // 86400
                        rem = frames % 86400
                        mm = rem // 3600
                        rem = rem % 3600
                        ss = rem // 24
                        ff = rem % 24
                        return f"{hh:02d}:{mm:02d}:{ss:02d}:{ff:02d}"

                    src_out = sec_to_tc(src_out_sec)
                    rec_in = sec_to_tc(rec_in_sec)
                    rec_out = sec_to_tc(rec_out_sec)
                    
                    line = f"{i+1:03d}  AX       V     C        {src_in} {src_out} {rec_in} {rec_out}\n"
                    edl.write(line)
                    edl.write(f"* FROM CLIP NAME: {os.path.basename(v_name)}\n\n")
                    
                    rec_in_sec = rec_out_sec
            
            report_state["edl_file"] = os.path.basename(edl_path)
            update_report("Complete", 100, "EDL Generated.")
            
            debug_s11 += f"\nEDL saved to: {edl_path}"

        # Prepare Outputs
        def to_tensor(imgs):
            if not imgs: return torch.zeros((1, 512, 512, 3))
            tensors = []
            base_w, base_h = imgs[0].size
            for img in imgs:
                if img.size != (base_w, base_h):
                    img = img.resize((base_w, base_h))
                i = np.array(img).astype(np.float32) / 255.0
                tensors.append(torch.from_numpy(i)[None,])
            return torch.cat(tensors, dim=0)

        return (
            to_tensor(env_imgs), to_tensor(prop_imgs), to_tensor(actor_imgs), to_tensor(scene_images),
            debug_s1, debug_s2, debug_s3, debug_s4, debug_s5, 
            debug_s6, debug_s7, debug_s8, debug_s9, debug_s10, debug_s11,
            cost_json, final_video_path
        )

    def _clean_json(self, text):
        text = text.strip()
        # Remove markdown code blocks if present
        if "```" in text:
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
        
        # Try to find JSON object or array
        start_obj = text.find('{')
        start_arr = text.find('[')
        
        if start_obj == -1 and start_arr == -1:
            return text # Return as is, probably fail
            
        if start_obj != -1 and (start_arr == -1 or start_obj < start_arr):
            # Object
            end = text.rfind('}')
            if end != -1: text = text[start_obj:end+1]
        else:
            # Array
            end = text.rfind(']')
            if end != -1: text = text[start_arr:end+1]
            
        return text.strip()

class SceneGenExtractor:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "timeline_data": ("STRING", {"forceInput": True, "tooltip": "Connect 'Timeline Data (S9)' output from Scene Gen node here."}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("Positive Prompts", "Negative Prompts", "Motion Prompts", "Start Frames", "End Frames", "Duration (Frames)", "Duration (Seconds)")
    OUTPUT_IS_LIST = (True, True, True, True, True, True, True)
    FUNCTION = "extract"
    CATEGORY = "Scene Gen"

    def extract(self, timeline_data):
        try:
            data = json.loads(timeline_data)
        except:
            print("SceneGenExtractor: Invalid JSON input")
            return ([], [], [], [], [], [], [])
        
        pos = []
        neg = []
        motion = []
        start_frames = []
        end_frames = []
        dur_frames = []
        dur_sec = []
        
        current_time = 0.0
        fps = 24.0 # Assumption based on SceneGen default
        
        for item in data:
            pos.append(item.get("positive_prompt", ""))
            neg.append(item.get("negative_prompt", ""))
            motion.append(item.get("video_trigger_prompt", ""))
            
            trim_dur = float(item.get("trim_duration", 5.0))
            
            s_frame = int(current_time * fps)
            e_frame = int((current_time + trim_dur) * fps)
            
            start_frames.append(str(s_frame))
            end_frames.append(str(e_frame))
            dur_frames.append(str(e_frame - s_frame))
            dur_sec.append(f"{trim_dur:.2f}")
            
            current_time += trim_dur
            
        return (pos, neg, motion, start_frames, end_frames, dur_frames, dur_sec)

class SceneGenVideoPlayer:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "video_path": ("STRING", {"forceInput": True, "tooltip": "Absolute path to the video file."}),
            }
        }

    RETURN_TYPES = ()
    FUNCTION = "play_video"
    CATEGORY = "Scene Gen"
    OUTPUT_NODE = True

    def play_video(self, video_path):
        # To display video in ComfyUI, it needs to be in the output directory or accessible via URL.
        output_dir = folder_paths.get_output_directory()
        
        # Normalize paths for comparison
        video_path_norm = os.path.normpath(video_path)
        output_dir_norm = os.path.normpath(output_dir)
        
        # Check if path is absolute and inside output_dir
        if os.path.isabs(video_path_norm) and video_path_norm.startswith(output_dir_norm):
            rel_path = os.path.relpath(video_path_norm, output_dir_norm)
            subfolder = os.path.dirname(rel_path)
            filename = os.path.basename(rel_path)
        else:
            # Fallback or if it's already a relative path (though we usually pass absolute)
            filename = os.path.basename(video_path)
            subfolder = ""
            
        return {"ui": {"videos": [{"filename": filename, "subfolder": subfolder, "type": "output"}]}}

class StringifyTextInput:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "text": ("STRING", {"multiline": True, "default": "", "dynamicPrompts": False}),
            }
        }

    RETURN_TYPES = ("STRING",)
    FUNCTION = "stringify"
    CATEGORY = "Scene Gen"

    def stringify(self, text):
        return (text,)

class SceneGenCostAnalyzer:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "cost_data": ("STRING", {"forceInput": True, "tooltip": "Connect 'Cost Data (JSON)' from Scene Gen node."}),
                "open_report": ("BOOLEAN", {"default": True, "tooltip": "Automatically open the report in browser after generation."}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("HTML Report Path",)
    FUNCTION = "analyze"
    CATEGORY = "Scene Gen"

    def analyze(self, cost_data, open_report):
        try:
            data = json.loads(cost_data)
        except:
            return ("Invalid JSON Data",)
            
        # Pricing Constants (Estimates)
        PRICE_GEMINI_INPUT = 3.50 / 1000000 # Per token (approx)
        PRICE_GEMINI_OUTPUT = 10.50 / 1000000 # Per token
        PRICE_GEMINI_IMAGE = 0.04 # Per image
        
        # Replicate Pricing (Approx per second)
        PRICE_REPLICATE = {
            "wan-video": 0.10,
            "kling": 0.10, # Per sec approx equivalent
            "hailuo": 0.05,
            "veo": 0.10,
            "omni-human": 0.05
        }
        DEFAULT_REP_PRICE = 0.08
        
        # Calculate Costs
        c_gem_in = data.get("gemini_text_input_tokens", 0) * PRICE_GEMINI_INPUT
        c_gem_out = data.get("gemini_text_output_tokens", 0) * PRICE_GEMINI_OUTPUT
        c_gem_img = data.get("gemini_images_generated", 0) * PRICE_GEMINI_IMAGE
        
        c_rep = 0.0
        rep_details = ""
        for model, seconds in data.get("replicate_seconds", {}).items():
            rate = DEFAULT_REP_PRICE
            for k, v in PRICE_REPLICATE.items():
                if k in model: rate = v; break
            cost = seconds * rate
            c_rep += cost
            rep_details += f"<tr><td>{model}</td><td>{seconds:.1f}s</td><td>${cost:.4f}</td></tr>"
            
        total_cost = c_gem_in + c_gem_out + c_gem_img + c_rep
        
        # Build Asset Gallery HTML
        subfolder = data.get("session_subfolder", "")
        assets_html = ""
        for asset in data.get("generated_assets", []):
            fpath = f"{subfolder}/{asset['file']}"
            assets_html += f"""
            <div class="gallery-item">
                <img src="{fpath}" alt="{asset['name']}" loading="lazy">
                <div class="caption">{asset['name']} ({asset['type']})</div>
            </div>
            """
            
        # Build Video Segments HTML
        segments_html = ""
        for seg in sorted(data.get("generated_segments", []), key=lambda x: x['index']):
            fpath = f"{subfolder}/{seg['file']}"
            segments_html += f"""
            <div class="video-item">
                <video controls preload="none">
                    <source src="{fpath}" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="caption">Segment {seg['index']}</div>
            </div>
            """

        # Generate HTML
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Scene Gen Report</title>
            <style>
                body {{ font-family: 'Segoe UI', sans-serif; background: #1a1a1a; color: #e0e0e0; margin: 0; padding: 20px; }}
                .container {{ width: 95%; max-width: none; margin: 0 auto; background: #252525; border-radius: 15px; overflow: hidden; box-shadow: 0 10px 30px rgba(0,0,0,0.5); }}
                .header {{ 
                    background: url('pablo-4k.jpeg') center/cover no-repeat; 
                    height: 250px; position: relative; 
                }}
                .header::after {{ content: ''; position: absolute; top:0; left:0; right:0; bottom:0; background: linear-gradient(to bottom, rgba(0,0,0,0.2), #252525); }}
                .title {{ position: absolute; bottom: 30px; left: 30px; z-index: 2; }}
                h1 {{ margin: 0; font-size: 3em; text-shadow: 0 2px 10px rgba(0,0,0,0.8); color: #fff; }}
                h2 {{ margin: 5px 0 0; font-size: 1.2em; color: #ccc; font-weight: 300; }}
                .content {{ padding: 40px; }}
                
                /* Tables */
                table {{ width: 100%; border-collapse: collapse; margin-bottom: 20px; background: #2a2a2a; border-radius: 8px; overflow: hidden; }}
                th, td {{ text-align: left; padding: 15px; border-bottom: 1px solid #333; word-break: normal; overflow-wrap: anywhere; }}
                th {{ background: #333; color: #aaa; font-weight: 600; text-transform: uppercase; font-size: 0.85em; letter-spacing: 1px; }}
                tr:last-child td {{ border-bottom: none; }}
                
                .total {{ font-size: 2.5em; color: #4CAF50; margin: 20px 0; text-align: right; font-weight: bold; }}
                
                /* Collapsible Sections */
                details {{ margin-bottom: 20px; background: #2a2a2a; border-radius: 8px; overflow: hidden; }}
                summary {{ padding: 15px; cursor: pointer; font-weight: bold; background: #333; user-select: none; outline: none; transition: background 0.2s; }}
                summary:hover {{ background: #3a3a3a; }}
                summary::-webkit-details-marker {{ color: #888; }}
                .details-content {{ padding: 20px; }}
                
                /* Grid Layouts */
                .gallery {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); gap: 15px; }}
                .gallery-item {{ background: #333; border-radius: 8px; overflow: hidden; transition: transform 0.2s; }}
                .gallery-item:hover {{ transform: translateY(-5px); }}
                .gallery-item img {{ width: 100%; height: 150px; object-fit: cover; display: block; }}
                
                .video-grid {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); gap: 20px; }}
                .video-item {{ background: #333; border-radius: 8px; overflow: hidden; }}
                .video-item video {{ width: 100%; display: block; }}
                
                .caption {{ padding: 8px; font-size: 0.8em; color: #aaa; text-align: center; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }}
                
                .footer {{ text-align: center; padding: 40px; border-top: 1px solid #333; background: #222; }}
                .btn-coffee {{ display: inline-block; transition: transform 0.2s; filter: drop-shadow(0 4px 6px rgba(0,0,0,0.3)); }}
                .btn-coffee:hover {{ transform: scale(1.05); }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <div class="title">
                        <h1>Generation Report</h1>
                        <h2>Scene Gen by Paul Lazniak</h2>
                    </div>
                </div>
                <div class="content">
                    
                    <div class="total">Total Estimated Cost: ${total_cost:.4f}</div>
                    
                    <details open>
                        <summary>üí∞ Cost Breakdown</summary>
                        <div class="details-content">
                            <table>
                                <tr><th>Resource</th><th>Usage</th><th>Cost</th></tr>
                                <tr><td>Gemini Text Input</td><td>{data.get("gemini_text_input_tokens", 0)} toks</td><td>${c_gem_in:.4f}</td></tr>
                                <tr><td>Gemini Text Output</td><td>{data.get("gemini_text_output_tokens", 0)} toks</td><td>${c_gem_out:.4f}</td></tr>
                                <tr><td>Gemini Images</td><td>{data.get("gemini_images_generated", 0)} imgs</td><td>${c_gem_img:.4f}</td></tr>
                                {rep_details}
                            </table>
                        </div>
                    </details>
                    
                    <details>
                        <summary>üñºÔ∏è Generated Assets ({len(data.get("generated_assets", []))})</summary>
                        <div class="details-content">
                            <div class="gallery">
                                {assets_html}
                            </div>
                        </div>
                    </details>
                    
                    <details>
                        <summary>üé¨ Video Segments ({len(data.get("generated_segments", []))})</summary>
                        <div class="details-content">
                            <div class="video-grid">
                                {segments_html}
                            </div>
                        </div>
                    </details>
                    
                </div>
                <div class="footer">
                    <p style="margin-bottom: 20px; font-size: 1.1em; color: #fff;">Enjoying Scene Gen? Support the development!</p>
                    <a href="https://www.buymeacoffee.com/EYB8tkx3tO" class="btn-coffee" target="_blank">
                        <img src="https://img.buymeacoffee.com/button-api/?text=Support project development&emoji=‚ù§Ô∏è&slug=EYB8tkx3tO&button_colour=FFDD00&font_colour=000000&font_family=Lato&outline_colour=000000&coffee_colour=ffffff" />
                    </a>
                </div>
            </div>
        </body>
        </html>
        """
        
        output_dir = folder_paths.get_output_directory()
        report_filename = f"scenegen_report_{int(time.time())}.html"
        report_path = os.path.join(output_dir, report_filename)
        
        with open(report_path, "w", encoding="utf-8") as f: f.write(html)
        
        if open_report:
            try:
                webbrowser.open(f"file://{os.path.abspath(report_path)}", new=2)
            except:
                pass
        
        return (report_path,)

NODE_CLASS_MAPPINGS = {
    'SceneGenNode': SceneGenNode,
    'SceneGenExtractor': SceneGenExtractor,
    'SceneGenVideoPlayer': SceneGenVideoPlayer,
    'StringifyTextInput': StringifyTextInput
}

NODE_DISPLAY_NAME_MAPPINGS = {
    'SceneGenNode': 'Scene Gen (Gemini)',
    'SceneGenExtractor': 'Scene Gen Data Extractor',
    'SceneGenVideoPlayer': 'Scene Gen Video Player',
    'StringifyTextInput': 'Stringify Text Input'
}

