import os
import sys
sys.path.append(os.path.dirname(os.path.realpath(__file__)))
import asyncio
import numpy as np
import torch
import google.generativeai as genai
from PIL import Image
import io
import scipy.io.wavfile
import json
import concurrent.futures
import replicate
import tempfile
import requests
import subprocess
import shutil
import random
import time
import webbrowser
import base64
import folder_paths

class SceneGenNode:
    def __init__(self):
        # Locate ComfyUI output directory
        self.output_dir = folder_paths.get_output_directory()

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "The input audio file (WAV/MP3) to analyze and generate video for."}),
                "gemini_api_key": ("STRING", {"multiline": False, "default": "", "tooltip": "Your Google Gemini API Key. Required for analysis and prompt generation."}),
                "replicate_api_token": ("STRING", {"multiline": False, "default": "", "tooltip": "Your Replicate API Token. Required for video generation models."}),
                "prompt_instruction": ("STRING", {"multiline": True, "default": "Describe a scene matching the music.", "tooltip": "Main instruction for the AI. Describe the desired mood, style, story, or specific visual elements."}),
                "filename_prefix": ("STRING", {"default": "scene_gen", "tooltip": "Prefix for all generated files (video, images, logs)."}),
                "fps": ("FLOAT", {"default": 24.0, "min": 1.0, "max": 120.0, "step": 0.1, "tooltip": "Frame rate of the final output video."}),
                "render_mode": (["Full Render", "Prompt Mode"], {"default": "Full Render", "tooltip": "Full Render: Generates video using Replicate. Prompt Mode: Generates prompts and assets, then creates a slideshow from start frames (skips Replicate)."}),
                "model_text": ("STRING", {"default": "gemini-3-pro-preview", "tooltip": "Gemini model used for text analysis, scripting, and prompting."}),
                "model_image": ("STRING", {"default": "gemini-3-pro-image-preview", "tooltip": "Gemini model used for generating start frames and assets."}),
                "creativity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "0.0 = Strict adherence to prompt. 1.0 = High hallucination/creative freedom."}),
                "dynamicity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "0.0 = Slow, static, contemplative shots. 1.0 = Fast cuts, high movement, intense action."}),
                "video_quality": (["Low", "Medium", "High"], {"default": "Medium", "tooltip": "Controls resolution and quality settings for video models. High = 1080p (where available), Medium = 720p/768p, Low = 480p."}),
                "aspect_ratio": (["16:9", "1:1", "9:16", "4:3"], {"default": "16:9", "tooltip": "Aspect ratio of the generated video."}),
                "resolution_multiplier": ("FLOAT", {"default": 1.0, "min": 0.5, "max": 2.0, "step": 0.1, "tooltip": "Scales the resolution of generated start frames."}),
                "enable_prompt_expansion": ("BOOLEAN", {"default": True, "tooltip": "If True, the AI will expand your simple instructions into highly detailed visual prompts."}),
                "save_segments": ("BOOLEAN", {"default": False, "tooltip": "If True, saves every individual video clip generated by Replicate to the output folder."}),
                "save_images": ("BOOLEAN", {"default": False, "tooltip": "If True, saves the start frame images generated by Gemini."}),
                "save_assets": ("BOOLEAN", {"default": True, "tooltip": "If True, saves the generated asset images (characters, props, environments)."}),
                "gemini_concurrency": ("INT", {"default": 5, "min": 1, "max": 50, "tooltip": "Max parallel requests to Gemini API."}),
                "replicate_concurrency": ("INT", {"default": 10, "min": 1, "max": 50, "tooltip": "Max parallel video generation jobs on Replicate."}),
                "use_wan_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Wan 2.5 Fast model."}),
                "use_wan_2_5": ("BOOLEAN", {"default": True, "tooltip": "Enable Wan 2.5 Standard model."}),
                "use_kling_turbo": ("BOOLEAN", {"default": True, "tooltip": "Enable Kling v2.5 Turbo model."}),
                "use_omni_human": ("BOOLEAN", {"default": True, "tooltip": "Enable OmniHuman model (good for realistic human movement)."}),
                "use_hailuo": ("BOOLEAN", {"default": True, "tooltip": "Enable Hailuo 2.3 model."}),
                "use_hailuo_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Hailuo 2.3 Fast model."}),
                "use_veo_3_1": ("BOOLEAN", {"default": True, "tooltip": "Enable Google Veo 3.1 model."}),
                "use_veo_3_1_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Google Veo 3.1 Fast model."}),
                "aggressive_edit": ("BOOLEAN", {"default": False, "tooltip": "If True, forces fast-paced editing with cuts strictly on beat. Generates full clips but trims them aggressively."}),
                "word_influence": ("FLOAT", {"default": 0.0, "min": -1.0, "max": 1.0, "step": 0.1, "tooltip": "1.0 = Literal visualization of lyrics. -1.0 = Ignore lyrics, focus on vibe/atmosphere. 0.0 = Balanced."}),
                "save_edl": ("BOOLEAN", {"default": True, "tooltip": "If True, exports a CMX 3600 .edl file for importing the timeline into Premiere Pro/DaVinci Resolve."}),
                "open_coffee_link": ("BOOLEAN", {"default": True, "tooltip": "Support the creator! Opens Buy Me a Coffee page after generation."}),
                "render_mode": (["Full Render", "Prompt Mode"], {"default": "Full Render", "tooltip": "Full Render: Generates video using Replicate. Prompt Mode: Generates prompts and assets, then creates a slideshow from start frames (skips Replicate)."}),
                "open_report": ("BOOLEAN", {"default": True, "tooltip": "Opens a live HTML report that updates in real-time during generation."}),
            },
            "optional": {
                "reference_images": ("IMAGE", {"tooltip": "Optional images to use as references for style, characters, or environments."}),
            }
        }

    RETURN_TYPES = (
        "IMAGE", "IMAGE", "IMAGE", "IMAGE",
        "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING",
        "STRING", "STRING"
    )
    RETURN_NAMES = (
        "Environment Images", "Asset Images", "Actor Images", "Scene Start Frames",
        "Analysis (S1)", "Style (S2)", "Palette (S3)", "Assets (S4)", "Montage (S5)", 
        "Prompts (S6)", "Start Frames Info (S7)", "Motion Refinement (S8)", "Timeline Data (S9)", "Generation Status (S10)", "Stitching Info (S11)",
        "Cost Data (JSON)", "Final Video Path"
    )
    FUNCTION = "process"
    CATEGORY = "Scene Gen"

    def process(self, audio, gemini_api_key, replicate_api_token, prompt_instruction, filename_prefix, fps, model_text, model_image, creativity, dynamicity, video_quality, aspect_ratio, resolution_multiplier, enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency, use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, aggressive_edit, word_influence, save_edl, open_coffee_link, render_mode, open_report, reference_images=None):
        print(f"\n[SceneGen] === Starting Iterative Process ===")
        
        if not gemini_api_key: raise ValueError("Gemini API Key is required.")
        if not replicate_api_token: raise ValueError("Replicate API Token is required.")
        
        os.environ["REPLICATE_API_TOKEN"] = replicate_api_token
        genai.configure(api_key=gemini_api_key)

        # Setup Session
        timestamp = int(time.time())
        session_dir = os.path.join(self.output_dir, f"{filename_prefix}_{timestamp}")
        os.makedirs(session_dir, exist_ok=True)
        
        # Process Audio
        waveform = audio['waveform']
        sample_rate = audio['sample_rate']
        if waveform.dim() == 3: waveform = waveform.squeeze(0)
        audio_np = waveform.cpu().numpy()
        
        # Ensure correct shape (samples, channels)
        if audio_np.shape[0] < audio_np.shape[1]: 
            audio_np = audio_np.T
            
        # Safe conversion to int16 to prevent clipping/wrapping
        # Clip to [-1, 1] range first to avoid overflow artifacts
        audio_np = np.clip(audio_np, -1.0, 1.0)
        audio_int16 = (audio_np * 32767).astype(np.int16)
        
        # Handle Reference Images
        ref_images_pil = []
        if reference_images is not None:
            for img_tensor in reference_images:
                i = 255. * img_tensor.cpu().numpy()
                img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
                ref_images_pil.append(img)

        # Run Async Pipeline
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, self.async_process(
                audio_int16, sample_rate, prompt_instruction, filename_prefix, session_dir, fps, 
                model_text, model_image, creativity, dynamicity, video_quality, aspect_ratio, resolution_multiplier,
                enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency,
                use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, 
                aggressive_edit, word_influence, save_edl, ref_images_pil, render_mode, open_report
            ))
            result = future.result()
            
            if open_coffee_link:
                try:
                    webbrowser.open("https://buymeacoffee.com/eyb8tkx3to", new=2)
                except:
                    pass

            print(f"[SceneGen] === Process Complete ===\n")
            return result

    async def async_process(self, audio_data, sample_rate, instruction, prefix, session_dir, fps, model_text_name, model_image_name, creativity, dynamicity, video_quality, aspect_ratio, resolution_multiplier, enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency, use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, aggressive_edit, word_influence, save_edl, ref_images_pil, render_mode, open_report):
        
        # Usage Tracking
        usage_stats = {
            "gemini_text_input_tokens": 0,
            "gemini_text_output_tokens": 0,
            "gemini_images_generated": 0,
            "replicate_seconds": {},
            "session_subfolder": os.path.relpath(session_dir, self.output_dir),
            "generated_assets": [],
            "generated_segments": []
        }

        # --- Live Reporting Setup ---
        # --- Live Reporting Setup ---
        report_filename = f"report_{prefix}.html"
        report_js_filename = f"report_data_{prefix}.js"
        
        # Save report INSIDE the session directory
        report_path = os.path.join(session_dir, report_filename)
        report_js_path = os.path.join(session_dir, report_js_filename)
        
        # Paths for JS (relative to the HTML file)
        js_filename = report_js_filename
        js_session_path = "" # Assets are in the same folder
        
        # Embed Header Image (Base64) to avoid path issues
        header_bg_style = "background-color: #333;" # Fallback
        try:
            node_dir = os.path.dirname(os.path.realpath(__file__))
            img_path = os.path.join(node_dir, "pablo-4k.jpeg")
            if os.path.exists(img_path):
                with open(img_path, "rb") as img_f:
                    img_data = img_f.read()
                    b64_img = base64.b64encode(img_data).decode('utf-8')
                    header_bg_style = f"background: url('data:image/jpeg;base64,{b64_img}') center/cover no-repeat;"
                    print(f"[SceneGen] Header image loaded: {len(img_data)} bytes from {img_path}")
            else:
                print(f"[SceneGen] Header image not found at: {img_path}")
        except Exception as e:
            print(f"[SceneGen] Failed to embed header image: {e}")

        report_state = {
            "status": "Initializing...",
            "progress": 0,
            "logs": ["Process Started."],
            "costs": usage_stats,
            "assets": [],
            "videos": [],
            "session_path": js_session_path,
            "final_video": None,
            "montage": [],
            "style": {},
            "palette": {},
            "analysis": "",
            "edl_file": None
        }

        def update_report(status=None, progress=None, log=None):
            if status: report_state["status"] = status
            if progress is not None: report_state["progress"] = progress
            if log: 
                print(f"[SceneGen] {log}")
                report_state["logs"].append(f"[{time.strftime('%H:%M:%S')}] {log}")
            
            # Update costs in state
            report_state["costs"] = usage_stats
            
            try:
                # Write as JSONP / JS function call to bypass CORS
                json_str = json.dumps(report_state, indent=2)
                js_content = f"receiveData({json_str});"
                with open(report_js_path, "w", encoding="utf-8") as f:
                    f.write(js_content)
            except Exception as e:
                print(f"Report Update Error: {e}")

        # Create HTML File IMMEDIATELY
        html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scene Gen Live Report - {prefix}</title>
    <style>
        body {{ font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; background-color: #121212; color: #e0e0e0; margin: 0; padding: 0; }}
        .container {{ max-width: 1400px; margin: 0 auto; padding: 20px; }}
        
        /* Header */
        header {{ {header_bg_style} height: 350px; position: relative; border-radius: 0 0 15px 15px; overflow: hidden; box-shadow: 0 4px 20px rgba(0,0,0,0.5); }}
        header::after {{ content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(to bottom, rgba(0,0,0,0.1), #121212); }}
        .header-content {{ position: absolute; bottom: 20px; left: 20px; z-index: 10; display: flex; justify-content: space-between; align-items: flex-end; width: calc(100% - 40px); }}
        .title-group h1 {{ margin: 0; font-size: 2.5rem; text-shadow: 0 2px 4px rgba(0,0,0,0.8); }}
        .title-group .subtitle {{ font-size: 1.1rem; opacity: 0.8; }}
        .gh-links {{ display: flex; gap: 10px; }}
        .gh-btn {{ background: rgba(255,255,255,0.1); backdrop-filter: blur(5px); color: white; text-decoration: none; padding: 8px 15px; border-radius: 20px; font-size: 0.9rem; border: 1px solid rgba(255,255,255,0.2); transition: all 0.2s; display: flex; align-items: center; gap: 5px; }}
        .gh-btn:hover {{ background: rgba(255,255,255,0.2); transform: translateY(-2px); }}
        
        /* Status Bar */
        .status-bar {{ background: #1e1e1e; padding: 15px; margin-top: 20px; border-radius: 10px; display: flex; align-items: center; justify-content: space-between; box-shadow: 0 2px 10px rgba(0,0,0,0.2); }}
        .status-text {{ font-size: 1.2rem; font-weight: bold; color: #4caf50; }}
        .progress-container {{ flex-grow: 1; margin: 0 20px; background: #333; height: 10px; border-radius: 5px; overflow: hidden; }}
        .progress-bar {{ height: 100%; background: #4caf50; width: 0%; transition: width 0.5s ease; }}
        
        /* Layout */
        .grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 20px; }}
        .card {{ background: #1e1e1e; border-radius: 10px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.2); display: flex; flex-direction: column; }}
        .card h3 {{ margin-top: 0; border-bottom: 1px solid #333; padding-bottom: 10px; display: flex; align-items: center; justify-content: flex-start; gap: 10px; }}
        
        /* Final Video */
        .final-video-container {{ width: 100%; aspect-ratio: 16/9; background: #000; border-radius: 5px; overflow: hidden; display: flex; align-items: center; justify-content: center; }}
        .final-video-container video {{ width: 100%; height: 100%; }}
        .placeholder-text {{ color: #555; }}

        /* Timeline */
        .timeline-wrapper {{ overflow-x: auto; padding-bottom: 10px; }}
        .timeline {{ display: flex; height: 120px; background: #222; border-radius: 5px; position: relative; min-width: 100%; }}
        .timeline-segment {{ position: relative; height: 100%; border-right: 1px solid #444; overflow: hidden; transition: flex 0.3s; min-width: 40px; }}
        .timeline-segment:hover {{ filter: brightness(1.2); }}
        .timeline-segment img {{ width: 100%; height: 100%; object-fit: cover; opacity: 0.6; }}
        .timeline-segment .seg-label {{ position: absolute; bottom: 5px; left: 5px; font-size: 0.7rem; background: rgba(0,0,0,0.7); padding: 2px 4px; border-radius: 3px; pointer-events: none; }}
        .timeline-segment .seg-time {{ position: absolute; top: 5px; left: 5px; font-size: 0.6rem; color: #aaa; pointer-events: none; }}

        /* Data Visualization */
        .data-display {{ font-size: 0.9rem; color: #ccc; max-height: 300px; overflow-y: auto; }}
        .kv-row {{ display: flex; border-bottom: 1px solid #333; padding: 5px 0; }}
        .kv-key {{ font-weight: bold; width: 120px; color: #888; flex-shrink: 0; }}
        .kv-val {{ flex-grow: 1; }}
        
        .palette-container {{ display: flex; flex-wrap: wrap; gap: 10px; margin-top: 10px; }}
        .color-swatch {{ width: 60px; height: 60px; border-radius: 8px; display: flex; align-items: center; justify-content: center; font-size: 0.7rem; color: #fff; text-shadow: 0 1px 2px rgba(0,0,0,0.8); box-shadow: 0 2px 5px rgba(0,0,0,0.3); transition: transform 0.2s; cursor: pointer; }}
        .color-swatch:hover {{ transform: scale(1.1); z-index: 10; }}

        /* Logs & Galleries */
        .log-box {{ background: #000; font-family: 'Consolas', monospace; padding: 10px; height: 200px; overflow-y: auto; border-radius: 5px; font-size: 0.85rem; color: #aaa; }}
        .log-entry {{ margin-bottom: 5px; border-bottom: 1px solid #222; padding-bottom: 2px; }}
        
        .gallery {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(120px, 1fr)); gap: 10px; max-height: 300px; overflow-y: auto; }}
        .gallery-item {{ position: relative; border-radius: 5px; overflow: hidden; aspect-ratio: 16/9; background: #333; transition: transform 0.2s; }}
        .gallery-item:hover {{ transform: scale(1.05); z-index: 5; }}
        .gallery-item img, .gallery-item video {{ width: 100%; height: 100%; object-fit: cover; }}
        .gallery-item .label {{ position: absolute; bottom: 0; left: 0; right: 0; background: rgba(0,0,0,0.7); font-size: 0.6rem; padding: 2px 5px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }}
        
        .cost-table {{ width: 100%; border-collapse: collapse; font-size: 0.9rem; }}
        .cost-table td, .cost-table th {{ padding: 8px; border-bottom: 1px solid #333; text-align: left; }}
        .total-cost {{ font-size: 1.5rem; color: #ffeb3b; text-align: right; margin-top: 10px; font-weight: bold; }}
        
        .footer {{ text-align: center; margin-top: 40px; padding: 20px; border-top: 1px solid #333; }}
        .btn-coffee {{ display: inline-block; transition: transform 0.2s; }}
        .btn-coffee:hover {{ transform: scale(1.05); }}

        /* Loaders */
        .loading-placeholder {{ display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100%; min-height: 150px; color: #666; }}
        .spinner {{ width: 40px; height: 40px; border: 4px solid rgba(255,255,255,0.1); border-left-color: #4caf50; border-radius: 50%; animation: spin 1s linear infinite; margin-bottom: 10px; }}
        @keyframes spin {{ 0% {{ transform: rotate(0deg); }} 100% {{ transform: rotate(360deg); }} }}

        /* Tooltips & Overlays */
        .gallery-item .overlay {{ position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: rgba(0,0,0,0.6); opacity: 0; transition: opacity 0.2s; display: flex; align-items: center; justify-content: center; }}
        .gallery-item:hover .overlay {{ opacity: 1; }}
        .download-icon {{ color: white; font-size: 1.5rem; text-decoration: none; background: rgba(0,0,0,0.5); padding: 10px; border-radius: 50%; transition: transform 0.2s; }}
        .download-icon:hover {{ transform: scale(1.1); background: #4caf50; }}
        
        .btn-action {{ background: #4caf50; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; font-weight: bold; text-decoration: none; display: inline-block; margin-top: 10px; }}
        .btn-action:hover {{ background: #45a049; }}
        .btn-action.disabled {{ background: #333; color: #555; cursor: not-allowed; pointer-events: none; }}

        /* Help System */
        .card {{ position: relative; }}
        .help-btn {{ position: absolute; top: 15px; right: 15px; width: 24px; height: 24px; border-radius: 50%; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.3); color: #aaa; font-size: 0.8rem; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s; z-index: 20; }}
        .help-btn {{ position: absolute; top: 15px; right: 15px; width: 24px; height: 24px; border-radius: 50%; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.3); color: #aaa; font-size: 0.8rem; cursor: pointer; display: flex; align-items: center; justify-content: center; transition: all 0.2s; z-index: 20; }}
        .help-btn:hover {{ background: #4caf50; color: white; border-color: #4caf50; }}
        
        .copy-btn {{ cursor: pointer; margin-right: 8px; font-size: 1rem; opacity: 0.7; transition: opacity 0.2s; background: none; border: none; color: #ccc; padding: 0; }}
        .copy-btn:hover {{ opacity: 1; color: #fff; }}

        .palette-item {{ display: flex; flex-direction: column; align-items: center; width: 100px; margin-bottom: 10px; cursor: pointer; transition: transform 0.2s; }}
        .palette-item:hover {{ transform: scale(1.05); }}
        .palette-swatch {{ width: 100%; height: 60px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.3); margin-bottom: 5px; }}
        .palette-info {{ font-size: 0.7rem; text-align: center; color: #aaa; line-height: 1.2; }}
        .palette-hex {{ font-weight: bold; color: #fff; margin-bottom: 2px; }}
        
        /* Modal */
        .modal {{ display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0,0,0,0.8); backdrop-filter: blur(5px); }}
        .modal-content {{ background-color: #1e1e1e; margin: 10% auto; padding: 30px; border: 1px solid #333; width: 80%; max-width: 600px; border-radius: 10px; box-shadow: 0 5px 30px rgba(0,0,0,0.5); position: relative; }}
        .close-modal {{ color: #aaa; float: right; font-size: 28px; font-weight: bold; cursor: pointer; }}
        .close-modal:hover {{ color: white; }}
        .modal h2 {{ margin-top: 0; color: #4caf50; border-bottom: 1px solid #333; padding-bottom: 10px; }}
        .modal p {{ line-height: 1.6; color: #ccc; }}

    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="title-group">
                    <h1>Scene Gen Report</h1>
                    <div class="subtitle">Project: {prefix}</div>
                </div>
                <div class="gh-links">
                    <a href="https://www.youtube.com/channel/UCHotSlu3SDgp35lat92ExIQ" target="_blank" class="gh-btn" style="background: rgba(255,0,0,0.2); border-color: rgba(255,0,0,0.4);">üì∫ YouTube</a>
                    <a href="https://github.com/lazniak/scene_gen" target="_blank" class="gh-btn">‚≠ê Star on GitHub</a>
                    <a href="https://github.com/lazniak" target="_blank" class="gh-btn">üë§ Follow @lazniak</a>
                </div>
            </div>
        </header>
        
        <div class="status-bar">
            <div id="status" class="status-text">Initializing...</div>
            <div class="progress-container">
                <div id="progress" class="progress-bar"></div>
            </div>
            <div id="progress-text">0%</div>
        </div>
        
        <!-- Final Video Section -->
        <div class="card" style="margin-top: 20px;">
            <button class="help-btn" onclick="showHelp('final_video')">?</button>
            <h3>
                Final Video Output
            </h3>
            <div class="final-video-container" id="final-video-box">
                <div class="loading-placeholder">
                    <div class="spinner"></div>
                    <div>Waiting for video generation...</div>
                </div>
            </div>
        </div>

        <!-- Timeline Section -->
        <div class="card" style="margin-top: 20px;">
            <button class="help-btn" onclick="showHelp('timeline')">?</button>
            <h3>
                <button class="copy-btn" onclick="copyData('montage')" title="Copy Montage JSON">üìã</button>
                Montage Timeline
                <a id="btn-dl-edl" href="#" class="btn-action disabled" download style="margin-left: auto;">Download EDL</a>
            </h3>
            <div class="timeline-wrapper">
                <div id="timeline" class="timeline">
                    <div style="width:100%; display:flex; align-items:center; justify-content:center; color:#555;">
                        <div class="loading-placeholder" style="min-height:auto;">
                            <div class="spinner" style="width:20px; height:20px; border-width:2px;"></div>
                            <div style="font-size:0.8rem;">Waiting for montage plan...</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="grid">
            <!-- Style & Analysis -->
            <div class="card">
                <button class="help-btn" onclick="showHelp('analysis')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('analysis')" title="Copy Analysis">üìã</button>
                    Audio Analysis (S1)
                </h3>
                <div id="analysis-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Analyzing audio...</div>
                    </div>
                </div>
            </div>

            <div class="card">
                <button class="help-btn" onclick="showHelp('style')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('style')" title="Copy Style JSON">üìã</button>
                    Style & Atmosphere (S2)
                </h3>
                <div id="style-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Defining style...</div>
                    </div>
                </div>
            </div>
            
            <!-- Color Palette -->
            <div class="card">
                <button class="help-btn" onclick="showHelp('palette')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('palette')" title="Copy Palette JSON">üìã</button>
                    Color Palette (S3)
                </h3>
                <div id="palette-display" class="palette-container" style="min-height:100px; align-items:center; justify-content:center;">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Generating palette...</div>
                    </div>
                </div>
            </div>
        </div>

        <div class="grid">
            <!-- Prompts -->
            <div class="card">
                <button class="help-btn" onclick="showHelp('prompts')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('prompts')" title="Copy Prompts">üìã</button>
                    Scene Prompts (S6)
                </h3>
                <div id="prompts-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Generating prompts...</div>
                    </div>
                </div>
            </div>

            <!-- Motion -->
            <div class="card">
                <button class="help-btn" onclick="showHelp('motion')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('motion')" title="Copy Motion Data">üìã</button>
                    Motion Logic (S8)
                </h3>
                <div id="motion-display" class="data-display">
                    <div class="loading-placeholder">
                        <div class="spinner"></div>
                        <div>Refining motion...</div>
                    </div>
                </div>
            </div>

            <!-- Costs -->
            <div class="card">
                <button class="help-btn" onclick="showHelp('costs')">?</button>
                <h3>
                    <button class="copy-btn" onclick="copyData('costs')" title="Copy Cost Data">üìã</button>
                    Estimated Costs
                </h3>
                <div id="costs">Calculating...</div>
            </div>
        </div>
        
        <div id="dynamic-assets-container" class="grid">
            <!-- Dynamic Asset Categories will be injected here -->
            <div class="card">
                <button class="help-btn" onclick="showHelp('assets')">?</button>
                <h3>Generated Assets</h3>
                <div id="assets-gallery" class="gallery"></div>
            </div>
        </div>
        
        <div class="grid">
            <div class="card">
                <button class="help-btn" onclick="showHelp('segments')">?</button>
                <h3>Video Segments</h3>
                <div id="videos-gallery" class="gallery"></div>
            </div>
             <div class="card">
                <button class="help-btn" onclick="showHelp('logs')">?</button>
                <h3>Live Logs</h3>
                <div id="logs" class="log-box"></div>
            </div>
        </div>
        
        <div class="footer">
            <p>Enjoying Scene Gen? Support the development!</p>
            <a href="https://www.buymeacoffee.com/EYB8tkx3tO" class="btn-coffee" target="_blank">
                <img src="https://img.buymeacoffee.com/button-api/?text=Support project development&emoji=‚ù§Ô∏è&slug=EYB8tkx3tO&button_colour=FFDD00&font_colour=000000&font_family=Lato&outline_colour=000000&coffee_colour=ffffff" />
            </a>
        </div>
    </div>

    <!-- Help Modal -->
    <div id="helpModal" class="modal">
        <div class="modal-content">
            <span class="close-modal" onclick="closeHelp()">&times;</span>
            <h2 id="helpTitle">Help</h2>
            <div id="helpBody"></div>
        </div>
    </div>

    <script>
        const jsPath = '{js_filename}';
        const sessionPath = '{js_session_path}';
        
        const helpContent = {{
            "final_video": {{
                title: "Final Video Output",
                text: "This is the result of the entire generation process. It combines all generated video segments, stitched together and synchronized with your original audio. You can download the Edit Decision List (EDL) to import this timeline into professional video editing software like DaVinci Resolve or Premiere Pro."
            }},
            "timeline": {{
                title: "Montage Timeline",
                text: "Visualizes the structure of your video. Each block represents a scene generated by the AI. The width corresponds to the duration. Hover over segments to see the specific prompt and description used to generate that shot."
            }},
            "style": {{
                title: "Style & Atmosphere",
                text: "Derived from your audio analysis and instructions. This defines the visual language of the video, including mood, lighting, and artistic direction. The AI uses this 'Style Bible' to ensure consistency across all generated assets."
            }},
            "palette": {{
                title: "Color Palette",
                text: "The color scheme generated for your video to match the mood of the music. These colors are injected into the image generation prompts to maintain color harmony throughout the sequence."
            }},
            "costs": {{
                title: "Estimated Costs",
                text: "A real-time estimate of the API costs for this session. It tracks token usage for Gemini (Text/Image) and generation time for Replicate video models. Actual costs may vary slightly based on provider pricing changes."
            }},
            "assets": {{
                title: "Generated Assets",
                text: "The collection of static images generated by Gemini to serve as 'Start Frames' or reference sheets for the video. These images guide the video models to ensure the characters and environments look consistent."
            }},
            "segments": {{
                title: "Video Segments",
                text: "Individual video clips generated by models like Kling, Hailuo, or Veo. Each clip corresponds to a section of the timeline. You can download individual clips here if you want to re-edit them manually."
            }},
            "analysis": {{
                title: "Audio Analysis (S1)",
                text: "The initial analysis of your audio file by Gemini. It identifies genre, mood, lyrics, and structural elements to guide the entire generation process."
            }},
            "prompts": {{
                title: "Scene Prompts (S6)",
                text: "The specific visual prompts generated for each scene. These are constructed by combining the Style Bible, Asset descriptions, and the specific narrative moment."
            }},
            "motion": {{
                title: "Motion Logic (S8)",
                text: "The logic used to determine camera movement and subject action for each shot, ensuring the video flows dynamically and matches the energy of the audio."
            }},
            "logs": {{
                title: "Live Logs",
                text: "A real-time stream of the internal operations of the Scene Gen node. Useful for monitoring progress and debugging if something seems stuck."
            }}
        }};

        function showHelp(key) {{
            const data = helpContent[key];
            if (data) {{
                document.getElementById('helpTitle').innerText = data.title;
                document.getElementById('helpBody').innerText = data.text;
                document.getElementById('helpModal').style.display = "block";
            }}
        }}

        function closeHelp() {{
            document.getElementById('helpModal').style.display = "none";
        }}
        
        window.onclick = function(event) {{
            if (event.target == document.getElementById('helpModal')) {{
                closeHelp();
            }}
        }}

        function formatCost(stats) {{
            const P_GEM_IN = 3.50 / 1000000;
            const P_GEM_OUT = 10.50 / 1000000;
            const P_GEM_IMG = 0.04;
            const P_REP_DEF = 0.08;
            
            let c_gem = (stats.gemini_text_input_tokens * P_GEM_IN) + 
                        (stats.gemini_text_output_tokens * P_GEM_OUT) + 
                        (stats.gemini_images_generated * P_GEM_IMG);
            
            let c_rep = 0;
            let rep_html = '';
            for (const [model, secs] of Object.entries(stats.replicate_seconds || {{}})) {{
                let rate = P_REP_DEF;
                if (model.includes('hailuo') || model.includes('omni')) rate = 0.05;
                if (model.includes('wan') || model.includes('kling') || model.includes('veo')) rate = 0.10;
                let cost = secs * rate;
                c_rep += cost;
                rep_html += `<tr><td>${{model}}</td><td>${{secs.toFixed(1)}}s</td><td>$${{cost.toFixed(4)}}</td></tr>`;
            }}
            
            let total = c_gem + c_rep;
            
            return `
                <table class="cost-table">
                    <tr><th>Item</th><th>Qty</th><th>Est. Cost</th></tr>
                    <tr><td>Gemini Text</td><td>${{stats.gemini_text_input_tokens + stats.gemini_text_output_tokens}} toks</td><td>$${{(stats.gemini_text_input_tokens * P_GEM_IN + stats.gemini_text_output_tokens * P_GEM_OUT).toFixed(4)}}</td></tr>
                    <tr><td>Gemini Images</td><td>${{stats.gemini_images_generated}}</td><td>$${{(stats.gemini_images_generated * P_GEM_IMG).toFixed(4)}}</td></tr>
                    ${{rep_html}}
                </table>
                <div class="total-cost">Total: $${{total.toFixed(4)}}</div>
            `;
        }}

        let globalData = {{}};

        function copyData(key) {{
            let data = null;
            if (key === 'montage') data = globalData.montage;
            if (key === 'style') data = globalData.style;
            if (key === 'palette') data = globalData.palette;
            if (key === 'costs') data = globalData.costs;
            if (key === 'analysis') data = globalData.analysis;
            if (key === 'prompts') data = globalData.prompts;
            if (key === 'motion') data = globalData.motion;
            
            if (data) {{
                const text = JSON.stringify(data, null, 2);
                navigator.clipboard.writeText(text).then(() => {{
                    alert("JSON copied to clipboard!");
                }});
            }}
        }}

        function copyText(text) {{
            navigator.clipboard.writeText(text).then(() => {{
                // Optional: visual feedback
            }});
        }}

        function renderTimeline(montage, assets) {{
            if (!montage) return;
            if (typeof montage === 'string') {{
                try {{ montage = JSON.parse(montage); }} catch(e) {{ return; }}
            }}
            if (!Array.isArray(montage) || montage.length === 0) return;
            
            const container = document.getElementById('timeline');
            // Only clear if it contains loading placeholder
            if (container.querySelector('.loading-placeholder')) container.innerHTML = '';
            
            // Force re-render to ensure updates
            container.innerHTML = '';
            
            let totalDur = montage.reduce((acc, seg) => acc + (seg.duration || 4), 0);
            if (totalDur === 0) totalDur = 1;

            montage.forEach((seg, idx) => {{
                const dur = seg.duration || 4;
                const pct = (dur / totalDur) * 100;
                
                const div = document.createElement('div');
                div.className = 'timeline-segment';
                div.style.width = pct + '%';
                div.title = `Scene ${{idx}}: ${{seg.description}} (${{dur}}s)\nPrompt: ${{seg.video_trigger_prompt || 'Pending...'}}`;
                
                let imgUrl = '';
                if (seg.assets && seg.assets.length > 0) {{
                    const assetName = seg.assets[0];
                    const assetObj = assets.find(a => a.name === assetName || a.name.includes(assetName));
                    if (assetObj) imgUrl = sessionPath + assetObj.file;
                }}
                
                if (imgUrl) {{
                    div.innerHTML = `<img src="${{imgUrl}}"><div class="seg-label">Sc ${{idx}}</div><div class="seg-time">${{dur}}s</div>`;
                }} else {{
                    div.innerHTML = `<div class="seg-label">Sc ${{idx}}</div><div class="seg-time">${{dur}}s</div>`;
                    div.style.background = `hsl(${{idx * 40}}, 50%, 30%)`;
                }}
                
                container.appendChild(div);
            }});
        }}

        function renderStyle(styleData) {{
            if (!styleData || Object.keys(styleData).length === 0) return;
            const container = document.getElementById('style-display');
            if (container.querySelector('.loading-placeholder')) container.innerHTML = '';
            
            let html = '';
            for (const [k, v] of Object.entries(styleData)) {{
                if (typeof v === 'string') {{
                    html += `<div class="kv-row"><div class="kv-key">${{k}}</div><div class="kv-val">${{v}}</div></div>`;
                }}
            }}
            if (html && container.innerHTML !== html) container.innerHTML = html;
        }}

        function renderGenericText(data, elementId) {{
            if (!data) return;
            const container = document.getElementById(elementId);
            if (container.querySelector('.loading-placeholder')) container.innerHTML = '';
            
            let html = '';
            if (typeof data === 'object') {{
                html = `<pre style="white-space: pre-wrap; font-family: Consolas, monospace; font-size: 0.8rem; color: #aaa;">${{JSON.stringify(data, null, 2)}}</pre>`;
            }} else {{
                html = `<div style="white-space: pre-wrap; font-family: 'Segoe UI', sans-serif; font-size: 0.9rem; color: #ccc;">${{data}}</div>`;
            }}
            
            if (html && container.innerHTML !== html) container.innerHTML = html;
        }}

        function renderPalette(paletteData) {{
            if (!paletteData || !paletteData.palette) return;
            const container = document.getElementById('palette-display');
            if (container.querySelector('.loading-placeholder')) container.innerHTML = '';
            
            // Regex to parse: #HEX (Annotation)
            const regex = /#([0-9A-Fa-f]{6})(.*)/;
            
            const html = paletteData.palette.map(c => {{
                let hex = c;
                let annotation = "";
                const match = c.match(regex);
                if (match) {{
                    hex = "#" + match[1];
                    annotation = match[2].replace(/[()]/g, "").trim();
                }}
                
                return `
                <div class="palette-item" onclick="copyText('${{hex}}')" title="Click to Copy Hex">
                    <div class="palette-swatch" style="background:${{hex}}"></div>
                    <div class="palette-hex">${{hex}}</div>
                    <div class="palette-info">${{annotation}}</div>
                </div>`;
            }}).join('');
            
            if (html && container.innerHTML !== html) container.innerHTML = html;
        }}

        function receiveData(data) {{
            globalData = data;
            document.getElementById('status').innerText = data.status;
            document.getElementById('progress').style.width = data.progress + '%';
            document.getElementById('progress-text').innerText = data.progress + '%';
            
            const logBox = document.getElementById('logs');
            if (data.logs.length > logBox.childElementCount) {{
                 logBox.innerHTML = data.logs.map(l => `<div class="log-entry">${{l}}</div>`).join('');
                 logBox.scrollTop = logBox.scrollHeight;
            }}
            
            document.getElementById('costs').innerHTML = formatCost(data.costs);
            
            // Dynamic Assets Categories
            const assetsContainer = document.getElementById('dynamic-assets-container');
            // Group assets by type
            const assetsByType = {{}};
            data.assets.forEach(a => {{
                const type = a.type || "Other";
                if (!assetsByType[type]) assetsByType[type] = [];
                assetsByType[type].push(a);
            }});
            
            // If no assets yet, show placeholder in default container if it exists, or just clear
            if (Object.keys(assetsByType).length === 0) {{
                assetsContainer.innerHTML = `
                <div class="card">
                    <button class="help-btn" onclick="showHelp('assets')">?</button>
                    <h3>Generated Assets</h3>
                    <div class="gallery"></div>
                </div>`;
            }} else {{
                // Re-build asset cards if number of categories changed or just update content
                // For simplicity, we rebuild the HTML string and update if different
                let newHtml = '';
                for (const [type, list] of Object.entries(assetsByType)) {{
                    const itemsHtml = list.map(a => `
                        <div class="gallery-item" title="${{a.name}}">
                            <img src="${{sessionPath + a.file}}" loading="lazy">
                            <div class="label">${{a.name}}</div>
                            <div class="overlay">
                                <a href="${{sessionPath + a.file}}" download class="download-icon" title="Download Asset">‚¨áÔ∏è</a>
                            </div>
                        </div>
                    `).join('');
                    
                    newHtml += `
                    <div class="card">
                        <button class="help-btn" onclick="showHelp('assets')">?</button>
                        <h3>${{type}}s</h3>
                        <div class="gallery">${{itemsHtml}}</div>
                    </div>`;
                }}
                if (assetsContainer.innerHTML !== newHtml) assetsContainer.innerHTML = newHtml;
            }}
            
            // Videos with Download
            const vidsHtml = data.videos.map(v => `
                <div class="gallery-item" title="Segment ${{v.index}}">
                    <video src="${{sessionPath + v.file}}" controls preload="none"></video>
                    <div class="label">Seg ${{v.index}}</div>
                    <div class="overlay" style="pointer-events:none;">
                        <!-- Overlay blocks controls, so we position button carefully or just use link below -->
                    </div>
                    <a href="${{sessionPath + v.file}}" download class="download-icon" style="position:absolute; top:5px; right:5px; padding:5px; font-size:1rem;" title="Download Clip">‚¨áÔ∏è</a>
                </div>
            `).join('');
            if (document.getElementById('videos-gallery').innerHTML !== vidsHtml)
                document.getElementById('videos-gallery').innerHTML = vidsHtml;

            // Final Video
            if (data.final_video) {{
                const vidBox = document.getElementById('final-video-box');
                if (vidBox.querySelector('.loading-placeholder')) {{
                    vidBox.innerHTML = `<video src="${{sessionPath + data.final_video}}" controls autoplay loop></video>`;
                }}
            }}

            // EDL Button
            if (data.edl_file) {{
                const btn = document.getElementById('btn-dl-edl');
                btn.href = sessionPath + data.edl_file;
                btn.classList.remove('disabled');
            }}

            renderTimeline(data.montage, data.assets);
            renderStyle(data.style);
            renderPalette(data.palette);
            renderGenericText(data.analysis, 'analysis-display');
            renderGenericText(data.prompts, 'prompts-display');
            renderGenericText(data.motion, 'motion-display');
        }}

        function poll() {{
            const script = document.createElement('script');
            script.src = jsPath + '?t=' + Date.now();
            document.body.appendChild(script);
            script.onload = () => {{
                document.body.removeChild(script);
            }};
            script.onerror = () => {{
                console.log("Polling error (file might not exist yet)");
                document.body.removeChild(script);
            }};
        }}
        
        setInterval(poll, 2000);
        poll();
    </script>
</body>
</html>
        """
        
        try:
            with open(report_path, "w", encoding="utf-8") as f: f.write(html_content)
            print(f"[SceneGen] Created report at: {report_path}")
            
            # Initial write of JSON data
            update_report(status="Starting Process...", progress=0)
            
            if open_report:
                print(f"[SceneGen] Opening report in browser...")
                webbrowser.open(f"file://{os.path.abspath(report_path)}", new=2)
        except Exception as e:
            print(f"[SceneGen] Failed to create/open report: {e}")

        def track_text(input_str, output_str):
            # Rough estimate: 1 token ~= 4 chars
            usage_stats["gemini_text_input_tokens"] += len(str(input_str)) // 4
            usage_stats["gemini_text_output_tokens"] += len(str(output_str)) // 4
            # No need to update report on every token count, just at key stages

        # --- Setup ---
        model_text = genai.GenerativeModel(model_text_name)
        model_image_gen = genai.GenerativeModel(model_image_name)
        
        # Save original audio
        original_audio_path = os.path.join(session_dir, "original_audio.wav")
        scipy.io.wavfile.write(original_audio_path, sample_rate, audio_data)
        total_duration_sec = len(audio_data) / sample_rate
        
        # Resolution Setup
        ar_map = {"16:9": (1280, 720), "1:1": (1024, 1024), "9:16": (720, 1280), "4:3": (1024, 768)}
        base_w, base_h = ar_map.get(aspect_ratio, (1280, 720))
        img_w = int(base_w * resolution_multiplier)
        img_h = int(base_h * resolution_multiplier)
        print(f"[SceneGen] Resolution set to {img_w}x{img_h} ({aspect_ratio})")
        
        # Temp audio for Gemini
        with open(original_audio_path, "rb") as f:
            audio_bytes = f.read()

        available_models = []
        if use_wan_fast: available_models.append("wan-video/wan-2.5-i2v-fast")
        if use_wan_2_5: available_models.append("wan-video/wan-2.5-i2v")
        if use_kling_turbo: available_models.append("kwaivgi/kling-v2.5-turbo-pro")
        if use_omni_human: available_models.append("bytedance/omni-human-1.5")
        if use_hailuo: available_models.append("minimax/hailuo-2.3")
        if use_hailuo_fast: available_models.append("minimax/hailuo-2.3-fast")
        if use_veo_3_1: available_models.append("google/veo-3.1")
        if use_veo_3_1_fast: available_models.append("google/veo-3.1-fast")
        
        if not available_models:
            print("[SceneGen] No video models selected. Forcing Prompt Mode (Slideshow only).")
            render_mode = "Prompt Mode"
            # We add a dummy model just so the prompt logic doesn't break, 
            # but we won't use it because render_mode check skips Replicate.
            available_models.append("Slideshow") 

        # --- STAGE 1: Audio Analysis ---
        update_report("Stage 1: Analyzing Audio...", 5, "Sending audio to Gemini for analysis...")
        print("[SceneGen] Stage 1: Audio Analysis...")
        prompt_s1 = f"""
        Analyze this audio file (Lyrics, Rhythm, Mood, Genre). 
        Instruction: "{instruction}"
        Return a JSON object with:
        - "genre": Music genre.
        - "characteristics": Key musical characteristics.
        - "dramaturgy": Key moments of drama/dynamics changes (time and description).
        - "lyrics": Full lyrics with word-alignment (if applicable).
        - "structure": Proposed structure for the video (Intro, Verse, Chorus, etc.) with timestamps.
        """
        # Note: We can't easily track input tokens for audio files without API metadata, 
        # but we can track the text prompt part.
        track_text(prompt_s1, "") 
        
        resp_s1 = await model_text.generate_content_async([prompt_s1, {"mime_type": "audio/wav", "data": audio_bytes}])
        debug_s1 = resp_s1.text
        track_text("", debug_s1)
        track_text("", debug_s1)
        with open(os.path.join(session_dir, "stage1_analysis.txt"), "w", encoding="utf-8") as f: f.write(debug_s1)
        report_state["analysis"] = debug_s1
        update_report("Stage 2: Defining Style...", 10, "Audio analysis complete.")
        
        # --- STAGE 2: Style Definition ---
        print("[SceneGen] Stage 2: Style Definition...")
        prompt_s2 = f"""
        Based on the audio analysis: {debug_s1}
        And user instruction: "{instruction}"
        
        Define a precise, universal visual style instruction for the video.
        Return a JSON object with:
        - "style_instruction": A comprehensive style description (e.g., "Cyberpunk noir with neon lighting and grainy film texture").
        """
        track_text(prompt_s2, "")
        resp_s2 = await model_text.generate_content_async(prompt_s2)
        debug_s2 = resp_s2.text
        track_text("", debug_s2)
        with open(os.path.join(session_dir, "stage2_style.json"), "w", encoding="utf-8") as f: f.write(debug_s2)
        style_instruction = json.loads(self._clean_json(debug_s2)).get("style_instruction", "")
        report_state["style"] = {"style_instruction": style_instruction}
        update_report(None, None, "Style defined.")

        # --- STAGE 3: Color Palette ---
        print("[SceneGen] Stage 3: Color Palette...")
        prompt_s3 = f"""
        Based on the style: "{style_instruction}"
        And audio analysis: {debug_s1}
        
        Create a color palette.
        Return a JSON object with:
        - "palette": List of colors (HEX codes or descriptive names like "Electric Blue").
        - "lighting_mood": Description of the lighting atmosphere.
        """
        track_text(prompt_s3, "")
        resp_s3 = await model_text.generate_content_async(prompt_s3)
        debug_s3 = resp_s3.text
        track_text("", debug_s3)
        with open(os.path.join(session_dir, "stage3_palette.json"), "w", encoding="utf-8") as f: f.write(debug_s3)
        palette_data = json.loads(self._clean_json(debug_s3))
        report_state["palette"] = palette_data
        update_report(None, None, "Palette generated.")

        # --- STAGE 3.5: Reference Analysis ---
        print("[SceneGen] Stage 3.5: Reference Analysis...")
        ref_data = []
        asset_library = {} # Name -> Image
        
        # Output lists
        env_imgs = []
        prop_imgs = []
        actor_imgs = []

        if ref_images_pil:
            prompt_s3_5 = f"""
            Analyze the attached {len(ref_images_pil)} images in context of: "{instruction}" and Style: "{style_instruction}".
            For each image, identify its category (Environment, Prop, or Actor) and provide a descriptive name (unique_id) and visual description.
            Return JSON object with "references": list of objects:
            - "index": int (0-based index of the image provided)
            - "category": string ("Environment", "Prop", "Actor")
            - "description": string
            - "name": string (unique identifier)
            """
            try:
                req_content = [prompt_s3_5] + ref_images_pil
                track_text(prompt_s3_5, "")
                resp_s3_5 = await model_text.generate_content_async(req_content)
                track_text("", resp_s3_5.text)
                ref_data = json.loads(self._clean_json(resp_s3_5.text)).get("references", [])
                with open(os.path.join(session_dir, "stage3_5_references.json"), "w", encoding="utf-8") as f: json.dump(ref_data, f, indent=2)
                
                for r in ref_data:
                    idx = r.get("index")
                    if idx is not None and 0 <= idx < len(ref_images_pil):
                        img = ref_images_pil[idx]
                        cat = r.get("category", "").lower()
                        name = r.get("name", f"ref_{idx}")
                        asset_library[name] = img
                        if "env" in cat: env_imgs.append(img)
                        elif "prop" in cat: prop_imgs.append(img)
                        elif "actor" in cat: actor_imgs.append(img)
                        else: actor_imgs.append(img)
            except Exception as e:
                print(f"Stage 3.5 Error: {e}")

        # --- STAGE 4: Asset Definition & Generation ---
        update_report("Stage 4: Generating Assets...", 20, "Designing assets based on style...")
        print("[SceneGen] Stage 4: Asset Definition & Generation (Dependency-Aware)...")
        prompt_s4 = f"""
        Based on the style: "{style_instruction}"
        And existing Reference Assets: {json.dumps(ref_data)}
        
        Define the visual assets needed.
        GENERATE A LARGE NUMBER OF ASSETS (Props, Environments).
        Create MULTIPLE VARIANTS of environments to ensure coverage (e.g., "Kitchen_Wide", "Kitchen_CloseUp", "Kitchen_LowAngle").
        - Use "parent_asset" to link the variants to the main shot (e.g., "Kitchen_CloseUp" parent is "Kitchen_Wide").
        
        Return a JSON object with a list "new_assets":
        - "name": Unique ID (e.g. "Hero_Normal").
        - "category": "Environment", "Prop", or "Actor".
        - "description": Visual description.
        - "parent_asset": Name of the asset this is based on (from References or other New Assets). Null if none.
        
        IMPORTANT GUIDELINES:
        - Actors: Describe as a "Character Sheet" (full body, neutral background, consistent features, multiple views).
        - Props: Describe as an "Object Sheet" (neutral background, isolated, multiple angles).
        - Environments: Describe as "Wide Shot" (empty, no people, atmospheric, interior or exterior).
        """
        track_text(prompt_s4, "")
        resp_s4 = await model_text.generate_content_async(prompt_s4)
        debug_s4 = resp_s4.text
        track_text("", debug_s4)
        with open(os.path.join(session_dir, "stage4_assets.json"), "w", encoding="utf-8") as f: f.write(debug_s4)
        
        try:
            new_assets_list = json.loads(self._clean_json(debug_s4)).get("new_assets", [])
        except json.JSONDecodeError as e:
            print(f"[SceneGen] JSON Error in Stage 4: {e}")
            print(f"[SceneGen] Raw output: {debug_s4[:200]}...")
            # Fallback: Try to repair or just proceed with empty assets
            new_assets_list = []
            update_report("Stage 4 Warning", 40, "Failed to parse asset list. Proceeding without new assets.")
        
        # Helper to generate assets
        async def gen_asset_task(asset_data):
            name = asset_data["name"]
            cat = asset_data["category"].lower()
            desc = asset_data["description"]
            parent_name = asset_data.get("parent_asset")
            
            suffix = ""
            if "actor" in cat: suffix = "character sheet, full body, front view and side view, neutral background, isolated, high detail, concept art"
            elif "prop" in cat: suffix = "object sheet, isolated on white background, highly detailed, 3d render"
            elif "env" in cat: suffix = "wide shot, empty scene, no people, architectural photography, detailed environment"

            full_prompt = f"{style_instruction}. {desc}. {suffix}. {palette_data.get('lighting_mood', '')} --aspect {aspect_ratio}"
            
            input_parts = [full_prompt]
            if parent_name and parent_name in asset_library:
                input_parts.append(asset_library[parent_name])
                print(f"  -> Generating {name} using parent {parent_name}")
            
            if render_mode == "Prompt Mode":
                # Skip actual image generation
                print(f"  -> [Prompt Mode] Skipping generation for asset: {name}")
                return (name, Image.new('RGB', (img_w, img_h)), cat)

            try:
                r = await model_image_gen.generate_content_async(input_parts)
                if hasattr(r, 'parts'):
                    for p in r.parts:
                        if hasattr(p, 'inline_data'):
                            img = Image.open(io.BytesIO(p.inline_data.data))
                            img = img.resize((img_w, img_h)) # Ensure correct resolution
                            if save_assets:
                                fname = f"asset_{name}.png"
                                img.save(os.path.join(session_dir, fname))
                                usage_stats["generated_assets"].append({"name": name, "type": cat, "file": fname})
                                report_state["assets"].append({"name": name, "type": cat, "file": fname}) # Add to report state
                            usage_stats["gemini_images_generated"] += 1 # Track image generation
                            return (name, img, cat)
            except Exception as e:
                print(f"Asset Gen Error ({name}): {e}")
            return (name, Image.new('RGB', (img_w, img_h)), cat)

        # Dependency Loop
        pending = {a["name"]: a for a in new_assets_list}
        completed = set(asset_library.keys())
        
        while pending:
            ready = []
            for name, data in pending.items():
                p = data.get("parent_asset")
                if not p or p in completed:
                    ready.append(name)
            
            if not ready:
                print("Stage 4: Cyclic dependency or missing parent. Forcing remaining...")
                ready = list(pending.keys()) # Force run remaining to avoid infinite loop
            
            print(f"Generating batch: {ready}")
            # Run batch
            batch_results = await asyncio.gather(*[gen_asset_task(pending[n]) for n in ready])
            
            for name, img, cat in batch_results:
                asset_library[name] = img
                completed.add(name)
                del pending[name]
                
                if "env" in cat: env_imgs.append(img)
                elif "prop" in cat: prop_imgs.append(img)
                else: actor_imgs.append(img)
            
            # Update report after batch
            update_report(None, None, f"Generated batch of {len(ready)} assets.")
            
        available_assets_summary = ref_data + new_assets_list

        # --- STAGE 5: Montage Line ---
        update_report("Stage 5: Creating Montage...", 35, "Structuring video timeline...")
        print("[SceneGen] Stage 5: Montage Line...")
        prompt_s5 = f"""
        Create a video montage JSON.
        Context:
        - Audio Duration: {total_duration_sec}s
        - Analysis: {debug_s1}
        - Style: {style_instruction}
        - Assets Available: {json.dumps(available_assets_summary)}
        - Available Models: {json.dumps(available_models)}
        - Dynamicity: {dynamicity} (0-1)
        - Aggressive Edit: {aggressive_edit} (Boolean)
        
        Task:
        Create a list of scenes that strictly sums to {total_duration_sec}s.
        For each scene, choose a model and duration.
        - Wan/Kling: 5s or 10s.
        - Hailuo: 6s or 10s (Note: 10s is 768p only).
        - Veo: 4s, 6s, 8s.
        - OmniHuman: Any.
        
        If Aggressive Edit is True:
        - Create a fast-paced montage with frequent cuts (short target durations).
        - You MUST still respect the model generation minimums (e.g. 5s), but we will trim them later.
        - Specify the "trim_duration" in the JSON to indicate the actual length used in the edit (e.g. 2.0s).
        - Ensure cuts are strictly synchronized to the beat/dramaturgy.
        
        Return JSON object with "scenes": list of objects:
        - "duration": float (Generation duration, e.g. 5.0)
        - "trim_duration": float (Actual edit duration, e.g. 2.5. If not aggressive, same as duration)
        - "model": string
        - "clue": simple description of the shot content.
        - "model": string
        - "clue": simple description of the shot content.
        - "asset_refs": list of strings (names of assets from Assets Available to use). IMPORTANT: Use the EXACT "name" string from the Assets Available list (e.g., "Gen_Env_0", "Gen_Actor_1"). Do not invent new names.
        """
        track_text(prompt_s5, "")
        resp_s5 = await model_text.generate_content_async(prompt_s5)
        debug_s5 = resp_s5.text
        track_text("", debug_s5)
        with open(os.path.join(session_dir, "stage5_montage.json"), "w", encoding="utf-8") as f: f.write(debug_s5)
        montage_data = json.loads(self._clean_json(debug_s5)).get("scenes", [])

        # --- STAGE 6: Prompt Engineering (Batched) ---
        update_report("Stage 6: Writing Prompts...", 45, "Expanding scene details...")
        print("[SceneGen] Stage 6: Prompt Engineering (Batched)...")
        detailed_scenes = []
        batch_size = 10
        
        # Prepare batches
        total_batches = (len(montage_data) + batch_size - 1) // batch_size
        
        for b_idx in range(total_batches):
            start_i = b_idx * batch_size
            end_i = start_i + batch_size
            batch_scenes = montage_data[start_i:end_i]
            
            print(f"  -> Processing Batch {b_idx + 1}/{total_batches} (Scenes {start_i} to {min(end_i, len(montage_data))})...")
            
            prompt_s6 = f"""
            Expand the montage into detailed production prompts.
            Batch {b_idx + 1} of {total_batches}.
            
            Style: {style_instruction}
            Palette: {json.dumps(palette_data)}
            Word Influence: {word_influence} (-1.0 to 1.0)
            
            Scenes to Process: {json.dumps(batch_scenes)}
            
            Instruction for Word Influence:
            - If > 0.5: Prioritize LITERAL visualization of lyrics/text. Show exactly what is described.
            - If < -0.5: Prioritize VIBE/ATMOSPHERE. Ignore literal lyrics, focus on mood and abstract representation.
            - If ~0.0: Balance both.
            
            For EACH scene in this batch, generate:
            - "positive_prompt": A MASSIVE, DETAILED paragraph (50-100 words) describing the scene. 
              CRITICAL: You MUST explicitly describe the background/location using the specific Environment asset details to ensure continuity.
              Focus heavily on COMPOSITION, CAMERA ANGLE, LIGHTING, TEXTURE, and ATMOSPHERE. Make it visually rich and strictly adhering to the defined Style.
            - "video_trigger_prompt": Motion instruction for video generation.
            - "negative_prompt": Standard negative prompt.
            
            Return JSON object with "scenes": list of objects (same order as input).
            """
            
            try:
                track_text(prompt_s6, "")
                resp_s6 = await model_text.generate_content_async(prompt_s6)
                track_text("", resp_s6.text)
                batch_result = json.loads(self._clean_json(resp_s6.text)).get("scenes", [])
                
                # Merge details back into the original scene data for this batch
                for i, scene_detail in enumerate(batch_result):
                    if i < len(batch_scenes):
                        merged = {**batch_scenes[i], **scene_detail}
                        detailed_scenes.append(merged)
            except Exception as e:
                print(f"  Batch {b_idx + 1} Error: {e}")
                # On error, keep original batch scenes without updates
                detailed_scenes.extend(batch_scenes)
        
        final_scenes = detailed_scenes
        debug_s6 = json.dumps(final_scenes, indent=2)
        with open(os.path.join(session_dir, "stage6_prompts.json"), "w", encoding="utf-8") as f: f.write(debug_s6)
        
        # Update report state with prompts and montage
        report_state["prompts"] = debug_s6
        report_state["montage"] = final_scenes
        update_report("Stage 7: Generating Start Frames...", 50, "Scene prompts complete.")

        # --- STAGE 7: Image Generation (Start Frames) ---
        update_report("Stage 7: Generating Start Frames...", 55, f"Generating {len(final_scenes)} start frames...")
        print(f"[SceneGen] Stage 7: Generating Start Frames ({len(final_scenes)} scenes)...")
        
        async def gen_scene_image(idx, scene_data):
            prompt = f"{scene_data.get('positive_prompt')} --aspect {aspect_ratio}"
            
            input_parts = [prompt]
            
            # Smart Asset Selection
            refs = scene_data.get("asset_refs", [])
            ref_img_found = False
            attached_asset_names = []
            
            if refs:
                for r_name in refs:
                    r_name_clean = r_name.strip()
                    found_key = None
                    
                    # 1. Exact Match
                    if r_name_clean in asset_library:
                        found_key = r_name_clean
                    
                    # 2. Partial Match (if LLM added extra text like "Gen_Env_0 (Dark)")
                    if not found_key:
                        for key in asset_library.keys():
                            if key == r_name_clean.split(" ")[0]: # Check if first word matches key
                                found_key = key
                                break
        scene_images = []
        if render_mode == "Full Render":
            update_report("Stage 7: Generating Start Frames...", 55, f"Generating {len(final_scenes)} start frames...")
            print(f"[SceneGen] Stage 7: Generating Start Frames ({len(final_scenes)} scenes)...")
            
            async def gen_scene_image(idx, scene_data):
                prompt = f"{scene_data.get('positive_prompt')} --aspect {aspect_ratio}"
                
                input_parts = [prompt]
                
                # Smart Asset Selection
                refs = scene_data.get("asset_refs", [])
                ref_img_found = False
                attached_asset_names = []
                
                if refs:
                    for r_name in refs:
                        r_name_clean = r_name.strip()
                        found_key = None
                        
                        # 1. Exact Match
                        if r_name_clean in asset_library:
                            found_key = r_name_clean
                        
                        # 2. Partial Match (if LLM added extra text like "Gen_Env_0 (Dark)")
                        if not found_key:
                            for key in asset_library.keys():
                                if key == r_name_clean.split(" ")[0]: # Check if first word matches key
                                    found_key = key
                                    break
                        
                        if found_key:
                            input_parts.append(asset_library[found_key])
                            attached_asset_names.append(found_key)
                            ref_img_found = True
                            # We attach the first valid asset found to guide the scene. 
                            # Attaching too many might confuse the composition.
                            break 
                
                # Fallback
                if not ref_img_found:
                    if actor_imgs: 
                        input_parts.append(actor_imgs[0])
                        attached_asset_names.append("Fallback_Actor_0")
                    elif env_imgs: 
                        input_parts.append(env_imgs[0])
                        attached_asset_names.append("Fallback_Env_0")
                
                print(f"[SceneGen] Scene {idx}: Generating with assets {attached_asset_names}")
                
                try:
                    r = await model_image_gen.generate_content_async(input_parts)
                    if hasattr(r, 'parts'):
                        for p in r.parts:
                            if hasattr(p, 'inline_data'):
                                img = Image.open(io.BytesIO(p.inline_data.data))
                                img = img.resize((img_w, img_h))
                                if save_images:
                                    fname = f"{prefix}_scene_{idx:03d}.png"
                                    img.save(os.path.join(session_dir, fname))
                                    usage_stats["generated_assets"].append({"name": f"Scene {idx}", "type": "Start Frame", "file": fname})
                                    report_state["assets"].append({"name": f"Scene {idx}", "type": "Start Frame", "file": fname})
                                usage_stats["gemini_images_generated"] += 1 # Track image generation
                                return img
                except Exception as e:
                    print(f"Scene Image Error {idx}: {e}")
                return Image.new('RGB', (img_w, img_h))

            scene_images = await asyncio.gather(*[gen_scene_image(i, s) for i, s in enumerate(final_scenes)])
            update_report(None, 65, "Start frames generated.")
        else:
            update_report("Stage 7: Skipped (Prompt Mode - Text Only)", 65, "Skipping start frame generation.")
            print("[SceneGen] Stage 7: Skipped (Prompt Mode - Text Only)")
            # Provide placeholder images for subsequent stages that might expect them
            scene_images = [Image.new('RGB', (img_w, img_h))] * len(final_scenes)

        debug_s7 = f"Generated {len(scene_images)} start frames."
        with open(os.path.join(session_dir, "stage7_generation_status.txt"), "w", encoding="utf-8") as f: f.write(debug_s7)

        # --- STAGE 8: Vision-Aware Video Prompt Refinement ---
        update_report("Stage 8: Refining Motion...", 70, "Analyzing frames for motion prompts...")
        print(f"[SceneGen] Stage 8: Refining Video Prompts with Vision ({len(final_scenes)} scenes)...")
        
        current_time_s8 = 0.0
        refine_tasks = []
        
        async def refine_scene_prompt(idx, scene, img, start_time):
            model_name = scene.get("model", "generic")
            
            prompt_s8 = f"""
            Role: Expert Video Prompt Engineer for {model_name}.
            
            Input:
            1. Start Frame Image (Attached).
            2. Audio Analysis/Lyrics: {debug_s1}
            3. Current Scene Time: {start_time:.1f}s
            4. Context/Clue: "{scene.get('clue', '')}"
            
            Task:
            Write a highly specific "Motion Prompt" (video_trigger_prompt) for this exact shot.
            
            CRITICAL RULES:
            1. NEVER allow a static shot. Always describe motion.
            2. If the scene is calm, use terms like "subtle natural movement", "micro-movements", "delicate living environment", "slow camera drift", "wind blowing hair/cloth", "breathing".
            3. If energetic, describe the action vividly and strictly synced to the music/lyrics at {start_time:.1f}s.
            4. Use the specific prompting style best for {model_name} (e.g. for Wan/Veo use "Camera pan...", for Kling use "The character moves...").
            
            Return JSON: {{ "video_trigger_prompt": "..." }}
            """
            
            try:
                # Gemini supports image input directly
                track_text(prompt_s8, "")
                resp = await model_text.generate_content_async([prompt_s8, img])
                track_text("", resp.text)
                res_json = json.loads(self._clean_json(resp.text))
                return (idx, res_json.get("video_trigger_prompt", ""))
            except Exception as e:
                print(f"Stage 8 Error (Scene {idx}): {e}")
                return (idx, "")

        for i, scene in enumerate(final_scenes):
            dur = float(scene.get("duration", 5))
            # Ensure we have an image
            img = scene_images[i]
            refine_tasks.append(refine_scene_prompt(i, scene, img, current_time_s8))
            current_time_s8 += dur
            
        # Run with concurrency limit (chunking)
        refined_results = []
        chunk_size = gemini_concurrency
        for i in range(0, len(refine_tasks), chunk_size):
            chunk = refine_tasks[i:i+chunk_size]
            print(f"  -> Processing Vision Batch {i//chunk_size + 1}...")
            refined_results.extend(await asyncio.gather(*chunk))
            
        # Apply updates
        for idx, new_prompt in refined_results:
            if new_prompt:
                final_scenes[idx]["video_trigger_prompt"] = new_prompt
                
        debug_s8 = json.dumps([{"scene": i, "motion": p} for i, p in refined_results], indent=2)
        with open(os.path.join(session_dir, "stage8_prompts_status.txt"), "w", encoding="utf-8") as f: f.write(debug_s8)
        report_state["motion"] = debug_s8
        # Update montage with new prompts
        report_state["montage"] = final_scenes
        update_report("Stage 9: Audio Slicing...", 75, "Motion refinement complete.")

        # --- STAGE 9: Duration Verification & Audio Slicing ---
        print("[SceneGen] Stage 9: Duration Verification...")
        current_sample = 0
        replicate_tasks = []
        
        for i, scene in enumerate(final_scenes):
            target_dur = float(scene.get("duration", 5))
            trim_dur = float(scene.get("trim_duration", target_dur))
            if not aggressive_edit: trim_dur = target_dur # Safety fallback
            
            model = scene.get("model", available_models[0])
            
            # Snap logic
            gen_dur = target_dur
            if "wan-video" in model or "kling" in model:
                gen_dur = 5 if target_dur <= 5 else 10
            elif "hailuo" in model:
                gen_dur = 6 if target_dur <= 6 else 10
            elif "veo" in model:
                if target_dur <= 4: gen_dur = 4
                elif target_dur <= 6: gen_dur = 6
                else: gen_dur = 8
            
            if target_dur > gen_dur: target_dur = gen_dur
            
            # Audio Slice
            start_sec = current_sample / sample_rate
            gen_end_sec = start_sec + gen_dur
            start_samp = int(start_sec * sample_rate)
            gen_end_samp = int(gen_end_sec * sample_rate)
            
            if start_samp >= len(audio_data): break
            
            raw_slice = audio_data[start_samp:gen_end_samp]
            if len(raw_slice) < (gen_end_samp - start_samp):
                pad = (gen_end_samp - start_samp) - len(raw_slice)
                if pad > 0:
                    silence = np.zeros((pad,) + raw_slice.shape[1:], dtype=raw_slice.dtype)
                    raw_slice = np.concatenate((raw_slice, silence), axis=0)
            
            replicate_tasks.append({
                "index": i,
                "model": model,
                "prompt": f"{scene.get('video_trigger_prompt')} {scene.get('positive_prompt')}",
                "positive_prompt": scene.get('positive_prompt'),
                "video_trigger_prompt": scene.get('video_trigger_prompt'),
                "negative_prompt": scene.get("negative_prompt", ""),
                "duration": gen_dur,
                "trim_duration": trim_dur,
                "audio_slice": raw_slice,
                "image": scene_images[i]
            })
            
            current_sample += int(trim_dur * sample_rate)
            
        # Save Stage 9 details
        tasks_summary = []
        for t in replicate_tasks:
            tasks_summary.append({
                "index": t["index"],
                "model": t["model"],
                "prompt": t["prompt"],
                "positive_prompt": t["positive_prompt"],
                "video_trigger_prompt": t["video_trigger_prompt"],
                "negative_prompt": t["negative_prompt"],
                "duration": t["duration"],
                "trim_duration": t["trim_duration"]
            })
            
            # Track Replicate Usage
            m = t["model"]
            if m not in usage_stats["replicate_seconds"]: usage_stats["replicate_seconds"][m] = 0
            usage_stats["replicate_seconds"][m] += t["duration"]
        
        debug_s9 = json.dumps(tasks_summary, indent=2)
        with open(os.path.join(session_dir, "stage9_tasks.json"), "w", encoding="utf-8") as f: f.write(debug_s9)

        # --- STAGE 10: Replicate Execution ---
        update_report("Stage 10: Rendering Video...", 75, f"Starting render of {len(replicate_tasks)} clips...")
        print(f"[SceneGen] Stage 10: Replicate Execution ({len(replicate_tasks)} tasks)...")
        
        video_paths = [None] * len(replicate_tasks)
        
        if render_mode == "Full Render":
            def run_replicate(task):
                idx = task["index"]
                print(f"Task {idx} ({task['model']}) starting...")
                
                # Temp files
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tf_img:
                    task["image"].save(tf_img, format="PNG")
                    img_path = tf_img.name
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tf_aud:
                    scipy.io.wavfile.write(tf_aud.name, sample_rate, task["audio_slice"])
                    aud_path = tf_aud.name
                    
                try:
                    video_url = None
                    max_retries = 2
                    current_prompt = task["prompt"]
                    
                    for attempt in range(max_retries + 1):
                        try:
                            with open(img_path, "rb") as f_img, open(aud_path, "rb") as f_aud:
                                input_data = {}
                                # Map inputs based on model
                                if "wan-video" in task["model"]:
                                    res = "1080p" if video_quality == "High" else "720p"
                                    if video_quality == "Low": res = "480p"
                                    input_data = {"image": f_img, "audio": f_aud, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "resolution": res, "enable_prompt_expansion": enable_prompt_expansion}
                                elif "kling" in task["model"]:
                                    input_data = {"start_image": f_img, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "aspect_ratio": aspect_ratio, "guidance_scale": 0.5}
                                elif "omni-human" in task["model"]:
                                    input_data = {"image": f_img, "audio": f_aud, "prompt": current_prompt, "fast_mode": (video_quality != "High")}
                                elif "hailuo" in task["model"]:
                                    res = "1080p" if video_quality == "High" else "768p"
                                    if task["duration"] == 10: res = "768p" # Enforce resolution constraint
                                    input_data = {"first_frame_image": f_img, "prompt": current_prompt, "duration": task["duration"], "resolution": res, "prompt_optimizer": True}
                                elif "veo" in task["model"]:
                                    res = "1080p" if video_quality == "High" else "720p"
                                    input_data = {"image": f_img, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "resolution": res, "aspect_ratio": aspect_ratio, "generate_audio": True}

                                output = replicate.run(task["model"], input=input_data)
                                video_url = str(output)
                                break
                        except Exception as e:
                            if ("sensitive" in str(e).lower() or "E005" in str(e)) and attempt < max_retries:
                                print(f"Task {idx} Sensitive Content. Sanitizing...")
                                current_prompt = "A beautiful cinematic scene, safe for work, artistic style." 
                            else:
                                raise e

                    # Download
                    if save_segments: 
                        fname = f"{prefix}_{idx:03d}.mp4"
                        local_path = os.path.join(session_dir, fname)
                    else: 
                        fname = f"seg_{idx:03d}.mp4"
                        local_path = os.path.join(session_dir, fname)
                    
                    with requests.get(video_url, stream=True) as r:
                        r.raise_for_status()
                        with open(local_path, 'wb') as f:
                            for chunk in r.iter_content(chunk_size=8192): f.write(chunk)
                    
                    # Track for report
                    return (idx, local_path, fname)
                    
                except Exception as e:
                    print(f"Task {idx} Failed: {e}")
                    return (idx, None, None)
                finally:
                    try: os.unlink(img_path); os.unlink(aud_path)
                    except: pass

            loop = asyncio.get_running_loop()
            rep_futures = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=replicate_concurrency) as rep_exec:
                for task in replicate_tasks:
                    rep_futures.append(loop.run_in_executor(rep_exec, run_replicate, task))
                results = await asyncio.gather(*rep_futures)
                
            for idx, path, fname in results:
                if path: 
                    video_paths[idx] = path
                    usage_stats["generated_segments"].append({"index": idx, "file": fname})
                    report_state["videos"].append({"index": idx, "file": fname})
            
            update_report(None, 90, f"Render complete. {sum(1 for v in video_paths if v)} clips generated.")
            debug_s10 = f"Generated {sum(1 for v in video_paths if v)} videos."
        else:
            print("Prompt Mode: Skipping Replicate generation.")
            update_report(None, 90, "Prompt Mode: Skipped Replicate.")
            debug_s10 = "Skipped (Prompt Mode)"

        with open(os.path.join(session_dir, "stage10_paths.json"), "w", encoding="utf-8") as f: json.dump(video_paths, f, indent=2)

        # --- STAGE 11: Stitching ---
        update_report("Stage 11: Stitching...", 95, "Combining clips into final video...")
        print("[SceneGen] Stage 11: Stitching...")
        final_video_path = os.path.join(session_dir, f"{prefix}_final.mp4")
        
        if render_mode == "Full Render":
            valid_videos = [(i, v) for i, v in enumerate(video_paths) if v is not None]
            if valid_videos:
                concat_list = os.path.join(session_dir, "concat.txt")
                norm_videos = []
                target_w, target_h = img_w, img_h
                
                for i, v in valid_videos:
                    trim = replicate_tasks[i]["trim_duration"]
                    norm = os.path.join(session_dir, f"norm_{i:03d}.mp4")
                    cmd = [
                        "ffmpeg", "-y", "-i", v, "-t", str(trim),
                        "-vf", f"scale={target_w}:{target_h}:force_original_aspect_ratio=decrease,pad={target_w}:{target_h}:(ow-iw)/2:(oh-ih)/2",
                        "-r", "24", "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-an", norm
                    ]
                    try:
                        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                        norm_videos.append(norm)
                    except: pass
                
                if norm_videos:
                    with open(concat_list, "w") as f:
                        for n in norm_videos: f.write(f"file '{n.replace(os.sep, '/')}'\n")
                    
                    silent_path = os.path.join(session_dir, "silent.mp4")
                    subprocess.run(["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", concat_list, "-c", "copy", silent_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    
                    # Mux with original audio
                    subprocess.run([
                        "ffmpeg", "-y", "-i", silent_path, "-i", original_audio_path,
                        "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0", "-shortest", final_video_path
                    ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    
                    # Cleanup
                    if not save_segments:
                        for n in norm_videos: os.unlink(n)
                        os.unlink(concat_list)
                        os.unlink(silent_path)
                        for _, v in valid_videos: os.unlink(v)
        else:
            # Prompt Mode: Slideshow Stitching
            print("Prompt Mode: Creating Slideshow from Start Frames...")
            concat_list = os.path.join(session_dir, "concat_slideshow.txt")
            slideshow_clips = []
            
            for i, task in enumerate(replicate_tasks):
                img = task["image"]
                trim = task["trim_duration"]
                
                # Save temp image for ffmpeg
                slide_img_path = os.path.join(session_dir, f"slide_src_{i:03d}.png")
                img.save(slide_img_path)
                
                slide_vid_path = os.path.join(session_dir, f"slide_{i:03d}.mp4")
                
                # Create video from image with duration
                # ffmpeg -loop 1 -i img.png -t duration -c:v libx264 ...
                cmd = [
                    "ffmpeg", "-y", "-loop", "1", "-i", slide_img_path, "-t", str(trim),
                    "-c:v", "libx264", "-pix_fmt", "yuv420p", "-r", "24", "-vf", f"scale={img_w}:{img_h}", slide_vid_path
                ]
                try:
                    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    slideshow_clips.append(slide_vid_path)
                except Exception as e:
                    print(f"Slideshow frame {i} failed: {e}")
            
            if slideshow_clips:
                with open(concat_list, "w") as f:
                    for n in slideshow_clips: f.write(f"file '{n.replace(os.sep, '/')}'\n")
                
                silent_path = os.path.join(session_dir, "silent_slideshow.mp4")
                subprocess.run(["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", concat_list, "-c", "copy", silent_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # Mux with original audio
                subprocess.run([
                    "ffmpeg", "-y", "-i", silent_path, "-i", original_audio_path,
                    "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0", "-shortest", final_video_path
                ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # Cleanup
                for n in slideshow_clips: os.unlink(n)
                os.unlink(concat_list)
                os.unlink(silent_path)
                # Cleanup slide images
                for i in range(len(replicate_tasks)):
                    try: os.unlink(os.path.join(session_dir, f"slide_src_{i:03d}.png"))
                    except: pass

        debug_s11 = f"Final video: {final_video_path}"
        with open(os.path.join(session_dir, "stage11_output.txt"), "w", encoding="utf-8") as f: f.write(debug_s11)
        
        report_state["final_video"] = os.path.basename(final_video_path)
        update_report("Complete", 100, "Process finished successfully.")
        cost_json = json.dumps(usage_stats, indent=2)
        
        # EDL Generation
        if save_edl:
            edl_path = os.path.join(session_dir, f"{prefix}.edl")
            with open(edl_path, "w") as edl:
                edl.write(f"TITLE: {prefix}\nFCM: NON-DROP FRAME\n\n")
                # CMX 3600 Format
                # 001  AX       V     C        00:00:00:00 00:00:05:00 00:00:00:00 00:00:05:00
                
                rec_in_sec = 0.0
                valid_videos_for_edl = [(i, v) for i, v in enumerate(video_paths) if v is not None] if render_mode == "Full Render" else [(i, f"slide_{i:03d}.mp4") for i, _ in enumerate(replicate_tasks)]
                
                for i, v_name in valid_videos_for_edl:
                    trim = replicate_tasks[i]["trim_duration"]
                    
                    # Source TC (Assume 0 start for generated clips)
                    src_in = "00:00:00:00"
                    src_out_sec = trim
                    
                    # Record TC
                    rec_out_sec = rec_in_sec + trim
                    
                    def sec_to_tc(s):
                        frames = int(s * 24)
                        hh = frames // 86400
                        rem = frames % 86400
                        mm = rem // 3600
                        rem = rem % 3600
                        ss = rem // 24
                        ff = rem % 24
                        return f"{hh:02d}:{mm:02d}:{ss:02d}:{ff:02d}"

                    src_out = sec_to_tc(src_out_sec)
                    rec_in = sec_to_tc(rec_in_sec)
                    rec_out = sec_to_tc(rec_out_sec)
                    
                    line = f"{i+1:03d}  AX       V     C        {src_in} {src_out} {rec_in} {rec_out}\n"
                    edl.write(line)
                    edl.write(f"* FROM CLIP NAME: {os.path.basename(v_name)}\n\n")
                    
                    rec_in_sec = rec_out_sec
            
            report_state["edl_file"] = os.path.basename(edl_path)
            update_report("Complete", 100, "EDL Generated.")
            
            debug_s11 += f"\nEDL saved to: {edl_path}"

        # Prepare Outputs
        def to_tensor(imgs):
            if not imgs: return torch.zeros((1, 512, 512, 3))
            tensors = []
            base_w, base_h = imgs[0].size
            for img in imgs:
                if img.size != (base_w, base_h):
                    img = img.resize((base_w, base_h))
                i = np.array(img).astype(np.float32) / 255.0
                tensors.append(torch.from_numpy(i)[None,])
            return torch.cat(tensors, dim=0)

        return (
            to_tensor(env_imgs), to_tensor(prop_imgs), to_tensor(actor_imgs), to_tensor(scene_images),
            debug_s1, debug_s2, debug_s3, debug_s4, debug_s5, 
            debug_s6, debug_s7, debug_s8, debug_s9, debug_s10, debug_s11,
            cost_json, final_video_path
        )

    def _clean_json(self, text):
        text = text.strip()
        if text.startswith("```json"): text = text[7:]
        if text.startswith("```"): text = text[3:]
        if text.endswith("```"): text = text[:-3]
        return text.strip()

class SceneGenExtractor:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "timeline_data": ("STRING", {"forceInput": True, "tooltip": "Connect 'Timeline Data (S9)' output from Scene Gen node here."}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("Positive Prompts", "Negative Prompts", "Motion Prompts", "Start Frames", "End Frames", "Duration (Frames)", "Duration (Seconds)")
    OUTPUT_IS_LIST = (True, True, True, True, True, True, True)
    FUNCTION = "extract"
    CATEGORY = "Scene Gen"

    def extract(self, timeline_data):
        try:
            data = json.loads(timeline_data)
        except:
            print("SceneGenExtractor: Invalid JSON input")
            return ([], [], [], [], [], [], [])
        
        pos = []
        neg = []
        motion = []
        start_frames = []
        end_frames = []
        dur_frames = []
        dur_sec = []
        
        current_time = 0.0
        fps = 24.0 # Assumption based on SceneGen default
        
        for item in data:
            pos.append(item.get("positive_prompt", ""))
            neg.append(item.get("negative_prompt", ""))
            motion.append(item.get("video_trigger_prompt", ""))
            
            trim_dur = float(item.get("trim_duration", 5.0))
            
            s_frame = int(current_time * fps)
            e_frame = int((current_time + trim_dur) * fps)
            
            start_frames.append(str(s_frame))
            end_frames.append(str(e_frame))
            dur_frames.append(str(e_frame - s_frame))
            dur_sec.append(f"{trim_dur:.2f}")
            
            current_time += trim_dur
            
        return (pos, neg, motion, start_frames, end_frames, dur_frames, dur_sec)

class SceneGenVideoPlayer:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "video_path": ("STRING", {"forceInput": True, "tooltip": "Absolute path to the video file."}),
            }
        }

    RETURN_TYPES = ()
    FUNCTION = "play_video"
    CATEGORY = "Scene Gen"
    OUTPUT_NODE = True

    def play_video(self, video_path):
        # To display video in ComfyUI, it needs to be in the output directory or accessible via URL.
        output_dir = folder_paths.get_output_directory()
        
        # Check if path is absolute and inside output_dir
        if os.path.isabs(video_path) and video_path.startswith(output_dir):
            rel_path = os.path.relpath(video_path, output_dir)
            subfolder = os.path.dirname(rel_path)
            filename = os.path.basename(rel_path)
        else:
            # Fallback or if it's already a relative path (though we usually pass absolute)
            filename = os.path.basename(video_path)
            subfolder = ""
            
        return {"ui": {"videos": [{"filename": filename, "subfolder": subfolder, "type": "output"}]}}

class StringifyTextInput:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "text": ("STRING", {"multiline": True, "default": "", "dynamicPrompts": False}),
            }
        }

    RETURN_TYPES = ("STRING",)
    FUNCTION = "stringify"
    CATEGORY = "Scene Gen"

    def stringify(self, text):
        return (text,)

class SceneGenCostAnalyzer:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "cost_data": ("STRING", {"forceInput": True, "tooltip": "Connect 'Cost Data (JSON)' from Scene Gen node."}),
                "open_report": ("BOOLEAN", {"default": True, "tooltip": "Automatically open the report in browser after generation."}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("HTML Report Path",)
    FUNCTION = "analyze"
    CATEGORY = "Scene Gen"

    def analyze(self, cost_data, open_report):
        try:
            data = json.loads(cost_data)
        except:
            return ("Invalid JSON Data",)
            
        # Pricing Constants (Estimates)
        PRICE_GEMINI_INPUT = 3.50 / 1000000 # Per token (approx)
        PRICE_GEMINI_OUTPUT = 10.50 / 1000000 # Per token
        PRICE_GEMINI_IMAGE = 0.04 # Per image
        
        # Replicate Pricing (Approx per second)
        PRICE_REPLICATE = {
            "wan-video": 0.10,
            "kling": 0.10, # Per sec approx equivalent
            "hailuo": 0.05,
            "veo": 0.10,
            "omni-human": 0.05
        }
        DEFAULT_REP_PRICE = 0.08
        
        # Calculate Costs
        c_gem_in = data.get("gemini_text_input_tokens", 0) * PRICE_GEMINI_INPUT
        c_gem_out = data.get("gemini_text_output_tokens", 0) * PRICE_GEMINI_OUTPUT
        c_gem_img = data.get("gemini_images_generated", 0) * PRICE_GEMINI_IMAGE
        
        c_rep = 0.0
        rep_details = ""
        for model, seconds in data.get("replicate_seconds", {}).items():
            rate = DEFAULT_REP_PRICE
            for k, v in PRICE_REPLICATE.items():
                if k in model: rate = v; break
            cost = seconds * rate
            c_rep += cost
            rep_details += f"<tr><td>{model}</td><td>{seconds:.1f}s</td><td>${cost:.4f}</td></tr>"
            
        total_cost = c_gem_in + c_gem_out + c_gem_img + c_rep
        
        # Build Asset Gallery HTML
        subfolder = data.get("session_subfolder", "")
        assets_html = ""
        for asset in data.get("generated_assets", []):
            fpath = f"{subfolder}/{asset['file']}"
            assets_html += f"""
            <div class="gallery-item">
                <img src="{fpath}" alt="{asset['name']}" loading="lazy">
                <div class="caption">{asset['name']} ({asset['type']})</div>
            </div>
            """
            
        # Build Video Segments HTML
        segments_html = ""
        for seg in sorted(data.get("generated_segments", []), key=lambda x: x['index']):
            fpath = f"{subfolder}/{seg['file']}"
            segments_html += f"""
            <div class="video-item">
                <video controls preload="none">
                    <source src="{fpath}" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="caption">Segment {seg['index']}</div>
            </div>
            """

        # Generate HTML
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Scene Gen Report</title>
            <style>
                body {{ font-family: 'Segoe UI', sans-serif; background: #1a1a1a; color: #e0e0e0; margin: 0; padding: 20px; }}
                .container {{ max-width: 1000px; margin: 0 auto; background: #252525; border-radius: 15px; overflow: hidden; box-shadow: 0 10px 30px rgba(0,0,0,0.5); }}
                .header {{ 
                    background: url('pablo-4k.jpeg') center/cover no-repeat; 
                    height: 250px; position: relative; 
                }}
                .header::after {{ content: ''; position: absolute; top:0; left:0; right:0; bottom:0; background: linear-gradient(to bottom, rgba(0,0,0,0.2), #252525); }}
                .title {{ position: absolute; bottom: 30px; left: 30px; z-index: 2; }}
                h1 {{ margin: 0; font-size: 3em; text-shadow: 0 2px 10px rgba(0,0,0,0.8); color: #fff; }}
                h2 {{ margin: 5px 0 0; font-size: 1.2em; color: #ccc; font-weight: 300; }}
                .content {{ padding: 40px; }}
                
                /* Tables */
                table {{ width: 100%; border-collapse: collapse; margin-bottom: 20px; background: #2a2a2a; border-radius: 8px; overflow: hidden; }}
                th, td {{ text-align: left; padding: 15px; border-bottom: 1px solid #333; }}
                th {{ background: #333; color: #aaa; font-weight: 600; text-transform: uppercase; font-size: 0.85em; letter-spacing: 1px; }}
                tr:last-child td {{ border-bottom: none; }}
                
                .total {{ font-size: 2.5em; color: #4CAF50; margin: 20px 0; text-align: right; font-weight: bold; }}
                
                /* Collapsible Sections */
                details {{ margin-bottom: 20px; background: #2a2a2a; border-radius: 8px; overflow: hidden; }}
                summary {{ padding: 15px; cursor: pointer; font-weight: bold; background: #333; user-select: none; outline: none; transition: background 0.2s; }}
                summary:hover {{ background: #3a3a3a; }}
                summary::-webkit-details-marker {{ color: #888; }}
                .details-content {{ padding: 20px; }}
                
                /* Grid Layouts */
                .gallery {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); gap: 15px; }}
                .gallery-item {{ background: #333; border-radius: 8px; overflow: hidden; transition: transform 0.2s; }}
                .gallery-item:hover {{ transform: translateY(-5px); }}
                .gallery-item img {{ width: 100%; height: 150px; object-fit: cover; display: block; }}
                
                .video-grid {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); gap: 20px; }}
                .video-item {{ background: #333; border-radius: 8px; overflow: hidden; }}
                .video-item video {{ width: 100%; display: block; }}
                
                .caption {{ padding: 8px; font-size: 0.8em; color: #aaa; text-align: center; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }}
                
                .footer {{ text-align: center; padding: 40px; border-top: 1px solid #333; background: #222; }}
                .btn-coffee {{ display: inline-block; transition: transform 0.2s; filter: drop-shadow(0 4px 6px rgba(0,0,0,0.3)); }}
                .btn-coffee:hover {{ transform: scale(1.05); }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <div class="title">
                        <h1>Generation Report</h1>
                        <h2>Scene Gen by Paul Lazniak</h2>
                    </div>
                </div>
                <div class="content">
                    
                    <div class="total">Total Estimated Cost: ${total_cost:.4f}</div>
                    
                    <details open>
                        <summary>üí∞ Cost Breakdown</summary>
                        <div class="details-content">
                            <table>
                                <tr><th>Resource</th><th>Usage</th><th>Cost</th></tr>
                                <tr><td>Gemini Text Input</td><td>{data.get("gemini_text_input_tokens", 0)} toks</td><td>${c_gem_in:.4f}</td></tr>
                                <tr><td>Gemini Text Output</td><td>{data.get("gemini_text_output_tokens", 0)} toks</td><td>${c_gem_out:.4f}</td></tr>
                                <tr><td>Gemini Images</td><td>{data.get("gemini_images_generated", 0)} imgs</td><td>${c_gem_img:.4f}</td></tr>
                                {rep_details}
                            </table>
                        </div>
                    </details>
                    
                    <details>
                        <summary>üñºÔ∏è Generated Assets ({len(data.get("generated_assets", []))})</summary>
                        <div class="details-content">
                            <div class="gallery">
                                {assets_html}
                            </div>
                        </div>
                    </details>
                    
                    <details>
                        <summary>üé¨ Video Segments ({len(data.get("generated_segments", []))})</summary>
                        <div class="details-content">
                            <div class="video-grid">
                                {segments_html}
                            </div>
                        </div>
                    </details>
                    
                </div>
                <div class="footer">
                    <p style="margin-bottom: 20px; font-size: 1.1em; color: #fff;">Enjoying Scene Gen? Support the development!</p>
                    <a href="https://www.buymeacoffee.com/EYB8tkx3tO" class="btn-coffee" target="_blank">
                        <img src="https://img.buymeacoffee.com/button-api/?text=Support project development&emoji=‚ù§Ô∏è&slug=EYB8tkx3tO&button_colour=FFDD00&font_colour=000000&font_family=Lato&outline_colour=000000&coffee_colour=ffffff" />
                    </a>
                </div>
            </div>
        </body>
        </html>
        """
        
        output_dir = folder_paths.get_output_directory()
        report_filename = f"scenegen_report_{int(time.time())}.html"
        report_path = os.path.join(output_dir, report_filename)
        
        with open(report_path, "w", encoding="utf-8") as f: f.write(html)
        
        if open_report:
            try:
                webbrowser.open(f"file://{os.path.abspath(report_path)}", new=2)
            except:
                pass
        
        return (report_path,)

NODE_CLASS_MAPPINGS = {
    'SceneGenNode': SceneGenNode,
    'SceneGenExtractor': SceneGenExtractor,
    'SceneGenVideoPlayer': SceneGenVideoPlayer,
    'StringifyTextInput': StringifyTextInput
}

NODE_DISPLAY_NAME_MAPPINGS = {
    'SceneGenNode': 'Scene Gen (Gemini)',
    'SceneGenExtractor': 'Scene Gen Data Extractor',
    'SceneGenVideoPlayer': 'Scene Gen Video Player',
    'StringifyTextInput': 'Stringify Text Input'
}

