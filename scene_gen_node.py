import os
import sys
sys.path.append(os.path.dirname(os.path.realpath(__file__)))
import asyncio
import numpy as np
import torch
import google.generativeai as genai
from PIL import Image
import io
import scipy.io.wavfile
import json
import concurrent.futures
import replicate
import tempfile
import requests
import subprocess
import shutil
import random
import time

class SceneGenNode:
    def __init__(self):
        # Locate ComfyUI output directory
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
        self.output_dir = os.path.join(base_dir, "output")
        
        # Ensure directory exists
        if not os.path.exists(self.output_dir):
            try:
                os.makedirs(self.output_dir, exist_ok=True)
            except:
                self.output_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "output")
                os.makedirs(self.output_dir, exist_ok=True)

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "The input audio file (WAV/MP3) to analyze and generate video for."}),
                "gemini_api_key": ("STRING", {"multiline": False, "default": "", "tooltip": "Your Google Gemini API Key. Required for analysis and prompt generation."}),
                "replicate_api_token": ("STRING", {"multiline": False, "default": "", "tooltip": "Your Replicate API Token. Required for video generation models."}),
                "prompt_instruction": ("STRING", {"multiline": True, "default": "Describe a scene matching the music.", "tooltip": "Main instruction for the AI. Describe the desired mood, style, story, or specific visual elements."}),
                "filename_prefix": ("STRING", {"default": "scene_gen", "tooltip": "Prefix for all generated files (video, images, logs)."}),
                "fps": ("FLOAT", {"default": 24.0, "min": 1.0, "max": 120.0, "step": 0.1, "tooltip": "Frame rate of the final output video."}),
                "model_text": ("STRING", {"default": "gemini-3-pro-preview", "tooltip": "Gemini model used for text analysis, scripting, and prompting."}),
                "model_image": ("STRING", {"default": "gemini-3-pro-image-preview", "tooltip": "Gemini model used for generating start frames and assets."}),
                "creativity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "0.0 = Strict adherence to prompt. 1.0 = High hallucination/creative freedom."}),
                "dynamicity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "0.0 = Slow, static, contemplative shots. 1.0 = Fast cuts, high movement, intense action."}),
                "video_quality": (["Low", "Medium", "High"], {"default": "Medium", "tooltip": "Controls resolution and quality settings for video models. High = 1080p (where available), Medium = 720p/768p, Low = 480p."}),
                "aspect_ratio": (["16:9", "1:1", "9:16"], {"default": "16:9", "tooltip": "Aspect ratio of the generated video."}),
                "enable_prompt_expansion": ("BOOLEAN", {"default": True, "tooltip": "If True, the AI will expand your simple instructions into highly detailed visual prompts."}),
                "save_segments": ("BOOLEAN", {"default": False, "tooltip": "If True, saves every individual video clip generated by Replicate to the output folder."}),
                "save_images": ("BOOLEAN", {"default": False, "tooltip": "If True, saves the start frame images generated by Gemini."}),
                "save_assets": ("BOOLEAN", {"default": True, "tooltip": "If True, saves the generated asset images (characters, props, environments)."}),
                "gemini_concurrency": ("INT", {"default": 5, "min": 1, "max": 50, "tooltip": "Max parallel requests to Gemini API."}),
                "replicate_concurrency": ("INT", {"default": 10, "min": 1, "max": 50, "tooltip": "Max parallel video generation jobs on Replicate."}),
                "use_wan_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Wan 2.5 Fast model."}),
                "use_wan_2_5": ("BOOLEAN", {"default": True, "tooltip": "Enable Wan 2.5 Standard model."}),
                "use_kling_turbo": ("BOOLEAN", {"default": True, "tooltip": "Enable Kling v2.5 Turbo model."}),
                "use_omni_human": ("BOOLEAN", {"default": True, "tooltip": "Enable OmniHuman model (good for realistic human movement)."}),
                "use_hailuo": ("BOOLEAN", {"default": True, "tooltip": "Enable Hailuo 2.3 model."}),
                "use_hailuo_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Hailuo 2.3 Fast model."}),
                "use_veo_3_1": ("BOOLEAN", {"default": True, "tooltip": "Enable Google Veo 3.1 model."}),
                "use_veo_3_1_fast": ("BOOLEAN", {"default": True, "tooltip": "Enable Google Veo 3.1 Fast model."}),
                "aggressive_edit": ("BOOLEAN", {"default": False, "tooltip": "If True, forces fast-paced editing with cuts strictly on beat. Generates full clips but trims them aggressively."}),
                "word_influence": ("FLOAT", {"default": 0.0, "min": -1.0, "max": 1.0, "step": 0.1, "tooltip": "1.0 = Literal visualization of lyrics. -1.0 = Ignore lyrics, focus on vibe/atmosphere. 0.0 = Balanced."}),
                "save_edl": ("BOOLEAN", {"default": True, "tooltip": "If True, exports a CMX 3600 .edl file for importing the timeline into Premiere Pro/DaVinci Resolve."}),
            },
            "optional": {
                "reference_images": ("IMAGE", {"tooltip": "Optional images to use as references for style, characters, or environments."}),
            }
        }

    RETURN_TYPES = (
        "IMAGE", "IMAGE", "IMAGE", "IMAGE", # Environments, Assets, Actors, Scene Images
        "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING", # Debug Stages 1-11
        "STRING" # Final Video Path
    )
    RETURN_NAMES = (
        "environment_images", "asset_images", "actor_images", "scene_images",
        "debug_stage_1", "debug_stage_2", "debug_stage_3", "debug_stage_4", "debug_stage_5", 
        "debug_stage_6", "debug_stage_7", "debug_stage_8", "debug_stage_9", "debug_stage_10", "debug_stage_11",
        "final_video_path"
    )
    FUNCTION = "process"
    CATEGORY = "Scene Gen"

    def process(self, audio, gemini_api_key, replicate_api_token, prompt_instruction, filename_prefix, fps, model_text, model_image, creativity, dynamicity, video_quality, aspect_ratio, enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency, use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, aggressive_edit, word_influence, save_edl, reference_images=None):
        print(f"\n[SceneGen] === Starting Iterative Process ===")
        
        if not gemini_api_key: raise ValueError("Gemini API Key is required.")
        if not replicate_api_token: raise ValueError("Replicate API Token is required.")
        
        os.environ["REPLICATE_API_TOKEN"] = replicate_api_token
        genai.configure(api_key=gemini_api_key)

        # Setup Session
        timestamp = int(time.time())
        session_dir = os.path.join(self.output_dir, f"{filename_prefix}_{timestamp}")
        os.makedirs(session_dir, exist_ok=True)
        
        # Process Audio
        waveform = audio['waveform']
        sample_rate = audio['sample_rate']
        if waveform.dim() == 3: waveform = waveform.squeeze(0)
        audio_np = waveform.cpu().numpy()
        if audio_np.shape[0] < audio_np.shape[1]: audio_np = audio_np.T
        audio_int16 = (audio_np * 32767).astype(np.int16)
        
        # Handle Reference Images
        ref_images_pil = []
        if reference_images is not None:
            for img_tensor in reference_images:
                i = 255. * img_tensor.cpu().numpy()
                img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
                ref_images_pil.append(img)

        # Run Async Pipeline
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, self.async_process(
                audio_int16, sample_rate, prompt_instruction, filename_prefix, session_dir, fps, 
                model_text, model_image, creativity, dynamicity, video_quality, aspect_ratio, 
                enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency,
                use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, 
                aggressive_edit, word_influence, save_edl, ref_images_pil
            ))
            result = future.result()
            print(f"[SceneGen] === Process Complete ===\n")
            return result

    async def async_process(self, audio_data, sample_rate, instruction, prefix, session_dir, fps, model_text_name, model_image_name, creativity, dynamicity, video_quality, aspect_ratio, enable_prompt_expansion, save_segments, save_images, save_assets, gemini_concurrency, replicate_concurrency, use_wan_fast, use_wan_2_5, use_kling_turbo, use_omni_human, use_hailuo, use_hailuo_fast, use_veo_3_1, use_veo_3_1_fast, aggressive_edit, word_influence, save_edl, ref_images_pil):
        
        # --- Setup ---
        model_text = genai.GenerativeModel(model_text_name)
        model_image_gen = genai.GenerativeModel(model_image_name)
        
        # Save original audio
        original_audio_path = os.path.join(session_dir, "original_audio.wav")
        scipy.io.wavfile.write(original_audio_path, sample_rate, audio_data)
        total_duration_sec = len(audio_data) / sample_rate
        
        # Temp audio for Gemini
        with open(original_audio_path, "rb") as f:
            audio_bytes = f.read()

        available_models = []
        if use_wan_fast: available_models.append("wan-video/wan-2.5-i2v-fast")
        if use_wan_2_5: available_models.append("wan-video/wan-2.5-i2v")
        if use_kling_turbo: available_models.append("kwaivgi/kling-v2.5-turbo-pro")
        if use_omni_human: available_models.append("bytedance/omni-human-1.5")
        if use_hailuo: available_models.append("minimax/hailuo-2.3")
        if use_hailuo_fast: available_models.append("minimax/hailuo-2.3-fast")
        if use_veo_3_1: available_models.append("google/veo-3.1")
        if use_veo_3_1_fast: available_models.append("google/veo-3.1-fast")
        if not available_models: raise ValueError("No video models selected.")

        # --- STAGE 1: Audio Analysis ---
        print("[SceneGen] Stage 1: Audio Analysis...")
        prompt_s1 = f"""
        Analyze the attached audio file for a music video. User instruction: "{instruction}"
        Return a JSON object with:
        - "genre": Music genre.
        - "characteristics": Key musical characteristics.
        - "dramaturgy": Key moments of drama/dynamics changes (time and description).
        - "lyrics": Full lyrics with word-alignment (if applicable).
        - "structure": Proposed structure for the video (Intro, Verse, Chorus, etc.) with timestamps.
        """
        resp_s1 = await model_text.generate_content_async([prompt_s1, {"mime_type": "audio/wav", "data": audio_bytes}])
        debug_s1 = resp_s1.text
        with open(os.path.join(session_dir, "stage1_analysis.txt"), "w", encoding="utf-8") as f: f.write(debug_s1)
        
        # --- STAGE 2: Style Definition ---
        print("[SceneGen] Stage 2: Style Definition...")
        prompt_s2 = f"""
        Based on the audio analysis: {debug_s1}
        And user instruction: "{instruction}"
        
        Define a precise, universal visual style instruction for the video.
        Return a JSON object with:
        - "style_instruction": A comprehensive style description (e.g., "Cyberpunk noir with neon lighting and grainy film texture").
        """
        resp_s2 = await model_text.generate_content_async(prompt_s2)
        debug_s2 = resp_s2.text
        with open(os.path.join(session_dir, "stage2_style.json"), "w", encoding="utf-8") as f: f.write(debug_s2)
        style_instruction = json.loads(self._clean_json(debug_s2)).get("style_instruction", "")

        # --- STAGE 3: Color Palette ---
        print("[SceneGen] Stage 3: Color Palette...")
        prompt_s3 = f"""
        Based on the style: "{style_instruction}"
        And audio analysis: {debug_s1}
        
        Create a color palette.
        Return a JSON object with:
        - "palette": List of colors (HEX codes or descriptive names like "Electric Blue").
        - "lighting_mood": Description of the lighting atmosphere.
        """
        resp_s3 = await model_text.generate_content_async(prompt_s3)
        debug_s3 = resp_s3.text
        with open(os.path.join(session_dir, "stage3_palette.json"), "w", encoding="utf-8") as f: f.write(debug_s3)
        palette_data = json.loads(self._clean_json(debug_s3))

        # --- STAGE 3.5: Reference Analysis ---
        print("[SceneGen] Stage 3.5: Reference Analysis...")
        ref_data = []
        asset_library = {} # Name -> Image
        
        # Output lists
        env_imgs = []
        prop_imgs = []
        actor_imgs = []

        if ref_images_pil:
            prompt_s3_5 = f"""
            Analyze the attached {len(ref_images_pil)} images in context of: "{instruction}" and Style: "{style_instruction}".
            For each image, identify its category (Environment, Prop, or Actor) and provide a descriptive name (unique_id) and visual description.
            Return JSON object with "references": list of objects:
            - "index": int (0-based index of the image provided)
            - "category": string ("Environment", "Prop", "Actor")
            - "description": string
            - "name": string (unique identifier)
            """
            try:
                req_content = [prompt_s3_5] + ref_images_pil
                resp_s3_5 = await model_text.generate_content_async(req_content)
                ref_data = json.loads(self._clean_json(resp_s3_5.text)).get("references", [])
                with open(os.path.join(session_dir, "stage3_5_references.json"), "w", encoding="utf-8") as f: json.dump(ref_data, f, indent=2)
                
                for r in ref_data:
                    idx = r.get("index")
                    if idx is not None and 0 <= idx < len(ref_images_pil):
                        img = ref_images_pil[idx]
                        cat = r.get("category", "").lower()
                        name = r.get("name", f"ref_{idx}")
                        asset_library[name] = img
                        if "env" in cat: env_imgs.append(img)
                        elif "prop" in cat: prop_imgs.append(img)
                        elif "actor" in cat: actor_imgs.append(img)
                        else: actor_imgs.append(img)
            except Exception as e:
                print(f"Stage 3.5 Error: {e}")

        # --- STAGE 4: Asset Definition & Generation ---
        print("[SceneGen] Stage 4: Asset Definition & Generation...")
        prompt_s4 = f"""
        Based on the style: "{style_instruction}"
        And existing Reference Assets: {json.dumps(ref_data)}
        
        Define ONLY the ADDITIONAL visual assets needed to fulfill the user instruction: "{instruction}".
        If existing references are sufficient for a category, return an empty list for it.
        
        Return a JSON object with 3 lists:
        - "environments": List of visual descriptions for NEW locations.
        - "props": List of visual descriptions for NEW props.
        - "actors": List of visual descriptions for NEW characters.
        """
        resp_s4 = await model_text.generate_content_async(prompt_s4)
        debug_s4 = resp_s4.text
        with open(os.path.join(session_dir, "stage4_assets.json"), "w", encoding="utf-8") as f: f.write(debug_s4)
        assets_data = json.loads(self._clean_json(debug_s4))
        
        new_env_prompts = assets_data.get("environments", [])
        new_prop_prompts = assets_data.get("props", [])
        new_actor_prompts = assets_data.get("actors", [])
        
        # Helper to generate assets
        async def gen_asset(prompt, prefix_name):
            full_prompt = f"{style_instruction}. {prompt}. {palette_data.get('lighting_mood', '')}"
            try:
                r = await model_image_gen.generate_content_async(full_prompt)
                if hasattr(r, 'parts'):
                    for p in r.parts:
                        if hasattr(p, 'inline_data'):
                            img = Image.open(io.BytesIO(p.inline_data.data))
                            if save_assets:
                                img.save(os.path.join(session_dir, f"asset_{prefix_name}_{random.randint(0,9999)}.png"))
                            return img
            except Exception as e:
                print(f"Asset Gen Error: {e}")
            return Image.new('RGB', (512, 512))

        print(f"Generating {len(new_env_prompts)} new environments, {len(new_prop_prompts)} new props, {len(new_actor_prompts)} new actors...")
        new_env_imgs = await asyncio.gather(*[gen_asset(p, "env") for p in new_env_prompts])
        new_prop_imgs = await asyncio.gather(*[gen_asset(p, "prop") for p in new_prop_prompts])
        new_actor_imgs = await asyncio.gather(*[gen_asset(p, "actor") for p in new_actor_prompts])
        
        # Register new assets
        for i, img in enumerate(new_env_imgs):
            name = f"Gen_Env_{i}"
            asset_library[name] = img
            env_imgs.append(img)
        for i, img in enumerate(new_prop_imgs):
            name = f"Gen_Prop_{i}"
            asset_library[name] = img
            prop_imgs.append(img)
        for i, img in enumerate(new_actor_imgs):
            name = f"Gen_Actor_{i}"
            asset_library[name] = img
            actor_imgs.append(img)
            
        available_assets_summary = ref_data + [
            {"name": f"Gen_Env_{i}", "category": "Environment", "description": p} for i, p in enumerate(new_env_prompts)
        ] + [
            {"name": f"Gen_Prop_{i}", "category": "Prop", "description": p} for i, p in enumerate(new_prop_prompts)
        ] + [
            {"name": f"Gen_Actor_{i}", "category": "Actor", "description": p} for i, p in enumerate(new_actor_prompts)
        ]

        # --- STAGE 5: Montage Line ---
        print("[SceneGen] Stage 5: Montage Line...")
        prompt_s5 = f"""
        Create a video montage JSON.
        Context:
        - Audio Duration: {total_duration_sec}s
        - Analysis: {debug_s1}
        - Style: {style_instruction}
        - Assets Available: {json.dumps(available_assets_summary)}
        - Available Models: {json.dumps(available_models)}
        - Dynamicity: {dynamicity} (0-1)
        - Aggressive Edit: {aggressive_edit} (Boolean)
        
        Task:
        Create a list of scenes that strictly sums to {total_duration_sec}s.
        For each scene, choose a model and duration.
        - Wan/Kling: 5s or 10s.
        - Hailuo: 6s or 10s.
        - Veo: 4s, 6s, 8s.
        - OmniHuman: Any.
        
        If Aggressive Edit is True:
        - Create a fast-paced montage with frequent cuts (short target durations).
        - You MUST still respect the model generation minimums (e.g. 5s), but we will trim them later.
        - Specify the "trim_duration" in the JSON to indicate the actual length used in the edit (e.g. 2.0s).
        - Ensure cuts are strictly synchronized to the beat/dramaturgy.
        
        Return JSON object with "scenes": list of objects:
        - "duration": float (Generation duration, e.g. 5.0)
        - "trim_duration": float (Actual edit duration, e.g. 2.5. If not aggressive, same as duration)
        - "model": string
        - "clue": simple description of the shot content.
        - "asset_refs": list of strings (names of assets from Assets Available to use).
        """
        resp_s5 = await model_text.generate_content_async(prompt_s5)
        debug_s5 = resp_s5.text
        with open(os.path.join(session_dir, "stage5_montage.json"), "w", encoding="utf-8") as f: f.write(debug_s5)
        montage_data = json.loads(self._clean_json(debug_s5)).get("scenes", [])

        # --- STAGE 6: Prompt Engineering ---
        print("[SceneGen] Stage 6: Prompt Engineering...")
        prompt_s6 = f"""
        Expand the montage into detailed production prompts.
        Montage: {json.dumps(montage_data)}
        Style: {style_instruction}
        Palette: {json.dumps(palette_data)}
        Word Influence: {word_influence} (-1.0 to 1.0)
        
        Instruction for Word Influence:
        - If > 0.5: Prioritize LITERAL visualization of lyrics/text. Show exactly what is described.
        - If < -0.5: Prioritize VIBE/ATMOSPHERE. Ignore literal lyrics, focus on mood and abstract representation.
        - If ~0.0: Balance both.
        
        For EACH scene, generate:
        - "positive_prompt": Highly detailed visual description for image generation.
        - "video_trigger_prompt": Motion instruction for video generation.
        - "negative_prompt": Standard negative prompt.
        
        Return JSON object with "scenes": list of objects (same order).
        """
        resp_s6 = await model_text.generate_content_async(prompt_s6)
        debug_s6 = resp_s6.text
        with open(os.path.join(session_dir, "stage6_detailed_prompts.json"), "w", encoding="utf-8") as f: f.write(debug_s6)
        detailed_scenes = json.loads(self._clean_json(debug_s6)).get("scenes", [])
        
        # Merge detailed prompts into montage data
        final_scenes = []
        for i, scene in enumerate(montage_data):
            if i < len(detailed_scenes):
                scene.update(detailed_scenes[i])
            final_scenes.append(scene)

        # --- STAGE 7: Image Generation (Start Frames) ---
        print(f"[SceneGen] Stage 7: Generating Start Frames ({len(final_scenes)} scenes)...")
        
        ar_map = {"16:9": (1280, 720), "1:1": (1024, 1024), "9:16": (720, 1280)}
        img_w, img_h = ar_map.get(aspect_ratio, (1280, 720))
        
        async def gen_scene_image(idx, scene_data):
            prompt = f"{scene_data.get('positive_prompt')} --aspect {aspect_ratio}"
            
            input_parts = [prompt]
            
            # Smart Asset Selection
            refs = scene_data.get("asset_refs", [])
            ref_img_found = False
            if refs:
                for r_name in refs:
                    if r_name in asset_library:
                        input_parts.append(asset_library[r_name])
                        ref_img_found = True
                        break
            
            # Fallback
            if not ref_img_found:
                if actor_imgs: input_parts.append(actor_imgs[0])
                elif env_imgs: input_parts.append(env_imgs[0])
            
            try:
                r = await model_image_gen.generate_content_async(input_parts)
                if hasattr(r, 'parts'):
                    for p in r.parts:
                        if hasattr(p, 'inline_data'):
                            img = Image.open(io.BytesIO(p.inline_data.data))
                            img = img.resize((img_w, img_h))
                            if save_images:
                                img.save(os.path.join(session_dir, f"{prefix}_scene_{idx:03d}.png"))
                            return img
            except Exception as e:
                print(f"Scene Image Error {idx}: {e}")
            return Image.new('RGB', (img_w, img_h))

        scene_images = await asyncio.gather(*[gen_scene_image(i, s) for i, s in enumerate(final_scenes)])
        debug_s7 = f"Generated {len(scene_images)} start frames."

        # --- STAGE 8: Video Prompt Generation (Already done in Stage 6 really, but we refine for API) ---
        print("[SceneGen] Stage 8: Preparing Video Prompts...")
        debug_s8 = "Video prompts prepared from Stage 6 data."

        # --- STAGE 9: Duration Verification & Audio Slicing ---
        print("[SceneGen] Stage 9: Duration Verification...")
        current_sample = 0
        replicate_tasks = []
        
        for i, scene in enumerate(final_scenes):
            target_dur = float(scene.get("duration", 5))
            trim_dur = float(scene.get("trim_duration", target_dur))
            if not aggressive_edit: trim_dur = target_dur # Safety fallback
            
            model = scene.get("model", available_models[0])
            
            # Snap logic
            gen_dur = target_dur
            if "wan-video" in model or "kling" in model:
                gen_dur = 5 if target_dur <= 5 else 10
            elif "hailuo" in model:
                gen_dur = 6 if target_dur <= 6 else 10
            elif "veo" in model:
                if target_dur <= 4: gen_dur = 4
                elif target_dur <= 6: gen_dur = 6
                else: gen_dur = 8
            
            if target_dur > gen_dur: target_dur = gen_dur
            
            # Audio Slice
            start_sec = current_sample / sample_rate
            gen_end_sec = start_sec + gen_dur
            start_samp = int(start_sec * sample_rate)
            gen_end_samp = int(gen_end_sec * sample_rate)
            
            if start_samp >= len(audio_data): break
            
            raw_slice = audio_data[start_samp:gen_end_samp]
            if len(raw_slice) < (gen_end_samp - start_samp):
                pad = (gen_end_samp - start_samp) - len(raw_slice)
                if pad > 0:
                    silence = np.zeros((pad,) + raw_slice.shape[1:], dtype=raw_slice.dtype)
                    raw_slice = np.concatenate((raw_slice, silence), axis=0)
            
            replicate_tasks.append({
                "index": i,
                "model": model,
                "prompt": f"{scene.get('video_trigger_prompt')} {scene.get('positive_prompt')}",
                "negative_prompt": scene.get("negative_prompt", ""),
                "duration": gen_dur,
                "trim_duration": trim_dur,
                "audio_slice": raw_slice,
                "image": scene_images[i]
            })
            
            current_sample += int(trim_dur * sample_rate)
            
        debug_s9 = f"Prepared {len(replicate_tasks)} tasks with duration verification."

        # --- STAGE 10: Replicate Execution ---
        print(f"[SceneGen] Stage 10: Replicate Execution ({len(replicate_tasks)} tasks)...")
        
        video_paths = [None] * len(replicate_tasks)
        
        def run_replicate(task):
            idx = task["index"]
            print(f"Task {idx} ({task['model']}) starting...")
            
            # Temp files
            with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tf_img:
                task["image"].save(tf_img, format="PNG")
                img_path = tf_img.name
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tf_aud:
                scipy.io.wavfile.write(tf_aud.name, sample_rate, task["audio_slice"])
                aud_path = tf_aud.name
                
            try:
                video_url = None
                max_retries = 2
                current_prompt = task["prompt"]
                
                for attempt in range(max_retries + 1):
                    try:
                        with open(img_path, "rb") as f_img, open(aud_path, "rb") as f_aud:
                            input_data = {}
                            # Map inputs based on model
                            if "wan-video" in task["model"]:
                                res = "1080p" if video_quality == "High" else "720p"
                                if video_quality == "Low": res = "480p"
                                input_data = {"image": f_img, "audio": f_aud, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "resolution": res, "enable_prompt_expansion": enable_prompt_expansion}
                            elif "kling" in task["model"]:
                                input_data = {"start_image": f_img, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "aspect_ratio": aspect_ratio, "guidance_scale": 0.5}
                            elif "omni-human" in task["model"]:
                                input_data = {"image": f_img, "audio": f_aud, "prompt": current_prompt, "fast_mode": (video_quality != "High")}
                            elif "hailuo" in task["model"]:
                                res = "1080p" if video_quality == "High" else "768p"
                                input_data = {"first_frame_image": f_img, "prompt": current_prompt, "duration": task["duration"], "resolution": res, "prompt_optimizer": True}
                            elif "veo" in task["model"]:
                                res = "1080p" if video_quality == "High" else "720p"
                                input_data = {"image": f_img, "prompt": current_prompt, "negative_prompt": task["negative_prompt"], "duration": task["duration"], "resolution": res, "aspect_ratio": aspect_ratio, "generate_audio": True}

                            output = replicate.run(task["model"], input=input_data)
                            video_url = str(output)
                            break
                    except Exception as e:
                        if ("sensitive" in str(e).lower() or "E005" in str(e)) and attempt < max_retries:
                            print(f"Task {idx} Sensitive Content. Sanitizing...")
                            # Simple sanitization for now to save tokens/time, or call Gemini if needed
                            # For this implementation, let's just strip common bad words or retry with a generic prompt
                            # Ideally we call Gemini here as per previous logic.
                            current_prompt = "A beautiful cinematic scene, safe for work, artistic style." 
                        else:
                            raise e

                # Download
                if save_segments: local_path = os.path.join(session_dir, f"{prefix}_{idx:03d}.mp4")
                else: local_path = os.path.join(session_dir, f"seg_{idx:03d}.mp4")
                
                with requests.get(video_url, stream=True) as r:
                    r.raise_for_status()
                    with open(local_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192): f.write(chunk)
                return (idx, local_path)
                
            except Exception as e:
                print(f"Task {idx} Failed: {e}")
                return (idx, None)
            finally:
                try: os.unlink(img_path); os.unlink(aud_path)
                except: pass

        loop = asyncio.get_running_loop()
        rep_futures = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=replicate_concurrency) as rep_exec:
            for task in replicate_tasks:
                rep_futures.append(loop.run_in_executor(rep_exec, run_replicate, task))
            results = await asyncio.gather(*rep_futures)
            
        for idx, path in results:
            if path: video_paths[idx] = path
            
        debug_s10 = f"Generated {sum(1 for v in video_paths if v)} videos."

        # --- STAGE 11: Stitching ---
        print("[SceneGen] Stage 11: Stitching...")
        final_video_path = os.path.join(session_dir, f"{prefix}_final.mp4")
        
        valid_videos = [(i, v) for i, v in enumerate(video_paths) if v is not None]
        if valid_videos:
            concat_list = os.path.join(session_dir, "concat.txt")
            norm_videos = []
            target_w, target_h = img_w, img_h
            
            for i, v in valid_videos:
                trim = replicate_tasks[i]["trim_duration"]
                norm = os.path.join(session_dir, f"norm_{i:03d}.mp4")
                cmd = [
                    "ffmpeg", "-y", "-i", v, "-t", str(trim),
                    "-vf", f"scale={target_w}:{target_h}:force_original_aspect_ratio=decrease,pad={target_w}:{target_h}:(ow-iw)/2:(oh-ih)/2",
                    "-r", "24", "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-an", norm
                ]
                try:
                    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    norm_videos.append(norm)
                except: pass
            
            if norm_videos:
                with open(concat_list, "w") as f:
                    for n in norm_videos: f.write(f"file '{n.replace(os.sep, '/')}'\n")
                
                silent_path = os.path.join(session_dir, "silent.mp4")
                subprocess.run(["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", concat_list, "-c", "copy", silent_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # Mux with original audio
                subprocess.run([
                    "ffmpeg", "-y", "-i", silent_path, "-i", original_audio_path,
                    "-c:v", "copy", "-c:a", "aac", "-map", "0:v:0", "-map", "1:a:0", "-shortest", final_video_path
                ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                # Cleanup
                if not save_segments:
                    for n in norm_videos: os.unlink(n)
                    os.unlink(concat_list)
                    os.unlink(silent_path)
                    for _, v in valid_videos: os.unlink(v)

        debug_s11 = f"Final video: {final_video_path}"

        # EDL Generation
        if save_edl:
            edl_path = os.path.join(session_dir, f"{prefix}.edl")
            with open(edl_path, "w") as edl:
                edl.write(f"TITLE: {prefix}\nFCM: NON-DROP FRAME\n\n")
                # CMX 3600 Format
                # 001  AX       V     C        00:00:00:00 00:00:05:00 00:00:00:00 00:00:05:00
                
                rec_in_sec = 0.0
                for i, v in valid_videos:
                    trim = replicate_tasks[i]["trim_duration"]
                    
                    # Source TC (Assume 0 start for generated clips)
                    src_in = "00:00:00:00"
                    src_out_sec = trim
                    
                    # Record TC
                    rec_out_sec = rec_in_sec + trim
                    
                    def sec_to_tc(s):
                        frames = int(s * 24)
                        hh = frames // 86400
                        rem = frames % 86400
                        mm = rem // 3600
                        rem = rem % 3600
                        ss = rem // 24
                        ff = rem % 24
                        return f"{hh:02d}:{mm:02d}:{ss:02d}:{ff:02d}"

                    src_out = sec_to_tc(src_out_sec)
                    rec_in = sec_to_tc(rec_in_sec)
                    rec_out = sec_to_tc(rec_out_sec)
                    
                    line = f"{i+1:03d}  AX       V     C        {src_in} {src_out} {rec_in} {rec_out}\n"
                    edl.write(line)
                    edl.write(f"* FROM CLIP NAME: {os.path.basename(v)}\n\n")
                    
                    rec_in_sec = rec_out_sec
            debug_s11 += f"\nEDL saved to: {edl_path}"

        # Prepare Outputs
        def to_tensor(imgs):
            if not imgs: return torch.zeros((1, 512, 512, 3))
            tensors = []
            base_w, base_h = imgs[0].size
            for img in imgs:
                if img.size != (base_w, base_h):
                    img = img.resize((base_w, base_h))
                i = np.array(img).astype(np.float32) / 255.0
                tensors.append(torch.from_numpy(i)[None,])
            return torch.cat(tensors, dim=0)

        return (
            to_tensor(env_imgs), to_tensor(prop_imgs), to_tensor(actor_imgs), to_tensor(scene_images),
            debug_s1, debug_s2, debug_s3, debug_s4, debug_s5, debug_s6, debug_s7, debug_s8, debug_s9, debug_s10, debug_s11,
            final_video_path
        )

    def _clean_json(self, text):
        text = text.strip()
        if text.startswith("```json"): text = text[7:]
        if text.startswith("```"): text = text[3:]
        if text.endswith("```"): text = text[:-3]
        return text.strip()
